{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and thoughts\n",
    "\n",
    "#### Current best ideas / runs that work / etc.\n",
    "- Batch size = ~1/6th of training data size seems to make all the difference\n",
    "- nfilters is only 4 for a single class (perhaps 8 works better? Takes longer to train. Trade-offs...)\n",
    "- Perhaps the best method is to train 10 models, one for each class\n",
    "- Dropout after each maxpooling step in the down-path and after each upconv/conv/conv step in the up-path. I used p=0.2 for each dropout layer, however the code is written such that this can be changed ona  per-layer basis.\n",
    "- Activation = LeakyReLu\n",
    "- Batchnorm is *after* the activation\n",
    "- L2 reg is applied to each convolution with a lambda of 0.00001. Question remains: is this even doing anything?\n",
    "- lr is 0.001. This is the default for Adam. Would a higher value train faster?\n",
    "- Currently there are 400 training images\n",
    "    - Could this be reduced?\n",
    "    - Currently the data augmentation is applied to the entire data set, then it is randomly split for validation\n",
    "    - Should the test/val sets be split early and *then* apply data augmentation *seperately*?\n",
    "    - **NB** be aware that some classes only have a single image. Thus, the training/val sets should each contain examples of this class but it should be heavily augmented so as to be treated as different)\n",
    "\n",
    "#### Ideas\n",
    "- Weight map: sum the total area of all classes in y, then calculate each class' proportion of the total and use `1-value` in place of 1 in the binary mask. This will cause low frequency classes to contribute more to the total loss, i.e. penalizing the model when it fails to predict low frequency classes.\n",
    "\n",
    "#### Data augmentation / image manipulation\n",
    "- [Histogram Equalization](http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html#sphx-glr-auto-examples-color-exposure-plot-equalize-py) (also see [here](https://www.kaggle.com/gabrielaltay/dstl-satellite-imagery-feature-detection/exploring-color-scaling-images/discussion), [here](http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/) and [here](https://medium.com/@vivek.yadav/improved-performance-of-deep-learning-neural-network-models-on-traffic-sign-classification-using-6355346da2dc#.x9nidcsh6))\n",
    "- Rotation (with reflection): see data-augmentation.ipynb\n",
    "- Image normalization: see image-preprocessing-new.ipynb\n",
    "\n",
    "#### Overfitting solutions\n",
    "- CNNs are supposed to be more robust to this because of the shared weight matrix of each filter\n",
    "- **Data augmentation!**\n",
    "    - Have done random rotations on data increasing total n by 6-fold\n",
    "    - Not seeing drastic improvements\n",
    "- L2 regularization (added into the layers via `W_regularizer=l2(l=0.01)` parameter)\n",
    "    - Not seeing much improvement\n",
    "    - Currently set to 0.00001\n",
    "- Move batchnorm to *before* the relu takes place (see [here](http://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras)\n",
    "- Add batchnorm to upconv() layer\n",
    "- Try mode=1 in batchnorm\n",
    "- Dropout hurts the model in my experience..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U-Net architecture\n",
    "\n",
    "See [here](https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py#L19) for code and [here](https://arxiv.org/pdf/1505.04597.pdf) for the original literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Activation, Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "# \"tf\" assumes (rows, cols, channels) while \"th\" assumes (channels, rows, cols)\n",
    "# Possibly change this around natively in the data so the backend doesn't have to switch them\n",
    "# Only necessary if I use TF!\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_preds(preds):\n",
    "    for i in range(preds.shape[0]):\n",
    "        preds[i,0,:,:] = (preds[i,0,:,:].min() - preds[i,0,:,:])/(preds[i,0,:,:].min()-preds[i,0,:,:].max())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_all(i,classType):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.imshow(preds[i,0,...],cmap='spectral')\n",
    "\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.imshow(np.rint(preds[i,0,...]),cmap='spectral')\n",
    "\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax3.imshow(x[i,17,...],cmap='Greys')\n",
    "\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    ax4.imshow(y_oneclass[i,classType,:,:],cmap='spectral')\n",
    "    \n",
    "    plt.show()\n",
    "#plot_all(2,2,classType,0.5)\n",
    "#push('PICTURES!','The plots are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the components of the pushbullet API\n",
    "from pushbullet import Pushbullet\n",
    "\n",
    "pb = Pushbullet('o.YFPNNPfGRekivaCGHa4qMSgjZt8zJ6FL')\n",
    "phone = pb.devices[0]\n",
    "\n",
    "# Run this cell after anything you want to be notified about!\n",
    "def push(title='Done!',text='Whatever it was, it\\'s done'):\n",
    "    phone.push_note(title,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the training data\n",
    "def import_data():\n",
    "    x = np.load('./data/x_augmented.npy','r')\n",
    "    y = np.load('./data/y_augmented.npy','r')\n",
    "    \n",
    "    '''with open('./data/x_resized_array.pickle','rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        x = x.astype(np.float32)\n",
    "        \n",
    "    with open('./data/y_resized_raster.pickle','rb') as f:\n",
    "        y = pickle.load(f)\n",
    "        y = y.astype(np.float32)'''\n",
    "    \n",
    "    y_oneclass = y[:,3:4,...]\n",
    "    \n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''\n",
    "    \n",
    "    return x, y, y_oneclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run = 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, y_oneclass = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 41.1 ms, total: 1.73 s\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "V2.0 U-Net with batchnorm\n",
    "'''\n",
    "def builder(img_rows = x.shape[2],img_cols = x.shape[3],\n",
    "            nfilters = 32,activation = 'relu',init = 'he_normal',\n",
    "            lr=1.0,decay=0.0,momentum=0.0, nesterov=False,reg=0.01,p=[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2]):\n",
    "    \n",
    "    def jaccard(y_true, y_pred,smooth=1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "    \n",
    "    def Conv2DReluBatchNorm(n_filter, w_filter, h_filter, inputs, activation, init='he_uniform',dropout=0.2):\n",
    "        # Batch norm after activation\n",
    "        return BatchNormalization(mode=2, axis=1)(LeakyReLU()((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "        # Batch norm before activation\n",
    "        #return LeakyReLU()(BatchNormalization(mode=0, axis=1)((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "\n",
    "    def up_conv(nfilters,filter_factor,inputs,init=init,activation=activation):\n",
    "        # No batch norm\n",
    "        #return LeakyReLU()(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs)))\n",
    "        \n",
    "        # Batch norm after activation\n",
    "        return BatchNormalization(mode=2, axis=1)(LeakyReLU()(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs))))\n",
    "\n",
    "    inputs = Input((20, img_rows, img_cols))\n",
    "    padded = ZeroPadding2D(padding=(12,12))(inputs)\n",
    "    \n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, padded, activation=activation,init=init)\n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, conv1, activation=activation,init=init)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(p=p[0])(pool1)\n",
    "\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, pool1, activation=activation,init=init)\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv2, activation=activation,init=init)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(p=p[1])(pool2)\n",
    "\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, pool2, activation=activation,init=init)\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv3, activation=activation,init=init)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(p=p[2])(pool3)\n",
    "\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, pool3, activation=activation,init=init)\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv4, activation=activation,init=init)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = Dropout(p=p[3])(pool4)\n",
    "\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, pool4, activation=activation,init=init)\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, conv5, activation=activation,init=init)\n",
    "    conv5 = Dropout(p=p[4])(conv5)\n",
    "        \n",
    "    up6 = merge([up_conv(nfilters,8,conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, up6, activation=activation,init=init)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv6, activation=activation,init=init)\n",
    "    conv6 = Dropout(p=p[5])(conv6)\n",
    "\n",
    "    up7 = merge([up_conv(nfilters,4,conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, up7, activation=activation,init=init)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv7, activation=activation,init=init)\n",
    "    conv7 = Dropout(p=p[6])(conv7)\n",
    "\n",
    "    up8 = merge([up_conv(nfilters,2,conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, up8, activation=activation,init=init)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv8, activation=activation,init=init)\n",
    "    conv8 = Dropout(p=p[7])(conv8)\n",
    "\n",
    "    up9 = merge([up_conv(nfilters,1,conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, up9, activation=activation,init=init)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, conv9, activation=activation,init=init)\n",
    "    conv9 = Dropout(p=p[8])(conv9)\n",
    "    \n",
    "    conv10 = Conv2DReluBatchNorm(1, 1, 1, conv9, activation='relu',init=init)\n",
    "    cropped = Cropping2D(cropping=((12,12), (12,12)))(conv10)\n",
    "    output = Activation(activation='sigmoid')(cropped)\n",
    "    \n",
    "    model = Model(input=inputs, output=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=lr,decay=decay), loss='binary_crossentropy', metrics=[jaccard])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = builder(img_rows=x.shape[2],img_cols=x.shape[3],\n",
    "            nfilters=4,activation='relu',init='he_normal',\n",
    "            lr=0.001,decay=0,momentum=0,reg=0.00001,p=[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2])\n",
    "\n",
    "#push('The model is compiled','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run number: 75...\n",
      "Fitting model...\n",
      "------------------------------\n",
      "320/320 [==============================] - 7s - loss: 0.4163 - jaccard: 0.1166 - val_loss: 0.4101 - val_jaccard: 0.1227\n",
      "Epoch 30/1000\n",
      "320/320 [==============================] - 7s - loss: 0.4139 - jaccard: 0.1176 - val_loss: 0.4068 - val_jaccard: 0.1211\n",
      "Epoch 31/1000\n",
      "320/320 [==============================] - 7s - loss: 0.4104 - jaccard: 0.1190 - val_loss: 0.4047 - val_jaccard: 0.1212\n",
      "Epoch 32/1000\n",
      "320/320 [==============================] - 7s - loss: 0.4073 - jaccard: 0.1193 - val_loss: 0.4016 - val_jaccard: 0.1230\n",
      "Epoch 33/1000\n",
      "320/320 [==============================] - 7s - loss: 0.4052 - jaccard: 0.1197 - val_loss: 0.3999 - val_jaccard: 0.1210\n",
      "Epoch 34/1000\n",
      "320/320 [==============================] - 7s - loss: 0.4025 - jaccard: 0.1204 - val_loss: 0.3981 - val_jaccard: 0.1202\n",
      "Epoch 35/1000\n",
      "320/320 [==============================] - 7s - loss: 0.4005 - jaccard: 0.1194 - val_loss: 0.3961 - val_jaccard: 0.1208\n",
      "Epoch 36/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3972 - jaccard: 0.1205 - val_loss: 0.3929 - val_jaccard: 0.1258\n",
      "Epoch 37/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3952 - jaccard: 0.1208 - val_loss: 0.3904 - val_jaccard: 0.1241\n",
      "Epoch 38/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3926 - jaccard: 0.1221 - val_loss: 0.3884 - val_jaccard: 0.1253\n",
      "Epoch 39/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3900 - jaccard: 0.1206 - val_loss: 0.3871 - val_jaccard: 0.1297\n",
      "Epoch 40/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3880 - jaccard: 0.1232 - val_loss: 0.3846 - val_jaccard: 0.1236\n",
      "Epoch 41/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3857 - jaccard: 0.1223 - val_loss: 0.3826 - val_jaccard: 0.1249\n",
      "Epoch 42/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3831 - jaccard: 0.1231 - val_loss: 0.3810 - val_jaccard: 0.1287\n",
      "Epoch 43/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3817 - jaccard: 0.1239 - val_loss: 0.3793 - val_jaccard: 0.1262\n",
      "Epoch 44/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3791 - jaccard: 0.1254 - val_loss: 0.3772 - val_jaccard: 0.1250\n",
      "Epoch 45/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3763 - jaccard: 0.1257 - val_loss: 0.3748 - val_jaccard: 0.1296\n",
      "Epoch 46/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3757 - jaccard: 0.1255 - val_loss: 0.3742 - val_jaccard: 0.1244\n",
      "Epoch 47/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3738 - jaccard: 0.1245 - val_loss: 0.3721 - val_jaccard: 0.1253\n",
      "Epoch 48/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3718 - jaccard: 0.1231 - val_loss: 0.3707 - val_jaccard: 0.1287\n",
      "Epoch 49/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3692 - jaccard: 0.1280 - val_loss: 0.3687 - val_jaccard: 0.1262\n",
      "Epoch 50/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3675 - jaccard: 0.1271 - val_loss: 0.3666 - val_jaccard: 0.1287\n",
      "Epoch 51/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3655 - jaccard: 0.1261 - val_loss: 0.3644 - val_jaccard: 0.1331\n",
      "Epoch 52/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3636 - jaccard: 0.1289 - val_loss: 0.3629 - val_jaccard: 0.1308\n",
      "Epoch 53/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3618 - jaccard: 0.1286 - val_loss: 0.3612 - val_jaccard: 0.1320\n",
      "Epoch 54/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3612 - jaccard: 0.1276 - val_loss: 0.3603 - val_jaccard: 0.1316\n",
      "Epoch 55/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3589 - jaccard: 0.1293 - val_loss: 0.3585 - val_jaccard: 0.1302\n",
      "Epoch 56/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3570 - jaccard: 0.1282 - val_loss: 0.3571 - val_jaccard: 0.1370\n",
      "Epoch 57/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3556 - jaccard: 0.1306 - val_loss: 0.3563 - val_jaccard: 0.1339\n",
      "Epoch 58/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3546 - jaccard: 0.1292 - val_loss: 0.3545 - val_jaccard: 0.1335\n",
      "Epoch 59/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3527 - jaccard: 0.1315 - val_loss: 0.3529 - val_jaccard: 0.1333\n",
      "Epoch 60/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3516 - jaccard: 0.1317 - val_loss: 0.3524 - val_jaccard: 0.1290\n",
      "Epoch 61/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3499 - jaccard: 0.1319 - val_loss: 0.3506 - val_jaccard: 0.1294\n",
      "Epoch 62/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3497 - jaccard: 0.1287 - val_loss: 0.3504 - val_jaccard: 0.1368\n",
      "Epoch 63/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3468 - jaccard: 0.1322 - val_loss: 0.3484 - val_jaccard: 0.1358\n",
      "Epoch 64/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3448 - jaccard: 0.1335 - val_loss: 0.3463 - val_jaccard: 0.1372\n",
      "Epoch 65/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3442 - jaccard: 0.1341 - val_loss: 0.3453 - val_jaccard: 0.1345\n",
      "Epoch 66/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3433 - jaccard: 0.1331 - val_loss: 0.3447 - val_jaccard: 0.1322\n",
      "Epoch 67/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3415 - jaccard: 0.1355 - val_loss: 0.3442 - val_jaccard: 0.1299\n",
      "Epoch 68/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3420 - jaccard: 0.1316 - val_loss: 0.3426 - val_jaccard: 0.1362\n",
      "Epoch 69/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3390 - jaccard: 0.1345 - val_loss: 0.3409 - val_jaccard: 0.1378\n",
      "Epoch 70/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3373 - jaccard: 0.1338 - val_loss: 0.3400 - val_jaccard: 0.1429\n",
      "Epoch 71/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3375 - jaccard: 0.1355 - val_loss: 0.3391 - val_jaccard: 0.1403\n",
      "Epoch 72/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3347 - jaccard: 0.1376 - val_loss: 0.3375 - val_jaccard: 0.1398\n",
      "Epoch 73/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3336 - jaccard: 0.1358 - val_loss: 0.3363 - val_jaccard: 0.1426\n",
      "Epoch 74/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3337 - jaccard: 0.1385 - val_loss: 0.3367 - val_jaccard: 0.1334\n",
      "Epoch 75/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3326 - jaccard: 0.1359 - val_loss: 0.3356 - val_jaccard: 0.1408\n",
      "Epoch 76/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3307 - jaccard: 0.1372 - val_loss: 0.3340 - val_jaccard: 0.1414\n",
      "Epoch 77/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3305 - jaccard: 0.1385 - val_loss: 0.3326 - val_jaccard: 0.1401\n",
      "Epoch 78/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3290 - jaccard: 0.1387 - val_loss: 0.3322 - val_jaccard: 0.1389\n",
      "Epoch 79/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3277 - jaccard: 0.1370 - val_loss: 0.3320 - val_jaccard: 0.1466\n",
      "Epoch 80/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3267 - jaccard: 0.1410 - val_loss: 0.3302 - val_jaccard: 0.1454\n",
      "Epoch 81/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3258 - jaccard: 0.1399 - val_loss: 0.3305 - val_jaccard: 0.1419\n",
      "Epoch 82/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3242 - jaccard: 0.1393 - val_loss: 0.3283 - val_jaccard: 0.1440\n",
      "Epoch 83/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3235 - jaccard: 0.1420 - val_loss: 0.3273 - val_jaccard: 0.1438\n",
      "Epoch 84/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3220 - jaccard: 0.1429 - val_loss: 0.3266 - val_jaccard: 0.1458\n",
      "Epoch 85/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3217 - jaccard: 0.1404 - val_loss: 0.3258 - val_jaccard: 0.1490\n",
      "Epoch 86/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3216 - jaccard: 0.1428 - val_loss: 0.3250 - val_jaccard: 0.1408\n",
      "Epoch 87/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3192 - jaccard: 0.1432 - val_loss: 0.3240 - val_jaccard: 0.1425\n",
      "Epoch 88/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3187 - jaccard: 0.1409 - val_loss: 0.3237 - val_jaccard: 0.1482\n",
      "Epoch 89/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3182 - jaccard: 0.1450 - val_loss: 0.3230 - val_jaccard: 0.1440\n",
      "Epoch 90/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3184 - jaccard: 0.1405 - val_loss: 0.3225 - val_jaccard: 0.1456\n",
      "Epoch 91/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3172 - jaccard: 0.1437 - val_loss: 0.3218 - val_jaccard: 0.1387\n",
      "Epoch 92/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3169 - jaccard: 0.1423 - val_loss: 0.3208 - val_jaccard: 0.1426\n",
      "Epoch 93/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3150 - jaccard: 0.1450 - val_loss: 0.3199 - val_jaccard: 0.1455\n",
      "Epoch 94/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3128 - jaccard: 0.1470 - val_loss: 0.3188 - val_jaccard: 0.1453\n",
      "Epoch 95/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3129 - jaccard: 0.1448 - val_loss: 0.3177 - val_jaccard: 0.1474\n",
      "Epoch 96/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3115 - jaccard: 0.1481 - val_loss: 0.3177 - val_jaccard: 0.1434\n",
      "Epoch 97/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3107 - jaccard: 0.1468 - val_loss: 0.3164 - val_jaccard: 0.1477\n",
      "Epoch 98/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3100 - jaccard: 0.1490 - val_loss: 0.3170 - val_jaccard: 0.1423\n",
      "Epoch 99/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3108 - jaccard: 0.1453 - val_loss: 0.3156 - val_jaccard: 0.1468\n",
      "Epoch 100/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3096 - jaccard: 0.1478 - val_loss: 0.3144 - val_jaccard: 0.1483\n",
      "Epoch 101/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3071 - jaccard: 0.1499 - val_loss: 0.3146 - val_jaccard: 0.1443\n",
      "Epoch 102/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3071 - jaccard: 0.1487 - val_loss: 0.3134 - val_jaccard: 0.1521\n",
      "Epoch 103/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3069 - jaccard: 0.1504 - val_loss: 0.3124 - val_jaccard: 0.1504\n",
      "Epoch 104/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3056 - jaccard: 0.1503 - val_loss: 0.3119 - val_jaccard: 0.1483\n",
      "Epoch 105/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3055 - jaccard: 0.1499 - val_loss: 0.3121 - val_jaccard: 0.1458\n",
      "Epoch 106/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3057 - jaccard: 0.1503 - val_loss: 0.3130 - val_jaccard: 0.1417\n",
      "Epoch 107/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3047 - jaccard: 0.1497 - val_loss: 0.3112 - val_jaccard: 0.1520\n",
      "Epoch 108/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3030 - jaccard: 0.1511 - val_loss: 0.3103 - val_jaccard: 0.1534\n",
      "Epoch 109/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3030 - jaccard: 0.1536 - val_loss: 0.3100 - val_jaccard: 0.1475\n",
      "Epoch 110/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3025 - jaccard: 0.1532 - val_loss: 0.3097 - val_jaccard: 0.1452\n",
      "Epoch 111/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3013 - jaccard: 0.1507 - val_loss: 0.3085 - val_jaccard: 0.1506\n",
      "Epoch 112/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2997 - jaccard: 0.1551 - val_loss: 0.3081 - val_jaccard: 0.1479\n",
      "Epoch 113/1000\n",
      "320/320 [==============================] - 7s - loss: 0.3003 - jaccard: 0.1519 - val_loss: 0.3069 - val_jaccard: 0.1536\n",
      "Epoch 114/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2998 - jaccard: 0.1525 - val_loss: 0.3074 - val_jaccard: 0.1571\n",
      "Epoch 115/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2997 - jaccard: 0.1539 - val_loss: 0.3070 - val_jaccard: 0.1485\n",
      "Epoch 116/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2982 - jaccard: 0.1533 - val_loss: 0.3064 - val_jaccard: 0.1571\n",
      "Epoch 117/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2976 - jaccard: 0.1556 - val_loss: 0.3054 - val_jaccard: 0.1519\n",
      "Epoch 118/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2993 - jaccard: 0.1531 - val_loss: 0.3064 - val_jaccard: 0.1495\n",
      "Epoch 119/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2987 - jaccard: 0.1520 - val_loss: 0.3069 - val_jaccard: 0.1594\n",
      "Epoch 120/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2972 - jaccard: 0.1508 - val_loss: 0.3061 - val_jaccard: 0.1582\n",
      "Epoch 121/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2959 - jaccard: 0.1570 - val_loss: 0.3042 - val_jaccard: 0.1571\n",
      "Epoch 122/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2952 - jaccard: 0.1561 - val_loss: 0.3031 - val_jaccard: 0.1579\n",
      "Epoch 123/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2935 - jaccard: 0.1598 - val_loss: 0.3023 - val_jaccard: 0.1596\n",
      "Epoch 124/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2924 - jaccard: 0.1599 - val_loss: 0.3025 - val_jaccard: 0.1582\n",
      "Epoch 125/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2937 - jaccard: 0.1573 - val_loss: 0.3022 - val_jaccard: 0.1531\n",
      "Epoch 126/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2917 - jaccard: 0.1594 - val_loss: 0.3008 - val_jaccard: 0.1598\n",
      "Epoch 127/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2916 - jaccard: 0.1595 - val_loss: 0.3009 - val_jaccard: 0.1559\n",
      "Epoch 128/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2932 - jaccard: 0.1558 - val_loss: 0.3002 - val_jaccard: 0.1565\n",
      "Epoch 129/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2920 - jaccard: 0.1585 - val_loss: 0.2999 - val_jaccard: 0.1553\n",
      "Epoch 130/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2908 - jaccard: 0.1617 - val_loss: 0.2994 - val_jaccard: 0.1580\n",
      "Epoch 131/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2892 - jaccard: 0.1599 - val_loss: 0.2997 - val_jaccard: 0.1663\n",
      "Epoch 132/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2889 - jaccard: 0.1626 - val_loss: 0.2988 - val_jaccard: 0.1608\n",
      "Epoch 133/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2888 - jaccard: 0.1629 - val_loss: 0.2984 - val_jaccard: 0.1568\n",
      "Epoch 134/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2871 - jaccard: 0.1626 - val_loss: 0.2975 - val_jaccard: 0.1654\n",
      "Epoch 135/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2872 - jaccard: 0.1640 - val_loss: 0.2978 - val_jaccard: 0.1599\n",
      "Epoch 136/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2876 - jaccard: 0.1623 - val_loss: 0.2976 - val_jaccard: 0.1643\n",
      "Epoch 137/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2865 - jaccard: 0.1641 - val_loss: 0.2968 - val_jaccard: 0.1575\n",
      "Epoch 138/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2870 - jaccard: 0.1621 - val_loss: 0.3010 - val_jaccard: 0.1468\n",
      "Epoch 139/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2881 - jaccard: 0.1607 - val_loss: 0.2972 - val_jaccard: 0.1590\n",
      "Epoch 140/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2866 - jaccard: 0.1626 - val_loss: 0.2966 - val_jaccard: 0.1542\n",
      "Epoch 141/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2850 - jaccard: 0.1640 - val_loss: 0.2948 - val_jaccard: 0.1638\n",
      "Epoch 142/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2852 - jaccard: 0.1632 - val_loss: 0.2957 - val_jaccard: 0.1712\n",
      "Epoch 143/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2839 - jaccard: 0.1663 - val_loss: 0.2942 - val_jaccard: 0.1654\n",
      "Epoch 144/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2849 - jaccard: 0.1661 - val_loss: 0.2959 - val_jaccard: 0.1579\n",
      "Epoch 145/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2840 - jaccard: 0.1644 - val_loss: 0.2939 - val_jaccard: 0.1690\n",
      "Epoch 146/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2839 - jaccard: 0.1650 - val_loss: 0.2933 - val_jaccard: 0.1659\n",
      "Epoch 147/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2818 - jaccard: 0.1660 - val_loss: 0.2940 - val_jaccard: 0.1691\n",
      "Epoch 148/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2822 - jaccard: 0.1692 - val_loss: 0.2941 - val_jaccard: 0.1580\n",
      "Epoch 149/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2836 - jaccard: 0.1636 - val_loss: 0.2924 - val_jaccard: 0.1673\n",
      "Epoch 150/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2811 - jaccard: 0.1664 - val_loss: 0.2927 - val_jaccard: 0.1703\n",
      "Epoch 151/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2803 - jaccard: 0.1693 - val_loss: 0.2924 - val_jaccard: 0.1696\n",
      "Epoch 152/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2802 - jaccard: 0.1692 - val_loss: 0.2910 - val_jaccard: 0.1700\n",
      "Epoch 153/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2797 - jaccard: 0.1718 - val_loss: 0.2921 - val_jaccard: 0.1585\n",
      "Epoch 154/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2817 - jaccard: 0.1647 - val_loss: 0.2913 - val_jaccard: 0.1717\n",
      "Epoch 155/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2794 - jaccard: 0.1698 - val_loss: 0.2921 - val_jaccard: 0.1641\n",
      "Epoch 156/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2789 - jaccard: 0.1708 - val_loss: 0.2910 - val_jaccard: 0.1676\n",
      "Epoch 157/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2794 - jaccard: 0.1678 - val_loss: 0.2912 - val_jaccard: 0.1744\n",
      "Epoch 158/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2791 - jaccard: 0.1705 - val_loss: 0.2908 - val_jaccard: 0.1733\n",
      "Epoch 159/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2786 - jaccard: 0.1699 - val_loss: 0.2903 - val_jaccard: 0.1652\n",
      "Epoch 160/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2785 - jaccard: 0.1687 - val_loss: 0.2901 - val_jaccard: 0.1767\n",
      "Epoch 161/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2775 - jaccard: 0.1741 - val_loss: 0.2912 - val_jaccard: 0.1566\n",
      "Epoch 162/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2794 - jaccard: 0.1655 - val_loss: 0.2905 - val_jaccard: 0.1647\n",
      "Epoch 163/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2769 - jaccard: 0.1734 - val_loss: 0.2895 - val_jaccard: 0.1644\n",
      "Epoch 164/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2761 - jaccard: 0.1732 - val_loss: 0.2884 - val_jaccard: 0.1742\n",
      "Epoch 165/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2758 - jaccard: 0.1731 - val_loss: 0.2888 - val_jaccard: 0.1676\n",
      "Epoch 166/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2779 - jaccard: 0.1701 - val_loss: 0.2886 - val_jaccard: 0.1650\n",
      "Epoch 167/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2772 - jaccard: 0.1675 - val_loss: 0.2885 - val_jaccard: 0.1728\n",
      "Epoch 168/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2744 - jaccard: 0.1755 - val_loss: 0.2874 - val_jaccard: 0.1728\n",
      "Epoch 169/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2747 - jaccard: 0.1757 - val_loss: 0.2876 - val_jaccard: 0.1669\n",
      "Epoch 170/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2757 - jaccard: 0.1720 - val_loss: 0.2880 - val_jaccard: 0.1663\n",
      "Epoch 171/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2742 - jaccard: 0.1730 - val_loss: 0.2864 - val_jaccard: 0.1754\n",
      "Epoch 172/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2742 - jaccard: 0.1738 - val_loss: 0.2873 - val_jaccard: 0.1723\n",
      "Epoch 173/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2734 - jaccard: 0.1745 - val_loss: 0.2872 - val_jaccard: 0.1822\n",
      "Epoch 174/1000\n",
      "320/320 [==============================] - 7s - loss: 0.2745 - jaccard: 0.1754 - val_loss: 0.2870 - val_jaccard: 0.1727\n",
      "Epoch 175/1000\n",
      "300/320 [===========================>..] - ETA: 0s - loss: 0.2714 - jaccard: 0.1753"
     ]
    }
   ],
   "source": [
    "run += 1\n",
    "def train_and_predict(model,fit=True,use_existing=False):\n",
    "    print('This is run number: '+ str(run) + '...')\n",
    "    \n",
    "    if use_existing:\n",
    "        model.load_weights('./data/model_weights_class_3_run_'+str(run)+'.hdf5')\n",
    "        \n",
    "    if fit:\n",
    "        \n",
    "        print('Fitting model...')\n",
    "        print('-'*30)\n",
    "        \n",
    "        quitter = EarlyStopping(monitor='loss', min_delta=0.001, patience=100, verbose=1, mode='auto')\n",
    "        lrreducer = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, verbose=1, mode='auto', epsilon=0.001, cooldown=2, min_lr=0)\n",
    "        model_checkpoint = ModelCheckpoint('./data/model_weights_class_3_run_'+str(run)+'.hdf5', monitor='loss', save_best_only=True)\n",
    "        csvlogger = CSVLogger('./data/training_log_run_'+str(run), separator=',', append=False)\n",
    "\n",
    "        #tensorboard = TensorBoard(log_dir='./logs/'+'run_'+str(run), histogram_freq=0, write_graph=True, write_images=False)\n",
    "        '''\n",
    "        screen -S tensorboard\n",
    "        tensorboard --logdir=logs\n",
    "        <ctrl + a,d to exit>\n",
    "        screen -r tensorboard\n",
    "        '''\n",
    "        \n",
    "        model.fit(x, y_oneclass,\n",
    "                  batch_size=30,\n",
    "                  nb_epoch=1000,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  callbacks=[model_checkpoint,csvlogger],\n",
    "                  validation_split=0.2,\n",
    "                  initial_epoch=0)\n",
    "    \n",
    "        print('Predicting masks on test data...')\n",
    "        print('-'*30)\n",
    "        \n",
    "    imgs_mask_test = model.predict(x, verbose=1)\n",
    "    np.save('jaccard_preds_all_data.npy', imgs_mask_test)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = train_and_predict(model,fit=True,use_existing=False)\n",
    "push('Training is done',\n",
    "     'Train loss: %f, train jaccard: %f, val loss %f, val jaccard%f' %(model.history.history['loss'][-1],model.history.history['jaccard'][-1],model.history.history['val_loss'][-1],model.history.history['val_jaccard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.load('jaccard_preds_all_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(x[120:150,...],y_oneclass[120:150,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm = False\n",
    "if norm:\n",
    "    preds = norm_preds(preds)\n",
    "i = np.random.choice(range(120),1)[0]\n",
    "print(i)\n",
    "plot_all(i,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_everything():\n",
    "    fig = plt.figure(figsize=(12,36))\n",
    "    #num = np.random.randint(10,25,10)\n",
    "    num = np.arange(0,10)\n",
    "    for i in range(1,10):\n",
    "        ax1 = fig.add_subplot(10,8,8*i-7)\n",
    "        ax1.imshow(preds[num[i],0,...],cmap='spectral')\n",
    "        \n",
    "        ax2 = fig.add_subplot(10,8,8*i-6)\n",
    "        ax2.imshow(y_oneclass[num[i],0,...],cmap='spectral')\n",
    "        \n",
    "        ax3 = fig.add_subplot(10,8,8*i-5)\n",
    "        ax3.imshow(preds[num[i],1,...],cmap='spectral')\n",
    "        \n",
    "        ax4 = fig.add_subplot(10,8,8*i-4)\n",
    "        ax4.imshow(y_oneclass[num[i],1,...],cmap='spectral')\n",
    "        \n",
    "        ax5 = fig.add_subplot(10,8,8*i-3)\n",
    "        ax5.imshow(preds[num[i],2,...],cmap='spectral')\n",
    "        \n",
    "        ax6 = fig.add_subplot(10,8,8*i-2)\n",
    "        ax6.imshow(y_oneclass[num[i],2,...],cmap='spectral')\n",
    "        \n",
    "        ax7 = fig.add_subplot(10,8,8*i-1)\n",
    "        ax7.imshow(preds[num[i],3,...],cmap='spectral')\n",
    "        \n",
    "        ax8 = fig.add_subplot(10,8,8*i)\n",
    "        ax8.imshow(y_oneclass[num[i],3,...],cmap='spectral')\n",
    "        \n",
    "    plt.show()\n",
    "plot_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_classifier(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defunct bits and pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_cancelled(model):\n",
    "    model.load_weights('./data/model_weights_class_3_run_66.hdf5')\n",
    "    imgs_mask_test = model.predict(x, verbose=1)\n",
    "    return imgs_mask_test\n",
    "\n",
    "preds = load_cancelled(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_classifier(model):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(model.history.history['loss'], \"-\",color='blue',label=\"Training final loss: \"+str(round(model.history.history['loss'][-1],4)))\n",
    "    ax.plot(model.history.history['jaccard'], \"-\",color='orange',label=\"testing final loss: \"+str(round(model.history.history['jaccard'][-1],4)))\n",
    "    #ax.set_xlim([0, epochs])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Jaccard / Loss')\n",
    "    plt.legend(loc='best')\n",
    "    ax.set_title('Metrics vs time (epochs)')\n",
    "    \n",
    "    '''ax = fig.add_subplot(122)\n",
    "    ax.plot(model.history['acc'], \"-\",color='blue',label=\"Training final acc: \"+str(round(model.history['acc'][-1],4)))\n",
    "    ax.plot(model.history['val_acc'], \"-\",color='orange',label=\"testing final acc: \"+str(round(model.history['val_acc'][-1],4)))\n",
    "    #ax.set_xlim([0, epochs])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    ax.set_title('Accuracy vs time (epochs)')    \n",
    "    '''\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the training data\n",
    "def import_data(load_whitened = False, normalize = True):\n",
    "    if load_whitened:\n",
    "        with open('./data/x_whitened_resized_array.pickle','rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            x = x.astype('float32')\n",
    "    else:\n",
    "        with open('./data/x_resized_array.pickle','rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            x = x.astype('float32')\n",
    "            # Normalize data to max values\n",
    "            for i in range(x.shape[0]):\n",
    "                for j in range(x.shape[1]):\n",
    "                    x[i,j,:,:] *= 1/x[i,j,:,:].max()\n",
    "                    \n",
    "    # Normalize values between 1 and 0\n",
    "    if normalize:\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                #x[i,j,:,:] /= x[i,j,:,:].max()\n",
    "                x[i,j,:,:] = (x[i,j,:,:].min() - x[i,j,:,:])/(x[i,j,:,:].min() - x[i,j,:,:].max())\n",
    "\n",
    "    with open('./data/y_resized_raster.pickle','rb') as f:\n",
    "        y = pickle.load(f)\n",
    "        y = y.astype(np.float32)\n",
    "        \n",
    "    y_oneclass = y[:,1:6,...]\n",
    "    \n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''\n",
    "    \n",
    "    return x, y, y_oneclass\n",
    "    \n",
    "    # Just use a single class: roads\n",
    "    #y = y[:,4,:,:]\n",
    "    #y = y[:,np.newaxis,:,:]\n",
    "\n",
    "    # y = y.reshape(y.shape[0],-1)\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def np_jaccard(y_true,y_pred,smooth=1.):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true * np.rint(y_pred))\n",
    "    return (intersection+smooth) / (np.sum(y_true) + np.sum(y_pred) - intersection+smooth)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
