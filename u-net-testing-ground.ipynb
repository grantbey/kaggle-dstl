{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U-Net architecture\n",
    "\n",
    "See [here](https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py#L19) for code and [here](https://arxiv.org/pdf/1505.04597.pdf) for the original literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restart_kernel(restart=False):\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    if restart:\n",
    "        app.kernel.do_shutdown(True)\n",
    "\n",
    "restart_kernel(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Activation, Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "# \"tf\" assumes (rows, cols, channels) while \"th\" assumes (channels, rows, cols)\n",
    "# Possibly change this around natively in the data so the backend doesn't have to switch them\n",
    "# Only necessary if I use TF!\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pushbullet import Pushbullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run # 90\n"
     ]
    }
   ],
   "source": [
    "# Pushbullet notifier\n",
    "def push(title='Done!',text=''):\n",
    "    Pushbullet('o.YFPNNPfGRekivaCGHa4qMSgjZt8zJ6FL').devices[0].push_note(title,text)\n",
    "    \n",
    "# Import the training data\n",
    "def import_data(class_):\n",
    "    x = np.load('./data/x_augmented.npy','r')\n",
    "    y = np.load('./data/y_augmented.npy','r')\n",
    "    y_oneclass = y[:,class_:class_+1,...]\n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''    \n",
    "    return x, y, y_oneclass\n",
    "\n",
    "class_ = 4\n",
    "x, y, y_oneclass = import_data(class_)\n",
    "\n",
    "# Increment the counter\n",
    "def counter():\n",
    "    run = np.load('./data/run_counter.npy')\n",
    "    run += 1\n",
    "    np.save('./data/run_counter.npy',run)\n",
    "    return run\n",
    "run = counter()\n",
    "\n",
    "# Set the counter to a specific value\n",
    "def set_counter(run):\n",
    "    run = run\n",
    "    np.save('./data/run_counter.npy',run)\n",
    "    return run\n",
    "# Uncomment the next line to manually set the counter if something goes wrong\n",
    "run = set_counter(90)\n",
    "print('This is run # %i' %run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 s, sys: 43.2 ms, total: 1.98 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def compiler(img_rows = x.shape[2],img_cols = x.shape[3],\n",
    "            nfilters = 32,activation = 'relu',init = 'he_normal',\n",
    "            lr=1.0,decay=0.0,momentum=0.0, nesterov=False,reg=0.01,p=[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2]):\n",
    "    \n",
    "    def jaccard(y_true, y_pred,smooth=1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "    \n",
    "    def Conv2DReluBatchNorm(n_filter, w_filter, h_filter, inputs, activation, init='he_uniform',dropout=0.2):\n",
    "        # Batch norm after activation / leakyrelu\n",
    "        #return BatchNormalization(mode=2, axis=1)(LeakyReLU()((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "        # Batch norm before activation\n",
    "        #return LeakyReLU()(BatchNormalization(mode=0, axis=1)((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "        # Batch norm after activation / relu\n",
    "        return BatchNormalization(mode=2, axis=1)(Activation(activation=activation)((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "    def up_conv(nfilters,filter_factor,inputs,init=init,activation=activation):\n",
    "        # No batch norm\n",
    "        #return LeakyReLU()(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs)))\n",
    "        \n",
    "        # Batch norm after activation\n",
    "        #return BatchNormalization(mode=2, axis=1)(LeakyReLU()(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs))))\n",
    "        \n",
    "        # Batch norm after activation / relu\n",
    "        return BatchNormalization(mode=2, axis=1)(Activation(activation=activation)(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs))))\n",
    "\n",
    "    inputs = Input((20, img_rows, img_cols))\n",
    "    padded = ZeroPadding2D(padding=(12,12))(inputs)\n",
    "    \n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, padded, activation=activation,init=init)\n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, conv1, activation=activation,init=init)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(p=p[0])(pool1)\n",
    "\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, pool1, activation=activation,init=init)\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv2, activation=activation,init=init)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(p=p[1])(pool2)\n",
    "\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, pool2, activation=activation,init=init)\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv3, activation=activation,init=init)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(p=p[2])(pool3)\n",
    "\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, pool3, activation=activation,init=init)\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv4, activation=activation,init=init)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = Dropout(p=p[3])(pool4)\n",
    "\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, pool4, activation=activation,init=init)\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, conv5, activation=activation,init=init)\n",
    "    conv5 = Dropout(p=p[4])(conv5)\n",
    "        \n",
    "    up6 = merge([up_conv(nfilters,8,conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, up6, activation=activation,init=init)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv6, activation=activation,init=init)\n",
    "    conv6 = Dropout(p=p[5])(conv6)\n",
    "\n",
    "    up7 = merge([up_conv(nfilters,4,conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, up7, activation=activation,init=init)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv7, activation=activation,init=init)\n",
    "    conv7 = Dropout(p=p[6])(conv7)\n",
    "\n",
    "    up8 = merge([up_conv(nfilters,2,conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, up8, activation=activation,init=init)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv8, activation=activation,init=init)\n",
    "    conv8 = Dropout(p=p[7])(conv8)\n",
    "\n",
    "    up9 = merge([up_conv(nfilters,1,conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, up9, activation=activation,init=init)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, conv9, activation=activation,init=init)\n",
    "    conv9 = Dropout(p=p[8])(conv9)\n",
    "    \n",
    "    conv10 = Conv2DReluBatchNorm(1, 1, 1, conv9, activation='relu',init=init)\n",
    "    cropped = Cropping2D(cropping=((12,12), (12,12)))(conv10)\n",
    "    output = Activation(activation='sigmoid')(cropped)\n",
    "    \n",
    "    model = Model(input=inputs, output=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=lr,decay=decay), loss='binary_crossentropy', metrics=[jaccard])\n",
    "    \n",
    "    return model\n",
    "\n",
    "p=[0.1,0.2,0.3,0.4,0.5,0.4,0.3,0.2,0.1] # current version\n",
    "#p=[0.2,0.3,0.4,0.5,0.5,0.5,0.4,0.3,0.2] # symmetric but more dropout\n",
    "#p=[0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6] # increasing\n",
    "\n",
    "model = compiler(img_rows=x.shape[2],img_cols=x.shape[3],\n",
    "            nfilters=16,activation='relu',init='he_normal',\n",
    "            lr=0.001,decay=0,momentum=0,reg=0,p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run # 90\n",
      "Train on 320 samples, validate on 80 samples\n",
      "Epoch 1/1000\n",
      "320/320 [==============================] - 20s - loss: 0.6855 - jaccard: 0.2394 - val_loss: 0.6476 - val_jaccard: 0.2384\n",
      "Epoch 2/1000\n",
      "320/320 [==============================] - 20s - loss: 0.6290 - jaccard: 0.2609 - val_loss: 0.5999 - val_jaccard: 0.2982\n",
      "Epoch 3/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5840 - jaccard: 0.3100 - val_loss: 0.5889 - val_jaccard: 0.3187\n",
      "Epoch 4/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5671 - jaccard: 0.3210 - val_loss: 0.5598 - val_jaccard: 0.3356\n",
      "Epoch 5/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5468 - jaccard: 0.3438 - val_loss: 0.5409 - val_jaccard: 0.3541\n",
      "Epoch 6/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5411 - jaccard: 0.3447 - val_loss: 0.5311 - val_jaccard: 0.3584\n",
      "Epoch 7/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5300 - jaccard: 0.3560 - val_loss: 0.5199 - val_jaccard: 0.3672\n",
      "Epoch 8/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5261 - jaccard: 0.3579 - val_loss: 0.5157 - val_jaccard: 0.3642\n",
      "Epoch 9/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5195 - jaccard: 0.3622 - val_loss: 0.5097 - val_jaccard: 0.3750\n",
      "Epoch 10/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5162 - jaccard: 0.3614 - val_loss: 0.5092 - val_jaccard: 0.3738\n",
      "Epoch 11/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5147 - jaccard: 0.3689 - val_loss: 0.5095 - val_jaccard: 0.3726\n",
      "Epoch 12/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5085 - jaccard: 0.3704 - val_loss: 0.4995 - val_jaccard: 0.3801\n",
      "Epoch 13/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5034 - jaccard: 0.3718 - val_loss: 0.4952 - val_jaccard: 0.3801\n",
      "Epoch 14/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5006 - jaccard: 0.3746 - val_loss: 0.4912 - val_jaccard: 0.3878\n",
      "Epoch 15/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4979 - jaccard: 0.3771 - val_loss: 0.4910 - val_jaccard: 0.3856\n",
      "Epoch 16/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4957 - jaccard: 0.3809 - val_loss: 0.4911 - val_jaccard: 0.3874\n",
      "Epoch 17/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4972 - jaccard: 0.3809 - val_loss: 0.4876 - val_jaccard: 0.3857\n",
      "Epoch 18/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4982 - jaccard: 0.3770 - val_loss: 0.4859 - val_jaccard: 0.3885\n",
      "Epoch 19/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4896 - jaccard: 0.3837 - val_loss: 0.4817 - val_jaccard: 0.3905\n",
      "Epoch 20/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4833 - jaccard: 0.3905 - val_loss: 0.4776 - val_jaccard: 0.4010\n",
      "Epoch 21/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4815 - jaccard: 0.3928 - val_loss: 0.4758 - val_jaccard: 0.3985\n",
      "Epoch 22/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4765 - jaccard: 0.3939 - val_loss: 0.4744 - val_jaccard: 0.4062\n",
      "Epoch 23/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4776 - jaccard: 0.3970 - val_loss: 0.4709 - val_jaccard: 0.4026\n",
      "Epoch 24/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4816 - jaccard: 0.3938 - val_loss: 0.4779 - val_jaccard: 0.3946\n",
      "Epoch 25/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4773 - jaccard: 0.3937 - val_loss: 0.4701 - val_jaccard: 0.4069\n",
      "Epoch 26/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4748 - jaccard: 0.3995 - val_loss: 0.4689 - val_jaccard: 0.4053\n",
      "Epoch 27/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4721 - jaccard: 0.4001 - val_loss: 0.4673 - val_jaccard: 0.4068\n",
      "Epoch 28/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4764 - jaccard: 0.3969 - val_loss: 0.4709 - val_jaccard: 0.4031\n",
      "Epoch 29/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4702 - jaccard: 0.4021 - val_loss: 0.4662 - val_jaccard: 0.4097\n",
      "Epoch 30/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4697 - jaccard: 0.4027 - val_loss: 0.4629 - val_jaccard: 0.4170\n",
      "Epoch 31/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4693 - jaccard: 0.4060 - val_loss: 0.4644 - val_jaccard: 0.4067\n",
      "Epoch 32/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4666 - jaccard: 0.4029 - val_loss: 0.4619 - val_jaccard: 0.4165\n",
      "Epoch 33/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4661 - jaccard: 0.4069 - val_loss: 0.4587 - val_jaccard: 0.4155\n",
      "Epoch 34/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4601 - jaccard: 0.4097 - val_loss: 0.4575 - val_jaccard: 0.4175\n",
      "Epoch 35/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4606 - jaccard: 0.4120 - val_loss: 0.4570 - val_jaccard: 0.4146\n",
      "Epoch 36/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4582 - jaccard: 0.4129 - val_loss: 0.4560 - val_jaccard: 0.4170\n",
      "Epoch 37/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4557 - jaccard: 0.4141 - val_loss: 0.4538 - val_jaccard: 0.4239\n",
      "Epoch 38/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4602 - jaccard: 0.4155 - val_loss: 0.4593 - val_jaccard: 0.4127\n",
      "Epoch 39/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4602 - jaccard: 0.4097 - val_loss: 0.4553 - val_jaccard: 0.4247\n",
      "Epoch 40/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4610 - jaccard: 0.4140 - val_loss: 0.4573 - val_jaccard: 0.4198\n",
      "Epoch 41/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4612 - jaccard: 0.4096 - val_loss: 0.4548 - val_jaccard: 0.4227\n",
      "Epoch 42/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4566 - jaccard: 0.4175 - val_loss: 0.4531 - val_jaccard: 0.4241\n",
      "Epoch 43/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4547 - jaccard: 0.4176 - val_loss: 0.4502 - val_jaccard: 0.4321\n",
      "Epoch 44/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4526 - jaccard: 0.4219 - val_loss: 0.4480 - val_jaccard: 0.4307\n",
      "Epoch 45/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4534 - jaccard: 0.4178 - val_loss: 0.4488 - val_jaccard: 0.4356\n",
      "Epoch 46/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4491 - jaccard: 0.4239 - val_loss: 0.4465 - val_jaccard: 0.4305\n",
      "Epoch 47/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4468 - jaccard: 0.4259 - val_loss: 0.4448 - val_jaccard: 0.4367\n",
      "Epoch 48/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4460 - jaccard: 0.4264 - val_loss: 0.4447 - val_jaccard: 0.4316\n",
      "Epoch 49/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4488 - jaccard: 0.4210 - val_loss: 0.4444 - val_jaccard: 0.4343\n",
      "Epoch 50/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4455 - jaccard: 0.4282 - val_loss: 0.4430 - val_jaccard: 0.4346\n",
      "Epoch 51/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4430 - jaccard: 0.4288 - val_loss: 0.4418 - val_jaccard: 0.4398\n",
      "Epoch 52/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4432 - jaccard: 0.4299 - val_loss: 0.4410 - val_jaccard: 0.4361\n",
      "Epoch 53/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4424 - jaccard: 0.4321 - val_loss: 0.4431 - val_jaccard: 0.4330\n",
      "Epoch 54/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4439 - jaccard: 0.4261 - val_loss: 0.4414 - val_jaccard: 0.4452\n",
      "Epoch 55/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4502 - jaccard: 0.4213 - val_loss: 0.4500 - val_jaccard: 0.4392\n",
      "Epoch 56/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4588 - jaccard: 0.4172 - val_loss: 0.4509 - val_jaccard: 0.4325\n",
      "Epoch 57/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4515 - jaccard: 0.4242 - val_loss: 0.4481 - val_jaccard: 0.4397\n",
      "Epoch 58/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4458 - jaccard: 0.4293 - val_loss: 0.4416 - val_jaccard: 0.4354\n",
      "Epoch 59/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4426 - jaccard: 0.4282 - val_loss: 0.4411 - val_jaccard: 0.4424\n",
      "Epoch 60/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4442 - jaccard: 0.4294 - val_loss: 0.4405 - val_jaccard: 0.4382\n",
      "Epoch 61/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4409 - jaccard: 0.4326 - val_loss: 0.4389 - val_jaccard: 0.4391\n",
      "Epoch 62/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4400 - jaccard: 0.4306 - val_loss: 0.4389 - val_jaccard: 0.4452\n",
      "Epoch 63/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4418 - jaccard: 0.4303 - val_loss: 0.4399 - val_jaccard: 0.4443\n",
      "Epoch 64/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4377 - jaccard: 0.4366 - val_loss: 0.4367 - val_jaccard: 0.4418\n",
      "Epoch 65/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4371 - jaccard: 0.4352 - val_loss: 0.4354 - val_jaccard: 0.4461\n",
      "Epoch 66/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4355 - jaccard: 0.4379 - val_loss: 0.4353 - val_jaccard: 0.4415\n",
      "Epoch 67/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4347 - jaccard: 0.4363 - val_loss: 0.4345 - val_jaccard: 0.4510\n",
      "Epoch 68/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4339 - jaccard: 0.4387 - val_loss: 0.4331 - val_jaccard: 0.4492\n",
      "Epoch 69/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4332 - jaccard: 0.4394 - val_loss: 0.4326 - val_jaccard: 0.4509\n",
      "Epoch 70/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4324 - jaccard: 0.4424 - val_loss: 0.4328 - val_jaccard: 0.4428\n",
      "Epoch 71/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4334 - jaccard: 0.4371 - val_loss: 0.4368 - val_jaccard: 0.4548\n",
      "Epoch 72/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4361 - jaccard: 0.4364 - val_loss: 0.4366 - val_jaccard: 0.4389\n",
      "Epoch 73/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4383 - jaccard: 0.4345 - val_loss: 0.4348 - val_jaccard: 0.4551\n",
      "Epoch 74/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4320 - jaccard: 0.4448 - val_loss: 0.4319 - val_jaccard: 0.4539\n",
      "Epoch 75/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4323 - jaccard: 0.4404 - val_loss: 0.4323 - val_jaccard: 0.4458\n",
      "Epoch 76/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4301 - jaccard: 0.4431 - val_loss: 0.4318 - val_jaccard: 0.4563\n",
      "Epoch 77/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4282 - jaccard: 0.4439 - val_loss: 0.4292 - val_jaccard: 0.4514\n",
      "Epoch 78/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4287 - jaccard: 0.4442 - val_loss: 0.4293 - val_jaccard: 0.4513\n",
      "Epoch 79/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4505 - jaccard: 0.4284 - val_loss: 0.4498 - val_jaccard: 0.4215\n",
      "Epoch 80/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4418 - jaccard: 0.4349 - val_loss: 0.4369 - val_jaccard: 0.4449\n",
      "Epoch 81/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4355 - jaccard: 0.4384 - val_loss: 0.4324 - val_jaccard: 0.4490\n",
      "Epoch 82/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4304 - jaccard: 0.4448 - val_loss: 0.4292 - val_jaccard: 0.4487\n",
      "Epoch 83/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4280 - jaccard: 0.4449 - val_loss: 0.4277 - val_jaccard: 0.4536\n",
      "Epoch 84/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4266 - jaccard: 0.4473 - val_loss: 0.4272 - val_jaccard: 0.4565\n",
      "Epoch 85/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4253 - jaccard: 0.4487 - val_loss: 0.4272 - val_jaccard: 0.4497\n",
      "Epoch 86/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4252 - jaccard: 0.4461 - val_loss: 0.4265 - val_jaccard: 0.4598\n",
      "Epoch 87/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4239 - jaccard: 0.4512 - val_loss: 0.4253 - val_jaccard: 0.4573\n",
      "Epoch 88/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4229 - jaccard: 0.4497 - val_loss: 0.4253 - val_jaccard: 0.4570\n",
      "Epoch 89/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4228 - jaccard: 0.4491 - val_loss: 0.4249 - val_jaccard: 0.4587\n",
      "Epoch 90/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4225 - jaccard: 0.4513 - val_loss: 0.4254 - val_jaccard: 0.4587\n",
      "Epoch 91/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4230 - jaccard: 0.4518 - val_loss: 0.4243 - val_jaccard: 0.4620\n",
      "Epoch 92/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4205 - jaccard: 0.4556 - val_loss: 0.4236 - val_jaccard: 0.4584\n",
      "Epoch 93/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4194 - jaccard: 0.4551 - val_loss: 0.4225 - val_jaccard: 0.4609\n",
      "Epoch 94/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4204 - jaccard: 0.4530 - val_loss: 0.4229 - val_jaccard: 0.4663\n",
      "Epoch 95/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4232 - jaccard: 0.4454 - val_loss: 0.4254 - val_jaccard: 0.4682\n",
      "Epoch 96/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4214 - jaccard: 0.4580 - val_loss: 0.4235 - val_jaccard: 0.4629\n",
      "Epoch 97/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4252 - jaccard: 0.4496 - val_loss: 0.4251 - val_jaccard: 0.4542\n",
      "Epoch 98/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4189 - jaccard: 0.4572 - val_loss: 0.4219 - val_jaccard: 0.4610\n",
      "Epoch 99/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4176 - jaccard: 0.4552 - val_loss: 0.4204 - val_jaccard: 0.4636\n",
      "Epoch 100/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4180 - jaccard: 0.4563 - val_loss: 0.4203 - val_jaccard: 0.4617\n",
      "Epoch 101/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4173 - jaccard: 0.4571 - val_loss: 0.4213 - val_jaccard: 0.4553\n",
      "Epoch 102/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4171 - jaccard: 0.4564 - val_loss: 0.4193 - val_jaccard: 0.4658\n",
      "Epoch 103/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4156 - jaccard: 0.4584 - val_loss: 0.4193 - val_jaccard: 0.4703\n",
      "Epoch 104/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4151 - jaccard: 0.4604 - val_loss: 0.4176 - val_jaccard: 0.4660\n",
      "Epoch 105/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4147 - jaccard: 0.4593 - val_loss: 0.4178 - val_jaccard: 0.4668\n",
      "Epoch 106/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4138 - jaccard: 0.4575 - val_loss: 0.4174 - val_jaccard: 0.4674\n",
      "Epoch 107/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4132 - jaccard: 0.4625 - val_loss: 0.4166 - val_jaccard: 0.4718\n",
      "Epoch 108/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4135 - jaccard: 0.4566 - val_loss: 0.4167 - val_jaccard: 0.4697\n",
      "Epoch 109/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4114 - jaccard: 0.4652 - val_loss: 0.4165 - val_jaccard: 0.4631\n",
      "Epoch 110/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4119 - jaccard: 0.4618 - val_loss: 0.4168 - val_jaccard: 0.4708\n",
      "Epoch 111/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4133 - jaccard: 0.4618 - val_loss: 0.4176 - val_jaccard: 0.4703\n",
      "Epoch 112/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4119 - jaccard: 0.4612 - val_loss: 0.4154 - val_jaccard: 0.4666\n",
      "Epoch 113/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4109 - jaccard: 0.4632 - val_loss: 0.4152 - val_jaccard: 0.4714\n",
      "Epoch 114/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4091 - jaccard: 0.4678 - val_loss: 0.4155 - val_jaccard: 0.4671\n",
      "Epoch 115/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4105 - jaccard: 0.4628 - val_loss: 0.4197 - val_jaccard: 0.4506\n",
      "Epoch 116/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4122 - jaccard: 0.4611 - val_loss: 0.4179 - val_jaccard: 0.4692\n",
      "Epoch 117/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4128 - jaccard: 0.4660 - val_loss: 0.4160 - val_jaccard: 0.4651\n",
      "Epoch 118/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4105 - jaccard: 0.4638 - val_loss: 0.4162 - val_jaccard: 0.4773\n",
      "Epoch 119/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4102 - jaccard: 0.4672 - val_loss: 0.4136 - val_jaccard: 0.4689\n",
      "Epoch 120/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4072 - jaccard: 0.4658 - val_loss: 0.4130 - val_jaccard: 0.4711\n",
      "Epoch 121/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4065 - jaccard: 0.4680 - val_loss: 0.4104 - val_jaccard: 0.4743\n",
      "Epoch 122/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4057 - jaccard: 0.4677 - val_loss: 0.4106 - val_jaccard: 0.4728\n",
      "Epoch 123/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4106 - jaccard: 0.4614 - val_loss: 0.4210 - val_jaccard: 0.4663\n",
      "Epoch 124/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4133 - jaccard: 0.4619 - val_loss: 0.4184 - val_jaccard: 0.4712\n",
      "Epoch 125/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4162 - jaccard: 0.4598 - val_loss: 0.4232 - val_jaccard: 0.4694\n",
      "Epoch 126/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4224 - jaccard: 0.4528 - val_loss: 0.4248 - val_jaccard: 0.4583\n",
      "Epoch 127/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4156 - jaccard: 0.4618 - val_loss: 0.4174 - val_jaccard: 0.4664\n",
      "Epoch 128/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4106 - jaccard: 0.4637 - val_loss: 0.4129 - val_jaccard: 0.4688\n",
      "Epoch 129/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4059 - jaccard: 0.4670 - val_loss: 0.4104 - val_jaccard: 0.4750\n",
      "Epoch 130/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4022 - jaccard: 0.4728 - val_loss: 0.4083 - val_jaccard: 0.4811\n",
      "Epoch 131/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4013 - jaccard: 0.4722 - val_loss: 0.4075 - val_jaccard: 0.4799\n",
      "Epoch 132/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4020 - jaccard: 0.4722 - val_loss: 0.4080 - val_jaccard: 0.4728\n",
      "Epoch 133/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4017 - jaccard: 0.4754 - val_loss: 0.4082 - val_jaccard: 0.4837\n",
      "Epoch 134/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3999 - jaccard: 0.4724 - val_loss: 0.4066 - val_jaccard: 0.4767\n",
      "Epoch 135/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3983 - jaccard: 0.4755 - val_loss: 0.4062 - val_jaccard: 0.4756\n",
      "Epoch 136/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3979 - jaccard: 0.4789 - val_loss: 0.4051 - val_jaccard: 0.4788\n",
      "Epoch 137/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3979 - jaccard: 0.4756 - val_loss: 0.4047 - val_jaccard: 0.4840\n",
      "Epoch 138/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3957 - jaccard: 0.4796 - val_loss: 0.4034 - val_jaccard: 0.4831\n",
      "Epoch 139/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3967 - jaccard: 0.4797 - val_loss: 0.4045 - val_jaccard: 0.4866\n",
      "Epoch 140/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3951 - jaccard: 0.4788 - val_loss: 0.4037 - val_jaccard: 0.4851\n",
      "Epoch 141/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3943 - jaccard: 0.4811 - val_loss: 0.4031 - val_jaccard: 0.4825\n",
      "Epoch 142/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3937 - jaccard: 0.4824 - val_loss: 0.4030 - val_jaccard: 0.4881\n",
      "Epoch 143/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3951 - jaccard: 0.4799 - val_loss: 0.4056 - val_jaccard: 0.4896\n",
      "Epoch 144/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3953 - jaccard: 0.4802 - val_loss: 0.4049 - val_jaccard: 0.4862\n",
      "Epoch 145/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3939 - jaccard: 0.4826 - val_loss: 0.4014 - val_jaccard: 0.4866\n",
      "Epoch 146/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3913 - jaccard: 0.4834 - val_loss: 0.4003 - val_jaccard: 0.4865\n",
      "Epoch 147/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3907 - jaccard: 0.4839 - val_loss: 0.4015 - val_jaccard: 0.4920\n",
      "Epoch 148/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3909 - jaccard: 0.4854 - val_loss: 0.3999 - val_jaccard: 0.4882\n",
      "Epoch 149/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3900 - jaccard: 0.4859 - val_loss: 0.4000 - val_jaccard: 0.4837\n",
      "Epoch 150/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3893 - jaccard: 0.4886 - val_loss: 0.3995 - val_jaccard: 0.4890\n",
      "Epoch 151/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3894 - jaccard: 0.4851 - val_loss: 0.4001 - val_jaccard: 0.4845\n",
      "Epoch 152/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3904 - jaccard: 0.4866 - val_loss: 0.3993 - val_jaccard: 0.4867\n",
      "Epoch 153/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3908 - jaccard: 0.4856 - val_loss: 0.3994 - val_jaccard: 0.4841\n",
      "Epoch 154/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3908 - jaccard: 0.4866 - val_loss: 0.3989 - val_jaccard: 0.4878\n",
      "Epoch 155/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3893 - jaccard: 0.4868 - val_loss: 0.3992 - val_jaccard: 0.4856\n",
      "Epoch 156/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3876 - jaccard: 0.4889 - val_loss: 0.3977 - val_jaccard: 0.4873\n",
      "Epoch 157/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3869 - jaccard: 0.4880 - val_loss: 0.3964 - val_jaccard: 0.4938\n",
      "Epoch 158/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3854 - jaccard: 0.4898 - val_loss: 0.3958 - val_jaccard: 0.4917\n",
      "Epoch 159/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3854 - jaccard: 0.4908 - val_loss: 0.3948 - val_jaccard: 0.4945\n",
      "Epoch 160/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3850 - jaccard: 0.4910 - val_loss: 0.3951 - val_jaccard: 0.4909\n",
      "Epoch 161/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3847 - jaccard: 0.4926 - val_loss: 0.3955 - val_jaccard: 0.4919\n",
      "Epoch 162/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3843 - jaccard: 0.4927 - val_loss: 0.3952 - val_jaccard: 0.4920\n",
      "Epoch 163/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3843 - jaccard: 0.4903 - val_loss: 0.3958 - val_jaccard: 0.4997\n",
      "Epoch 164/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3841 - jaccard: 0.4933 - val_loss: 0.3962 - val_jaccard: 0.4911\n",
      "Epoch 165/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3830 - jaccard: 0.4951 - val_loss: 0.3940 - val_jaccard: 0.4940\n",
      "Epoch 166/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3819 - jaccard: 0.4964 - val_loss: 0.3945 - val_jaccard: 0.4945\n",
      "Epoch 167/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3822 - jaccard: 0.4948 - val_loss: 0.3932 - val_jaccard: 0.4991\n",
      "Epoch 168/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3815 - jaccard: 0.4968 - val_loss: 0.3935 - val_jaccard: 0.4948\n",
      "Epoch 169/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3815 - jaccard: 0.4931 - val_loss: 0.3936 - val_jaccard: 0.5006\n",
      "Epoch 170/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3842 - jaccard: 0.4890 - val_loss: 0.3940 - val_jaccard: 0.4977\n",
      "Epoch 171/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3860 - jaccard: 0.4898 - val_loss: 0.3988 - val_jaccard: 0.4972\n",
      "Epoch 172/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3926 - jaccard: 0.4838 - val_loss: 0.4000 - val_jaccard: 0.4863\n",
      "Epoch 173/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3890 - jaccard: 0.4880 - val_loss: 0.3984 - val_jaccard: 0.4876\n",
      "Epoch 174/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3851 - jaccard: 0.4880 - val_loss: 0.3983 - val_jaccard: 0.5045\n",
      "Epoch 175/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3847 - jaccard: 0.4928 - val_loss: 0.3930 - val_jaccard: 0.4944\n",
      "Epoch 176/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3821 - jaccard: 0.4940 - val_loss: 0.3929 - val_jaccard: 0.4898\n",
      "Epoch 177/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3803 - jaccard: 0.4964 - val_loss: 0.3931 - val_jaccard: 0.5023\n",
      "Epoch 178/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3807 - jaccard: 0.4951 - val_loss: 0.3929 - val_jaccard: 0.5001\n",
      "Epoch 179/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3787 - jaccard: 0.4975 - val_loss: 0.3907 - val_jaccard: 0.4987\n",
      "Epoch 180/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3777 - jaccard: 0.4996 - val_loss: 0.3907 - val_jaccard: 0.5018\n",
      "Epoch 181/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3781 - jaccard: 0.4985 - val_loss: 0.3894 - val_jaccard: 0.4999\n",
      "Epoch 182/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3780 - jaccard: 0.4999 - val_loss: 0.3916 - val_jaccard: 0.5050\n",
      "Epoch 183/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3778 - jaccard: 0.4993 - val_loss: 0.3892 - val_jaccard: 0.4998\n",
      "Epoch 184/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3768 - jaccard: 0.4998 - val_loss: 0.3890 - val_jaccard: 0.5040\n",
      "Epoch 185/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3762 - jaccard: 0.5004 - val_loss: 0.3888 - val_jaccard: 0.4991\n",
      "Epoch 186/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3758 - jaccard: 0.5007 - val_loss: 0.3883 - val_jaccard: 0.5004\n",
      "Epoch 187/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3753 - jaccard: 0.5002 - val_loss: 0.3883 - val_jaccard: 0.5053\n",
      "Epoch 188/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3741 - jaccard: 0.5039 - val_loss: 0.3882 - val_jaccard: 0.5031\n",
      "Epoch 189/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3754 - jaccard: 0.4999 - val_loss: 0.3889 - val_jaccard: 0.5003\n",
      "Epoch 190/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3747 - jaccard: 0.5024 - val_loss: 0.3877 - val_jaccard: 0.5015\n",
      "Epoch 191/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3743 - jaccard: 0.5046 - val_loss: 0.3885 - val_jaccard: 0.5017\n",
      "Epoch 192/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3750 - jaccard: 0.4991 - val_loss: 0.3875 - val_jaccard: 0.5037\n",
      "Epoch 193/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3740 - jaccard: 0.5067 - val_loss: 0.3900 - val_jaccard: 0.4986\n",
      "Epoch 194/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3764 - jaccard: 0.4979 - val_loss: 0.3886 - val_jaccard: 0.5077\n",
      "Epoch 195/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3747 - jaccard: 0.5043 - val_loss: 0.3879 - val_jaccard: 0.5034\n",
      "Epoch 196/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3737 - jaccard: 0.5060 - val_loss: 0.3871 - val_jaccard: 0.5022\n",
      "Epoch 197/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3732 - jaccard: 0.5039 - val_loss: 0.3870 - val_jaccard: 0.5080\n",
      "Epoch 198/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3728 - jaccard: 0.5053 - val_loss: 0.3892 - val_jaccard: 0.5122\n",
      "Epoch 199/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3719 - jaccard: 0.5077 - val_loss: 0.3870 - val_jaccard: 0.5024\n",
      "Epoch 200/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3727 - jaccard: 0.5040 - val_loss: 0.3865 - val_jaccard: 0.5092\n",
      "Epoch 201/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3710 - jaccard: 0.5072 - val_loss: 0.3860 - val_jaccard: 0.5089\n",
      "Epoch 202/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3720 - jaccard: 0.5079 - val_loss: 0.3852 - val_jaccard: 0.5055\n",
      "Epoch 203/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3774 - jaccard: 0.4989 - val_loss: 0.3947 - val_jaccard: 0.5037\n",
      "Epoch 204/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3781 - jaccard: 0.5039 - val_loss: 0.3899 - val_jaccard: 0.4978\n",
      "Epoch 205/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3781 - jaccard: 0.5003 - val_loss: 0.3908 - val_jaccard: 0.5010\n",
      "Epoch 206/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3740 - jaccard: 0.5035 - val_loss: 0.3868 - val_jaccard: 0.5088\n",
      "Epoch 207/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3712 - jaccard: 0.5051 - val_loss: 0.3853 - val_jaccard: 0.5104\n",
      "Epoch 208/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3703 - jaccard: 0.5080 - val_loss: 0.3851 - val_jaccard: 0.5052\n",
      "Epoch 209/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3704 - jaccard: 0.5082 - val_loss: 0.3859 - val_jaccard: 0.5040\n",
      "Epoch 210/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3746 - jaccard: 0.5017 - val_loss: 0.3873 - val_jaccard: 0.5113\n",
      "Epoch 211/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3705 - jaccard: 0.5089 - val_loss: 0.3849 - val_jaccard: 0.5085\n",
      "Epoch 212/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3696 - jaccard: 0.5093 - val_loss: 0.3846 - val_jaccard: 0.5061\n",
      "Epoch 213/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3687 - jaccard: 0.5071 - val_loss: 0.3839 - val_jaccard: 0.5099\n",
      "Epoch 214/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3686 - jaccard: 0.5090 - val_loss: 0.3837 - val_jaccard: 0.5106\n",
      "Epoch 215/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3677 - jaccard: 0.5100 - val_loss: 0.3839 - val_jaccard: 0.5107\n",
      "Epoch 216/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3687 - jaccard: 0.5096 - val_loss: 0.3845 - val_jaccard: 0.5084\n",
      "Epoch 217/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3681 - jaccard: 0.5109 - val_loss: 0.3835 - val_jaccard: 0.5082\n",
      "Epoch 218/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3687 - jaccard: 0.5073 - val_loss: 0.3839 - val_jaccard: 0.5137\n",
      "Epoch 219/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3680 - jaccard: 0.5090 - val_loss: 0.3858 - val_jaccard: 0.5164\n",
      "Epoch 220/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3683 - jaccard: 0.5134 - val_loss: 0.3837 - val_jaccard: 0.5079\n",
      "Epoch 221/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3688 - jaccard: 0.5078 - val_loss: 0.3829 - val_jaccard: 0.5066\n",
      "Epoch 222/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3672 - jaccard: 0.5136 - val_loss: 0.3836 - val_jaccard: 0.5076\n",
      "Epoch 223/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3667 - jaccard: 0.5123 - val_loss: 0.3826 - val_jaccard: 0.5044\n",
      "Epoch 224/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3678 - jaccard: 0.5106 - val_loss: 0.3830 - val_jaccard: 0.5076\n",
      "Epoch 225/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3662 - jaccard: 0.5122 - val_loss: 0.3825 - val_jaccard: 0.5137\n",
      "Epoch 226/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3662 - jaccard: 0.5125 - val_loss: 0.3828 - val_jaccard: 0.5120\n",
      "Epoch 227/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3660 - jaccard: 0.5127 - val_loss: 0.3823 - val_jaccard: 0.5101\n",
      "Epoch 228/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3652 - jaccard: 0.5137 - val_loss: 0.3828 - val_jaccard: 0.5132\n",
      "Epoch 229/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3677 - jaccard: 0.5092 - val_loss: 0.3848 - val_jaccard: 0.5047\n",
      "Epoch 230/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3671 - jaccard: 0.5145 - val_loss: 0.3832 - val_jaccard: 0.5145\n",
      "Epoch 231/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3709 - jaccard: 0.5076 - val_loss: 0.3856 - val_jaccard: 0.5124\n",
      "Epoch 232/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3674 - jaccard: 0.5116 - val_loss: 0.3821 - val_jaccard: 0.5113\n",
      "Epoch 233/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3651 - jaccard: 0.5128 - val_loss: 0.3810 - val_jaccard: 0.5128\n",
      "Epoch 234/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3656 - jaccard: 0.5129 - val_loss: 0.3823 - val_jaccard: 0.5139\n",
      "Epoch 235/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3646 - jaccard: 0.5136 - val_loss: 0.3815 - val_jaccard: 0.5132\n",
      "Epoch 236/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3642 - jaccard: 0.5158 - val_loss: 0.3813 - val_jaccard: 0.5122\n",
      "Epoch 237/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3648 - jaccard: 0.5133 - val_loss: 0.3815 - val_jaccard: 0.5102\n",
      "Epoch 238/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3645 - jaccard: 0.5139 - val_loss: 0.3807 - val_jaccard: 0.5138\n",
      "Epoch 239/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3637 - jaccard: 0.5178 - val_loss: 0.3812 - val_jaccard: 0.5083\n",
      "Epoch 240/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3632 - jaccard: 0.5154 - val_loss: 0.3815 - val_jaccard: 0.5173\n",
      "Epoch 241/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3621 - jaccard: 0.5182 - val_loss: 0.3799 - val_jaccard: 0.5164\n",
      "Epoch 242/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3617 - jaccard: 0.5172 - val_loss: 0.3794 - val_jaccard: 0.5125\n",
      "Epoch 243/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3616 - jaccard: 0.5180 - val_loss: 0.3815 - val_jaccard: 0.5113\n",
      "Epoch 244/1000\n",
      " 60/320 [====>.........................] - ETA: 14s - loss: 0.3464 - jaccard: 0.5339"
     ]
    }
   ],
   "source": [
    "def trainer(model,fit=True,use_existing=False):\n",
    "    print('This is run # %i' %run)\n",
    "    \n",
    "    if use_existing:\n",
    "        model.load_weights('./data/model_weights_class_{}_run_{}.hdf5'.format(_class,run))\n",
    "        \n",
    "    if fit:\n",
    "        quitter = EarlyStopping(monitor='loss', min_delta=0.001, patience=100, verbose=1, mode='auto')\n",
    "        lrreducer = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, verbose=1, mode='auto', epsilon=0.001, cooldown=2, min_lr=0)\n",
    "        model_checkpoint = ModelCheckpoint('./data/model_weights_class_{}_run_{}.hdf5'.format(class_,run), monitor='loss', save_best_only=True)\n",
    "        csvlogger = CSVLogger('./data/training_log_run_'+str(run), separator=',', append=True)\n",
    "\n",
    "        model.fit(x, y_oneclass,\n",
    "                  batch_size=20,\n",
    "                  nb_epoch=1000,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  callbacks=[model_checkpoint,csvlogger],\n",
    "                  validation_split=0.2,\n",
    "                  initial_epoch=0)\n",
    "            \n",
    "    preds = model.predict(x, verbose=1)\n",
    "    np.save('preds.npy', preds)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = trainer(model,fit=True,use_existing=False)\n",
    "model.save('u-net-complete-model-run_{}_class_{}.h5'.format(run,class_))\n",
    "push('Training is done',\n",
    "     'Train loss: %f, train jaccard: %f, val loss %f, val jaccard%f' %(model.history.history['loss'][-1],model.history.history['jaccard'][-1],model.history.history['val_loss'][-1],model.history.history['val_jaccard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
