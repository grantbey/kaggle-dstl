{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and thoughts\n",
    "\n",
    "#### Current best ideas / runs that work / etc.\n",
    "- Batch size = ~1/6th of training data size seems to make all the difference\n",
    "- nfilters is only 4 for a single class (perhaps 8 works better? Takes longer to train. Trade-offs...)\n",
    "- Perhaps the best method is to train 10 models, one for each class\n",
    "- Dropout after each maxpooling step in the down-path and after each upconv/conv/conv step in the up-path. I used p=0.2 for each dropout layer, however the code is written such that this can be changed ona  per-layer basis.\n",
    "- Activation = LeakyReLu\n",
    "- Batchnorm is *after* the activation\n",
    "- L2 reg is applied to each convolution with a lambda of 0.00001. Question remains: is this even doing anything?\n",
    "- lr is 0.001. This is the default for Adam. Would a higher value train faster?\n",
    "- Currently there are 400 training images\n",
    "    - Could this be reduced?\n",
    "    - Currently the data augmentation is applied to the entire data set, then it is randomly split for validation\n",
    "    - Should the test/val sets be split early and *then* apply data augmentation *seperately*?\n",
    "    - **NB** be aware that some classes only have a single image. Thus, the training/val sets should each contain examples of this class but it should be heavily augmented so as to be treated as different)\n",
    "\n",
    "#### Ideas\n",
    "- Weight map: sum the total area of all classes in y, then calculate each class' proportion of the total and use `1-value` in place of 1 in the binary mask. This will cause low frequency classes to contribute more to the total loss, i.e. penalizing the model when it fails to predict low frequency classes.\n",
    "\n",
    "#### Data augmentation / image manipulation\n",
    "- [Histogram Equalization](http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html#sphx-glr-auto-examples-color-exposure-plot-equalize-py) (also see [here](https://www.kaggle.com/gabrielaltay/dstl-satellite-imagery-feature-detection/exploring-color-scaling-images/discussion), [here](http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/) and [here](https://medium.com/@vivek.yadav/improved-performance-of-deep-learning-neural-network-models-on-traffic-sign-classification-using-6355346da2dc#.x9nidcsh6))\n",
    "- Rotation (with reflection): see data-augmentation.ipynb\n",
    "- Image normalization: see image-preprocessing-new.ipynb\n",
    "\n",
    "#### Overfitting solutions\n",
    "- CNNs are supposed to be more robust to this because of the shared weight matrix of each filter\n",
    "- **Data augmentation!**\n",
    "    - Have done random rotations on data increasing total n by 6-fold\n",
    "    - Not seeing drastic improvements\n",
    "- L2 regularization (added into the layers via `W_regularizer=l2(l=0.01)` parameter)\n",
    "    - Not seeing much improvement\n",
    "    - Currently set to 0.00001\n",
    "- Move batchnorm to *before* the relu takes place (see [here](http://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras)\n",
    "- Add batchnorm to upconv() layer\n",
    "- Try mode=1 in batchnorm\n",
    "- Dropout hurts the model in my experience..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U-Net architecture\n",
    "\n",
    "See [here](https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py#L19) for code and [here](https://arxiv.org/pdf/1505.04597.pdf) for the original literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? \n",
      "Nothing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Activation, Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "# \"tf\" assumes (rows, cols, channels) while \"th\" assumes (channels, rows, cols)\n",
    "# Possibly change this around natively in the data so the backend doesn't have to switch them\n",
    "# Only necessary if I use TF!\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_preds(preds):\n",
    "    for i in range(preds.shape[0]):\n",
    "        preds[i,0,:,:] = (preds[i,0,:,:].min() - preds[i,0,:,:])/(preds[i,0,:,:].min()-preds[i,0,:,:].max())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_all(i,classType):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.imshow(preds[i,0,...],cmap='spectral')\n",
    "\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.imshow(np.rint(preds[i,0,...]),cmap='spectral')\n",
    "\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax3.imshow(x[i,17,...],cmap='Greys')\n",
    "\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    ax4.imshow(y_oneclass[i,classType,:,:],cmap='spectral')\n",
    "    \n",
    "    plt.show()\n",
    "#plot_all(2,2,classType,0.5)\n",
    "#push('PICTURES!','The plots are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the components of the pushbullet API\n",
    "from pushbullet import Pushbullet\n",
    "\n",
    "pb = Pushbullet('o.YFPNNPfGRekivaCGHa4qMSgjZt8zJ6FL')\n",
    "phone = pb.devices[0]\n",
    "\n",
    "# Run this cell after anything you want to be notified about!\n",
    "def push(title='Done!',text='Whatever it was, it\\'s done'):\n",
    "    phone.push_note(title,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the training data\n",
    "def import_data():\n",
    "    x = np.load('./data/x_augmented.npy','r')\n",
    "    y = np.load('./data/y_augmented.npy','r')\n",
    "    \n",
    "    '''with open('./data/x_resized_array.pickle','rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        x = x.astype(np.float32)\n",
    "        \n",
    "    with open('./data/y_resized_raster.pickle','rb') as f:\n",
    "        y = pickle.load(f)\n",
    "        y = y.astype(np.float32)'''\n",
    "    \n",
    "    y_oneclass = y[:,3:4,...]\n",
    "    \n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''\n",
    "    \n",
    "    return x, y, y_oneclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run = 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, y_oneclass = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 s, sys: 34.7 ms, total: 1.8 s\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "V2.0 U-Net with batchnorm\n",
    "'''\n",
    "def builder(img_rows = x.shape[2],img_cols = x.shape[3],\n",
    "            nfilters = 32,activation = 'relu',init = 'he_normal',\n",
    "            lr=1.0,decay=0.0,momentum=0.0, nesterov=False,reg=0.01,p=[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2]):\n",
    "    \n",
    "    def jaccard(y_true, y_pred,smooth=1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "    \n",
    "    def Conv2DReluBatchNorm(n_filter, w_filter, h_filter, inputs, activation, init='he_uniform',dropout=0.2):\n",
    "        # Batch norm after activation\n",
    "        return BatchNormalization(mode=2, axis=1)(LeakyReLU()((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "        # Batch norm before activation\n",
    "        #return LeakyReLU()(BatchNormalization(mode=0, axis=1)((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "\n",
    "    def up_conv(nfilters,filter_factor,inputs,init=init,activation=activation):\n",
    "        # No batch norm\n",
    "        #return LeakyReLU()(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs)))\n",
    "        \n",
    "        # Batch norm after activation\n",
    "        return BatchNormalization(mode=2, axis=1)(LeakyReLU()(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs))))\n",
    "\n",
    "    inputs = Input((20, img_rows, img_cols))\n",
    "    padded = ZeroPadding2D(padding=(12,12))(inputs)\n",
    "    \n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, padded, activation=activation,init=init)\n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, conv1, activation=activation,init=init)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(p=p[0])(pool1)\n",
    "\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, pool1, activation=activation,init=init)\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv2, activation=activation,init=init)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(p=p[1])(pool2)\n",
    "\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, pool2, activation=activation,init=init)\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv3, activation=activation,init=init)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(p=p[2])(pool3)\n",
    "\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, pool3, activation=activation,init=init)\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv4, activation=activation,init=init)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = Dropout(p=p[3])(pool4)\n",
    "\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, pool4, activation=activation,init=init)\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, conv5, activation=activation,init=init)\n",
    "    conv5 = Dropout(p=p[4])(conv5)\n",
    "        \n",
    "    up6 = merge([up_conv(nfilters,8,conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, up6, activation=activation,init=init)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv6, activation=activation,init=init)\n",
    "    conv6 = Dropout(p=p[5])(conv6)\n",
    "\n",
    "    up7 = merge([up_conv(nfilters,4,conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, up7, activation=activation,init=init)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv7, activation=activation,init=init)\n",
    "    conv7 = Dropout(p=p[6])(conv7)\n",
    "\n",
    "    up8 = merge([up_conv(nfilters,2,conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, up8, activation=activation,init=init)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv8, activation=activation,init=init)\n",
    "    conv8 = Dropout(p=p[7])(conv8)\n",
    "\n",
    "    up9 = merge([up_conv(nfilters,1,conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, up9, activation=activation,init=init)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, conv9, activation=activation,init=init)\n",
    "    conv9 = Dropout(p=p[8])(conv9)\n",
    "    \n",
    "    conv10 = Conv2DReluBatchNorm(1, 1, 1, conv9, activation='relu',init=init)\n",
    "    cropped = Cropping2D(cropping=((12,12), (12,12)))(conv10)\n",
    "    output = Activation(activation='sigmoid')(cropped)\n",
    "    \n",
    "    model = Model(input=inputs, output=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=lr,decay=decay), loss='binary_crossentropy', metrics=[jaccard])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = builder(img_rows=x.shape[2],img_cols=x.shape[3],\n",
    "            nfilters=4,activation='relu',init='he_normal',\n",
    "            lr=0.001,decay=0,momentum=0,reg=0.00001,p=[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2])\n",
    "\n",
    "#push('The model is compiled','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run number: 82...\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Train on 320 samples, validate on 80 samples\n",
      "Epoch 1/1000\n",
      "320/320 [==============================] - 6s - loss: 0.7416 - jaccard: 0.0966 - val_loss: 0.6398 - val_jaccard: 0.0966\n",
      "Epoch 2/1000\n",
      "320/320 [==============================] - 6s - loss: 0.6401 - jaccard: 0.0952 - val_loss: 0.5735 - val_jaccard: 0.0963\n",
      "Epoch 3/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5894 - jaccard: 0.0946 - val_loss: 0.5413 - val_jaccard: 0.0953\n",
      "Epoch 4/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5618 - jaccard: 0.0939 - val_loss: 0.5265 - val_jaccard: 0.0951\n",
      "Epoch 5/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5458 - jaccard: 0.0941 - val_loss: 0.5192 - val_jaccard: 0.0954\n",
      "Epoch 6/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5363 - jaccard: 0.0936 - val_loss: 0.5148 - val_jaccard: 0.0955\n",
      "Epoch 7/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5300 - jaccard: 0.0937 - val_loss: 0.5114 - val_jaccard: 0.0955\n",
      "Epoch 8/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5251 - jaccard: 0.0938 - val_loss: 0.5085 - val_jaccard: 0.0958\n",
      "Epoch 9/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5213 - jaccard: 0.0940 - val_loss: 0.5058 - val_jaccard: 0.0964\n",
      "Epoch 10/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5178 - jaccard: 0.0946 - val_loss: 0.5033 - val_jaccard: 0.0970\n",
      "Epoch 11/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5149 - jaccard: 0.0950 - val_loss: 0.5007 - val_jaccard: 0.0977\n",
      "Epoch 12/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5118 - jaccard: 0.0956 - val_loss: 0.4982 - val_jaccard: 0.0984\n",
      "Epoch 13/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5091 - jaccard: 0.0961 - val_loss: 0.4954 - val_jaccard: 0.0995\n",
      "Epoch 14/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5060 - jaccard: 0.0969 - val_loss: 0.4929 - val_jaccard: 0.0998\n",
      "Epoch 15/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5037 - jaccard: 0.0969 - val_loss: 0.4903 - val_jaccard: 0.1004\n",
      "Epoch 16/1000\n",
      "320/320 [==============================] - 6s - loss: 0.5009 - jaccard: 0.0981 - val_loss: 0.4878 - val_jaccard: 0.1018\n",
      "Epoch 17/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4983 - jaccard: 0.0992 - val_loss: 0.4857 - val_jaccard: 0.1018\n",
      "Epoch 18/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4957 - jaccard: 0.0995 - val_loss: 0.4831 - val_jaccard: 0.1027\n",
      "Epoch 19/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4931 - jaccard: 0.1006 - val_loss: 0.4807 - val_jaccard: 0.1041\n",
      "Epoch 20/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4908 - jaccard: 0.1016 - val_loss: 0.4785 - val_jaccard: 0.1037\n",
      "Epoch 21/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4883 - jaccard: 0.1019 - val_loss: 0.4759 - val_jaccard: 0.1061\n",
      "Epoch 22/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4859 - jaccard: 0.1036 - val_loss: 0.4737 - val_jaccard: 0.1060\n",
      "Epoch 23/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4839 - jaccard: 0.1034 - val_loss: 0.4713 - val_jaccard: 0.1073\n",
      "Epoch 24/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4812 - jaccard: 0.1053 - val_loss: 0.4697 - val_jaccard: 0.1072\n",
      "Epoch 25/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4795 - jaccard: 0.1043 - val_loss: 0.4675 - val_jaccard: 0.1075\n",
      "Epoch 26/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4773 - jaccard: 0.1061 - val_loss: 0.4653 - val_jaccard: 0.1095\n",
      "Epoch 27/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4751 - jaccard: 0.1060 - val_loss: 0.4638 - val_jaccard: 0.1081\n",
      "Epoch 28/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4733 - jaccard: 0.1063 - val_loss: 0.4615 - val_jaccard: 0.1106\n",
      "Epoch 29/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4711 - jaccard: 0.1075 - val_loss: 0.4599 - val_jaccard: 0.1098\n",
      "Epoch 30/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4693 - jaccard: 0.1075 - val_loss: 0.4579 - val_jaccard: 0.1113\n",
      "Epoch 31/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4674 - jaccard: 0.1077 - val_loss: 0.4567 - val_jaccard: 0.1106\n",
      "Epoch 32/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4657 - jaccard: 0.1084 - val_loss: 0.4543 - val_jaccard: 0.1124\n",
      "Epoch 33/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4640 - jaccard: 0.1085 - val_loss: 0.4535 - val_jaccard: 0.1108\n",
      "Epoch 34/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4621 - jaccard: 0.1087 - val_loss: 0.4516 - val_jaccard: 0.1135\n",
      "Epoch 35/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4604 - jaccard: 0.1095 - val_loss: 0.4499 - val_jaccard: 0.1121\n",
      "Epoch 36/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4586 - jaccard: 0.1091 - val_loss: 0.4481 - val_jaccard: 0.1141\n",
      "Epoch 37/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4570 - jaccard: 0.1108 - val_loss: 0.4466 - val_jaccard: 0.1131\n",
      "Epoch 38/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4550 - jaccard: 0.1100 - val_loss: 0.4449 - val_jaccard: 0.1141\n",
      "Epoch 39/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4538 - jaccard: 0.1109 - val_loss: 0.4434 - val_jaccard: 0.1141\n",
      "Epoch 40/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4520 - jaccard: 0.1107 - val_loss: 0.4425 - val_jaccard: 0.1146\n",
      "Epoch 41/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4508 - jaccard: 0.1106 - val_loss: 0.4403 - val_jaccard: 0.1146\n",
      "Epoch 42/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4487 - jaccard: 0.1114 - val_loss: 0.4387 - val_jaccard: 0.1169\n",
      "Epoch 43/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4473 - jaccard: 0.1124 - val_loss: 0.4371 - val_jaccard: 0.1155\n",
      "Epoch 44/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4455 - jaccard: 0.1121 - val_loss: 0.4357 - val_jaccard: 0.1159\n",
      "Epoch 45/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4444 - jaccard: 0.1122 - val_loss: 0.4347 - val_jaccard: 0.1175\n",
      "Epoch 46/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4424 - jaccard: 0.1135 - val_loss: 0.4329 - val_jaccard: 0.1165\n",
      "Epoch 47/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4408 - jaccard: 0.1132 - val_loss: 0.4314 - val_jaccard: 0.1180\n",
      "Epoch 48/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4392 - jaccard: 0.1141 - val_loss: 0.4301 - val_jaccard: 0.1167\n",
      "Epoch 49/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4380 - jaccard: 0.1142 - val_loss: 0.4291 - val_jaccard: 0.1182\n",
      "Epoch 50/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4363 - jaccard: 0.1144 - val_loss: 0.4271 - val_jaccard: 0.1184\n",
      "Epoch 51/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4359 - jaccard: 0.1138 - val_loss: 0.4253 - val_jaccard: 0.1187\n",
      "Epoch 52/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4334 - jaccard: 0.1140 - val_loss: 0.4244 - val_jaccard: 0.1194\n",
      "Epoch 53/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4320 - jaccard: 0.1165 - val_loss: 0.4237 - val_jaccard: 0.1185\n",
      "Epoch 54/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4306 - jaccard: 0.1151 - val_loss: 0.4221 - val_jaccard: 0.1188\n",
      "Epoch 55/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4292 - jaccard: 0.1168 - val_loss: 0.4208 - val_jaccard: 0.1208\n",
      "Epoch 56/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4280 - jaccard: 0.1155 - val_loss: 0.4191 - val_jaccard: 0.1195\n",
      "Epoch 57/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4262 - jaccard: 0.1169 - val_loss: 0.4184 - val_jaccard: 0.1207\n",
      "Epoch 58/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4249 - jaccard: 0.1178 - val_loss: 0.4161 - val_jaccard: 0.1210\n",
      "Epoch 59/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4239 - jaccard: 0.1174 - val_loss: 0.4159 - val_jaccard: 0.1192\n",
      "Epoch 60/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4239 - jaccard: 0.1148 - val_loss: 0.4138 - val_jaccard: 0.1225\n",
      "Epoch 61/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4213 - jaccard: 0.1175 - val_loss: 0.4127 - val_jaccard: 0.1200\n",
      "Epoch 62/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4200 - jaccard: 0.1180 - val_loss: 0.4115 - val_jaccard: 0.1235\n",
      "Epoch 63/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4188 - jaccard: 0.1187 - val_loss: 0.4109 - val_jaccard: 0.1207\n",
      "Epoch 64/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4174 - jaccard: 0.1174 - val_loss: 0.4093 - val_jaccard: 0.1224\n",
      "Epoch 65/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4160 - jaccard: 0.1198 - val_loss: 0.4080 - val_jaccard: 0.1230\n",
      "Epoch 66/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4148 - jaccard: 0.1189 - val_loss: 0.4064 - val_jaccard: 0.1226\n",
      "Epoch 67/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4136 - jaccard: 0.1184 - val_loss: 0.4059 - val_jaccard: 0.1229\n",
      "Epoch 68/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4121 - jaccard: 0.1208 - val_loss: 0.4043 - val_jaccard: 0.1240\n",
      "Epoch 69/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4110 - jaccard: 0.1202 - val_loss: 0.4033 - val_jaccard: 0.1231\n",
      "Epoch 70/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4105 - jaccard: 0.1184 - val_loss: 0.4015 - val_jaccard: 0.1255\n",
      "Epoch 71/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4093 - jaccard: 0.1209 - val_loss: 0.4020 - val_jaccard: 0.1221\n",
      "Epoch 72/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4085 - jaccard: 0.1194 - val_loss: 0.4001 - val_jaccard: 0.1254\n",
      "Epoch 73/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4066 - jaccard: 0.1204 - val_loss: 0.3991 - val_jaccard: 0.1224\n",
      "Epoch 74/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4053 - jaccard: 0.1205 - val_loss: 0.3975 - val_jaccard: 0.1267\n",
      "Epoch 75/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4043 - jaccard: 0.1216 - val_loss: 0.3966 - val_jaccard: 0.1255\n",
      "Epoch 76/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4032 - jaccard: 0.1222 - val_loss: 0.3950 - val_jaccard: 0.1258\n",
      "Epoch 77/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4019 - jaccard: 0.1206 - val_loss: 0.3945 - val_jaccard: 0.1246\n",
      "Epoch 78/1000\n",
      "320/320 [==============================] - 6s - loss: 0.4009 - jaccard: 0.1215 - val_loss: 0.3934 - val_jaccard: 0.1272\n",
      "Epoch 79/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3993 - jaccard: 0.1236 - val_loss: 0.3923 - val_jaccard: 0.1251\n",
      "Epoch 80/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3983 - jaccard: 0.1224 - val_loss: 0.3907 - val_jaccard: 0.1279\n",
      "Epoch 81/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3975 - jaccard: 0.1230 - val_loss: 0.3904 - val_jaccard: 0.1260\n",
      "Epoch 82/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3965 - jaccard: 0.1229 - val_loss: 0.3894 - val_jaccard: 0.1264\n",
      "Epoch 83/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3954 - jaccard: 0.1236 - val_loss: 0.3878 - val_jaccard: 0.1271\n",
      "Epoch 84/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3942 - jaccard: 0.1226 - val_loss: 0.3867 - val_jaccard: 0.1278\n",
      "Epoch 85/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3937 - jaccard: 0.1234 - val_loss: 0.3859 - val_jaccard: 0.1286\n",
      "Epoch 86/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3923 - jaccard: 0.1234 - val_loss: 0.3854 - val_jaccard: 0.1283\n",
      "Epoch 87/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3919 - jaccard: 0.1232 - val_loss: 0.3841 - val_jaccard: 0.1273\n",
      "Epoch 88/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3902 - jaccard: 0.1246 - val_loss: 0.3833 - val_jaccard: 0.1292\n",
      "Epoch 89/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3897 - jaccard: 0.1241 - val_loss: 0.3819 - val_jaccard: 0.1279\n",
      "Epoch 90/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3884 - jaccard: 0.1241 - val_loss: 0.3811 - val_jaccard: 0.1305\n",
      "Epoch 91/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3873 - jaccard: 0.1255 - val_loss: 0.3800 - val_jaccard: 0.1293\n",
      "Epoch 92/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3867 - jaccard: 0.1245 - val_loss: 0.3795 - val_jaccard: 0.1287\n",
      "Epoch 93/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3859 - jaccard: 0.1238 - val_loss: 0.3781 - val_jaccard: 0.1297\n",
      "Epoch 94/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3846 - jaccard: 0.1249 - val_loss: 0.3776 - val_jaccard: 0.1302\n",
      "Epoch 95/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3835 - jaccard: 0.1265 - val_loss: 0.3767 - val_jaccard: 0.1297\n",
      "Epoch 96/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3824 - jaccard: 0.1264 - val_loss: 0.3752 - val_jaccard: 0.1313\n",
      "Epoch 97/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3815 - jaccard: 0.1268 - val_loss: 0.3751 - val_jaccard: 0.1297\n",
      "Epoch 98/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3810 - jaccard: 0.1240 - val_loss: 0.3734 - val_jaccard: 0.1305\n",
      "Epoch 99/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3802 - jaccard: 0.1275 - val_loss: 0.3732 - val_jaccard: 0.1318\n",
      "Epoch 100/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3792 - jaccard: 0.1256 - val_loss: 0.3726 - val_jaccard: 0.1297\n",
      "Epoch 101/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3782 - jaccard: 0.1265 - val_loss: 0.3710 - val_jaccard: 0.1327\n",
      "Epoch 102/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3771 - jaccard: 0.1282 - val_loss: 0.3709 - val_jaccard: 0.1308\n",
      "Epoch 103/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3764 - jaccard: 0.1263 - val_loss: 0.3693 - val_jaccard: 0.1316\n",
      "Epoch 104/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3755 - jaccard: 0.1274 - val_loss: 0.3688 - val_jaccard: 0.1322\n",
      "Epoch 105/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3745 - jaccard: 0.1278 - val_loss: 0.3675 - val_jaccard: 0.1323\n",
      "Epoch 106/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3734 - jaccard: 0.1279 - val_loss: 0.3676 - val_jaccard: 0.1329\n",
      "Epoch 107/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3733 - jaccard: 0.1263 - val_loss: 0.3662 - val_jaccard: 0.1312\n",
      "Epoch 108/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3717 - jaccard: 0.1293 - val_loss: 0.3661 - val_jaccard: 0.1347\n",
      "Epoch 109/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3712 - jaccard: 0.1291 - val_loss: 0.3647 - val_jaccard: 0.1318\n",
      "Epoch 110/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3702 - jaccard: 0.1283 - val_loss: 0.3643 - val_jaccard: 0.1333\n",
      "Epoch 111/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3696 - jaccard: 0.1289 - val_loss: 0.3635 - val_jaccard: 0.1324\n",
      "Epoch 112/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3686 - jaccard: 0.1283 - val_loss: 0.3622 - val_jaccard: 0.1329\n",
      "Epoch 113/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3680 - jaccard: 0.1298 - val_loss: 0.3614 - val_jaccard: 0.1354\n",
      "Epoch 114/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3672 - jaccard: 0.1287 - val_loss: 0.3614 - val_jaccard: 0.1316\n",
      "Epoch 115/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3663 - jaccard: 0.1296 - val_loss: 0.3605 - val_jaccard: 0.1355\n",
      "Epoch 116/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3655 - jaccard: 0.1310 - val_loss: 0.3598 - val_jaccard: 0.1325\n",
      "Epoch 117/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3647 - jaccard: 0.1286 - val_loss: 0.3588 - val_jaccard: 0.1351\n",
      "Epoch 118/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3640 - jaccard: 0.1306 - val_loss: 0.3584 - val_jaccard: 0.1334\n",
      "Epoch 119/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3634 - jaccard: 0.1294 - val_loss: 0.3574 - val_jaccard: 0.1341\n",
      "Epoch 120/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3630 - jaccard: 0.1302 - val_loss: 0.3570 - val_jaccard: 0.1337\n",
      "Epoch 121/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3625 - jaccard: 0.1297 - val_loss: 0.3567 - val_jaccard: 0.1334\n",
      "Epoch 122/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3614 - jaccard: 0.1289 - val_loss: 0.3546 - val_jaccard: 0.1355\n",
      "Epoch 123/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3600 - jaccard: 0.1311 - val_loss: 0.3547 - val_jaccard: 0.1348\n",
      "Epoch 124/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3594 - jaccard: 0.1303 - val_loss: 0.3530 - val_jaccard: 0.1378\n",
      "Epoch 125/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3586 - jaccard: 0.1333 - val_loss: 0.3523 - val_jaccard: 0.1379\n",
      "Epoch 126/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3579 - jaccard: 0.1315 - val_loss: 0.3526 - val_jaccard: 0.1335\n",
      "Epoch 127/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3576 - jaccard: 0.1291 - val_loss: 0.3510 - val_jaccard: 0.1378\n",
      "Epoch 128/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3562 - jaccard: 0.1333 - val_loss: 0.3515 - val_jaccard: 0.1361\n",
      "Epoch 129/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3566 - jaccard: 0.1311 - val_loss: 0.3495 - val_jaccard: 0.1371\n",
      "Epoch 130/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3557 - jaccard: 0.1319 - val_loss: 0.3496 - val_jaccard: 0.1364\n",
      "Epoch 131/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3541 - jaccard: 0.1317 - val_loss: 0.3483 - val_jaccard: 0.1366\n",
      "Epoch 132/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3537 - jaccard: 0.1336 - val_loss: 0.3478 - val_jaccard: 0.1382\n",
      "Epoch 133/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3533 - jaccard: 0.1323 - val_loss: 0.3471 - val_jaccard: 0.1369\n",
      "Epoch 134/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3521 - jaccard: 0.1326 - val_loss: 0.3464 - val_jaccard: 0.1382\n",
      "Epoch 135/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3515 - jaccard: 0.1338 - val_loss: 0.3457 - val_jaccard: 0.1389\n",
      "Epoch 136/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3517 - jaccard: 0.1315 - val_loss: 0.3452 - val_jaccard: 0.1375\n",
      "Epoch 137/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3503 - jaccard: 0.1342 - val_loss: 0.3446 - val_jaccard: 0.1396\n",
      "Epoch 138/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3493 - jaccard: 0.1344 - val_loss: 0.3439 - val_jaccard: 0.1390\n",
      "Epoch 139/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3489 - jaccard: 0.1337 - val_loss: 0.3431 - val_jaccard: 0.1388\n",
      "Epoch 140/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3482 - jaccard: 0.1343 - val_loss: 0.3431 - val_jaccard: 0.1404\n",
      "Epoch 141/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3484 - jaccard: 0.1355 - val_loss: 0.3420 - val_jaccard: 0.1388\n",
      "Epoch 142/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3464 - jaccard: 0.1336 - val_loss: 0.3417 - val_jaccard: 0.1384\n",
      "Epoch 143/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3462 - jaccard: 0.1354 - val_loss: 0.3403 - val_jaccard: 0.1401\n",
      "Epoch 144/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3453 - jaccard: 0.1341 - val_loss: 0.3404 - val_jaccard: 0.1387\n",
      "Epoch 145/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3447 - jaccard: 0.1355 - val_loss: 0.3391 - val_jaccard: 0.1419\n",
      "Epoch 146/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3443 - jaccard: 0.1356 - val_loss: 0.3392 - val_jaccard: 0.1399\n",
      "Epoch 147/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3434 - jaccard: 0.1357 - val_loss: 0.3393 - val_jaccard: 0.1404\n",
      "Epoch 148/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3434 - jaccard: 0.1361 - val_loss: 0.3377 - val_jaccard: 0.1395\n",
      "Epoch 149/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3423 - jaccard: 0.1355 - val_loss: 0.3371 - val_jaccard: 0.1413\n",
      "Epoch 150/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3419 - jaccard: 0.1371 - val_loss: 0.3366 - val_jaccard: 0.1411\n",
      "Epoch 151/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3410 - jaccard: 0.1352 - val_loss: 0.3366 - val_jaccard: 0.1393\n",
      "Epoch 152/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3406 - jaccard: 0.1372 - val_loss: 0.3358 - val_jaccard: 0.1418\n",
      "Epoch 153/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3401 - jaccard: 0.1362 - val_loss: 0.3352 - val_jaccard: 0.1405\n",
      "Epoch 154/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3397 - jaccard: 0.1359 - val_loss: 0.3346 - val_jaccard: 0.1425\n",
      "Epoch 155/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3392 - jaccard: 0.1381 - val_loss: 0.3341 - val_jaccard: 0.1419\n",
      "Epoch 156/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3382 - jaccard: 0.1362 - val_loss: 0.3332 - val_jaccard: 0.1420\n",
      "Epoch 157/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3374 - jaccard: 0.1377 - val_loss: 0.3325 - val_jaccard: 0.1428\n",
      "Epoch 158/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3372 - jaccard: 0.1370 - val_loss: 0.3328 - val_jaccard: 0.1422\n",
      "Epoch 159/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3374 - jaccard: 0.1366 - val_loss: 0.3316 - val_jaccard: 0.1419\n",
      "Epoch 160/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3362 - jaccard: 0.1376 - val_loss: 0.3312 - val_jaccard: 0.1441\n",
      "Epoch 161/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3355 - jaccard: 0.1402 - val_loss: 0.3313 - val_jaccard: 0.1415\n",
      "Epoch 162/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3356 - jaccard: 0.1351 - val_loss: 0.3304 - val_jaccard: 0.1416\n",
      "Epoch 163/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3355 - jaccard: 0.1359 - val_loss: 0.3300 - val_jaccard: 0.1433\n",
      "Epoch 164/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3339 - jaccard: 0.1394 - val_loss: 0.3294 - val_jaccard: 0.1431\n",
      "Epoch 165/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3332 - jaccard: 0.1389 - val_loss: 0.3287 - val_jaccard: 0.1435\n",
      "Epoch 166/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3327 - jaccard: 0.1382 - val_loss: 0.3280 - val_jaccard: 0.1449\n",
      "Epoch 167/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3328 - jaccard: 0.1387 - val_loss: 0.3283 - val_jaccard: 0.1426\n",
      "Epoch 168/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3317 - jaccard: 0.1391 - val_loss: 0.3269 - val_jaccard: 0.1440\n",
      "Epoch 169/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3308 - jaccard: 0.1399 - val_loss: 0.3264 - val_jaccard: 0.1451\n",
      "Epoch 170/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3312 - jaccard: 0.1398 - val_loss: 0.3266 - val_jaccard: 0.1443\n",
      "Epoch 171/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3298 - jaccard: 0.1403 - val_loss: 0.3254 - val_jaccard: 0.1427\n",
      "Epoch 172/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3295 - jaccard: 0.1402 - val_loss: 0.3254 - val_jaccard: 0.1443\n",
      "Epoch 173/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3302 - jaccard: 0.1367 - val_loss: 0.3245 - val_jaccard: 0.1428\n",
      "Epoch 174/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3291 - jaccard: 0.1407 - val_loss: 0.3246 - val_jaccard: 0.1452\n",
      "Epoch 175/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3285 - jaccard: 0.1385 - val_loss: 0.3237 - val_jaccard: 0.1444\n",
      "Epoch 176/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3273 - jaccard: 0.1412 - val_loss: 0.3229 - val_jaccard: 0.1468\n",
      "Epoch 177/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3273 - jaccard: 0.1409 - val_loss: 0.3228 - val_jaccard: 0.1445\n",
      "Epoch 178/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3261 - jaccard: 0.1409 - val_loss: 0.3220 - val_jaccard: 0.1473\n",
      "Epoch 179/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3256 - jaccard: 0.1428 - val_loss: 0.3215 - val_jaccard: 0.1442\n",
      "Epoch 180/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3251 - jaccard: 0.1402 - val_loss: 0.3212 - val_jaccard: 0.1480\n",
      "Epoch 181/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3248 - jaccard: 0.1439 - val_loss: 0.3207 - val_jaccard: 0.1461\n",
      "Epoch 182/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3238 - jaccard: 0.1415 - val_loss: 0.3198 - val_jaccard: 0.1459\n",
      "Epoch 183/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3239 - jaccard: 0.1421 - val_loss: 0.3196 - val_jaccard: 0.1485\n",
      "Epoch 184/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3240 - jaccard: 0.1422 - val_loss: 0.3200 - val_jaccard: 0.1468\n",
      "Epoch 185/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3230 - jaccard: 0.1436 - val_loss: 0.3186 - val_jaccard: 0.1464\n",
      "Epoch 186/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3227 - jaccard: 0.1405 - val_loss: 0.3190 - val_jaccard: 0.1451\n",
      "Epoch 187/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3220 - jaccard: 0.1421 - val_loss: 0.3185 - val_jaccard: 0.1476\n",
      "Epoch 188/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3224 - jaccard: 0.1439 - val_loss: 0.3177 - val_jaccard: 0.1460\n",
      "Epoch 189/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3212 - jaccard: 0.1406 - val_loss: 0.3170 - val_jaccard: 0.1467\n",
      "Epoch 190/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3217 - jaccard: 0.1446 - val_loss: 0.3169 - val_jaccard: 0.1475\n",
      "Epoch 191/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3206 - jaccard: 0.1419 - val_loss: 0.3163 - val_jaccard: 0.1460\n",
      "Epoch 192/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3204 - jaccard: 0.1411 - val_loss: 0.3158 - val_jaccard: 0.1467\n",
      "Epoch 193/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3193 - jaccard: 0.1421 - val_loss: 0.3152 - val_jaccard: 0.1510\n",
      "Epoch 194/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3196 - jaccard: 0.1473 - val_loss: 0.3157 - val_jaccard: 0.1466\n",
      "Epoch 195/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3187 - jaccard: 0.1400 - val_loss: 0.3147 - val_jaccard: 0.1447\n",
      "Epoch 196/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3198 - jaccard: 0.1427 - val_loss: 0.3142 - val_jaccard: 0.1527\n",
      "Epoch 197/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3181 - jaccard: 0.1443 - val_loss: 0.3142 - val_jaccard: 0.1455\n",
      "Epoch 198/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3174 - jaccard: 0.1430 - val_loss: 0.3139 - val_jaccard: 0.1494\n",
      "Epoch 199/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3176 - jaccard: 0.1430 - val_loss: 0.3132 - val_jaccard: 0.1484\n",
      "Epoch 200/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3164 - jaccard: 0.1457 - val_loss: 0.3126 - val_jaccard: 0.1505\n",
      "Epoch 201/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3158 - jaccard: 0.1444 - val_loss: 0.3122 - val_jaccard: 0.1483\n",
      "Epoch 202/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3152 - jaccard: 0.1452 - val_loss: 0.3115 - val_jaccard: 0.1507\n",
      "Epoch 203/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3149 - jaccard: 0.1470 - val_loss: 0.3118 - val_jaccard: 0.1493\n",
      "Epoch 204/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3144 - jaccard: 0.1465 - val_loss: 0.3106 - val_jaccard: 0.1511\n",
      "Epoch 205/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3136 - jaccard: 0.1457 - val_loss: 0.3109 - val_jaccard: 0.1491\n",
      "Epoch 206/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3131 - jaccard: 0.1466 - val_loss: 0.3101 - val_jaccard: 0.1502\n",
      "Epoch 207/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3130 - jaccard: 0.1473 - val_loss: 0.3097 - val_jaccard: 0.1512\n",
      "Epoch 208/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3133 - jaccard: 0.1457 - val_loss: 0.3097 - val_jaccard: 0.1498\n",
      "Epoch 209/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3127 - jaccard: 0.1467 - val_loss: 0.3090 - val_jaccard: 0.1508\n",
      "Epoch 210/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3121 - jaccard: 0.1451 - val_loss: 0.3087 - val_jaccard: 0.1507\n",
      "Epoch 211/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3121 - jaccard: 0.1476 - val_loss: 0.3097 - val_jaccard: 0.1493\n",
      "Epoch 212/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3117 - jaccard: 0.1465 - val_loss: 0.3081 - val_jaccard: 0.1503\n",
      "Epoch 213/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3112 - jaccard: 0.1444 - val_loss: 0.3074 - val_jaccard: 0.1507\n",
      "Epoch 214/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3109 - jaccard: 0.1466 - val_loss: 0.3085 - val_jaccard: 0.1519\n",
      "Epoch 215/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3099 - jaccard: 0.1501 - val_loss: 0.3071 - val_jaccard: 0.1520\n",
      "Epoch 216/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3103 - jaccard: 0.1458 - val_loss: 0.3070 - val_jaccard: 0.1511\n",
      "Epoch 217/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3105 - jaccard: 0.1441 - val_loss: 0.3065 - val_jaccard: 0.1507\n",
      "Epoch 218/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3089 - jaccard: 0.1500 - val_loss: 0.3062 - val_jaccard: 0.1537\n",
      "Epoch 219/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3091 - jaccard: 0.1469 - val_loss: 0.3052 - val_jaccard: 0.1518\n",
      "Epoch 220/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3086 - jaccard: 0.1472 - val_loss: 0.3052 - val_jaccard: 0.1521\n",
      "Epoch 221/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3081 - jaccard: 0.1477 - val_loss: 0.3050 - val_jaccard: 0.1526\n",
      "Epoch 222/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3092 - jaccard: 0.1489 - val_loss: 0.3057 - val_jaccard: 0.1499\n",
      "Epoch 223/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3082 - jaccard: 0.1465 - val_loss: 0.3049 - val_jaccard: 0.1515\n",
      "Epoch 224/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3075 - jaccard: 0.1474 - val_loss: 0.3045 - val_jaccard: 0.1502\n",
      "Epoch 225/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3074 - jaccard: 0.1481 - val_loss: 0.3037 - val_jaccard: 0.1539\n",
      "Epoch 226/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3064 - jaccard: 0.1477 - val_loss: 0.3030 - val_jaccard: 0.1528\n",
      "Epoch 227/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3063 - jaccard: 0.1465 - val_loss: 0.3029 - val_jaccard: 0.1559\n",
      "Epoch 228/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3055 - jaccard: 0.1521 - val_loss: 0.3035 - val_jaccard: 0.1520\n",
      "Epoch 229/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3049 - jaccard: 0.1478 - val_loss: 0.3022 - val_jaccard: 0.1539\n",
      "Epoch 230/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3044 - jaccard: 0.1508 - val_loss: 0.3020 - val_jaccard: 0.1542\n",
      "Epoch 231/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3047 - jaccard: 0.1505 - val_loss: 0.3014 - val_jaccard: 0.1532\n",
      "Epoch 232/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3033 - jaccard: 0.1506 - val_loss: 0.3009 - val_jaccard: 0.1565\n",
      "Epoch 233/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3035 - jaccard: 0.1494 - val_loss: 0.3011 - val_jaccard: 0.1547\n",
      "Epoch 234/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3026 - jaccard: 0.1524 - val_loss: 0.3008 - val_jaccard: 0.1544\n",
      "Epoch 235/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3029 - jaccard: 0.1513 - val_loss: 0.3013 - val_jaccard: 0.1542\n",
      "Epoch 236/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3030 - jaccard: 0.1504 - val_loss: 0.3002 - val_jaccard: 0.1537\n",
      "Epoch 237/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3020 - jaccard: 0.1500 - val_loss: 0.2997 - val_jaccard: 0.1554\n",
      "Epoch 238/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3018 - jaccard: 0.1512 - val_loss: 0.3001 - val_jaccard: 0.1555\n",
      "Epoch 239/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3017 - jaccard: 0.1520 - val_loss: 0.2994 - val_jaccard: 0.1562\n",
      "Epoch 240/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3020 - jaccard: 0.1520 - val_loss: 0.2988 - val_jaccard: 0.1540\n",
      "Epoch 241/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3017 - jaccard: 0.1507 - val_loss: 0.2994 - val_jaccard: 0.1538\n",
      "Epoch 242/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3008 - jaccard: 0.1503 - val_loss: 0.2988 - val_jaccard: 0.1528\n",
      "Epoch 243/1000\n",
      "320/320 [==============================] - 6s - loss: 0.3006 - jaccard: 0.1508 - val_loss: 0.2985 - val_jaccard: 0.1555\n",
      "Epoch 244/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2994 - jaccard: 0.1533 - val_loss: 0.2972 - val_jaccard: 0.1557\n",
      "Epoch 245/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2990 - jaccard: 0.1520 - val_loss: 0.2968 - val_jaccard: 0.1571\n",
      "Epoch 246/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2988 - jaccard: 0.1521 - val_loss: 0.2969 - val_jaccard: 0.1563\n",
      "Epoch 247/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2984 - jaccard: 0.1560 - val_loss: 0.2966 - val_jaccard: 0.1579\n",
      "Epoch 248/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2992 - jaccard: 0.1517 - val_loss: 0.2969 - val_jaccard: 0.1532\n",
      "Epoch 249/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2988 - jaccard: 0.1513 - val_loss: 0.2964 - val_jaccard: 0.1578\n",
      "Epoch 250/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2984 - jaccard: 0.1547 - val_loss: 0.2962 - val_jaccard: 0.1544\n",
      "Epoch 251/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2978 - jaccard: 0.1508 - val_loss: 0.2954 - val_jaccard: 0.1575\n",
      "Epoch 252/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2973 - jaccard: 0.1534 - val_loss: 0.2962 - val_jaccard: 0.1563\n",
      "Epoch 253/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2967 - jaccard: 0.1558 - val_loss: 0.2944 - val_jaccard: 0.1578\n",
      "Epoch 254/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2961 - jaccard: 0.1533 - val_loss: 0.2946 - val_jaccard: 0.1581\n",
      "Epoch 255/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2955 - jaccard: 0.1565 - val_loss: 0.2943 - val_jaccard: 0.1589\n",
      "Epoch 256/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2953 - jaccard: 0.1545 - val_loss: 0.2934 - val_jaccard: 0.1582\n",
      "Epoch 257/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2952 - jaccard: 0.1547 - val_loss: 0.2933 - val_jaccard: 0.1603\n",
      "Epoch 258/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2947 - jaccard: 0.1577 - val_loss: 0.2932 - val_jaccard: 0.1584\n",
      "Epoch 259/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2943 - jaccard: 0.1556 - val_loss: 0.2927 - val_jaccard: 0.1589\n",
      "Epoch 260/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2942 - jaccard: 0.1551 - val_loss: 0.2927 - val_jaccard: 0.1590\n",
      "Epoch 261/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2937 - jaccard: 0.1568 - val_loss: 0.2927 - val_jaccard: 0.1596\n",
      "Epoch 262/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2935 - jaccard: 0.1560 - val_loss: 0.2922 - val_jaccard: 0.1593\n",
      "Epoch 263/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2939 - jaccard: 0.1553 - val_loss: 0.2917 - val_jaccard: 0.1610\n",
      "Epoch 264/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2941 - jaccard: 0.1540 - val_loss: 0.2921 - val_jaccard: 0.1590\n",
      "Epoch 265/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2929 - jaccard: 0.1584 - val_loss: 0.2923 - val_jaccard: 0.1599\n",
      "Epoch 266/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2925 - jaccard: 0.1574 - val_loss: 0.2909 - val_jaccard: 0.1586\n",
      "Epoch 267/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2922 - jaccard: 0.1543 - val_loss: 0.2908 - val_jaccard: 0.1595\n",
      "Epoch 268/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2921 - jaccard: 0.1593 - val_loss: 0.2901 - val_jaccard: 0.1622\n",
      "Epoch 269/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2914 - jaccard: 0.1561 - val_loss: 0.2900 - val_jaccard: 0.1606\n",
      "Epoch 270/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2909 - jaccard: 0.1584 - val_loss: 0.2909 - val_jaccard: 0.1594\n",
      "Epoch 271/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2909 - jaccard: 0.1579 - val_loss: 0.2896 - val_jaccard: 0.1615\n",
      "Epoch 272/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2906 - jaccard: 0.1581 - val_loss: 0.2900 - val_jaccard: 0.1608\n",
      "Epoch 273/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2912 - jaccard: 0.1564 - val_loss: 0.2894 - val_jaccard: 0.1599\n",
      "Epoch 274/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2912 - jaccard: 0.1542 - val_loss: 0.2891 - val_jaccard: 0.1631\n",
      "Epoch 275/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2903 - jaccard: 0.1619 - val_loss: 0.2894 - val_jaccard: 0.1601\n",
      "Epoch 276/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2907 - jaccard: 0.1542 - val_loss: 0.2887 - val_jaccard: 0.1606\n",
      "Epoch 277/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2898 - jaccard: 0.1580 - val_loss: 0.2881 - val_jaccard: 0.1638\n",
      "Epoch 278/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2899 - jaccard: 0.1577 - val_loss: 0.2882 - val_jaccard: 0.1606\n",
      "Epoch 279/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2888 - jaccard: 0.1590 - val_loss: 0.2888 - val_jaccard: 0.1605\n",
      "Epoch 280/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2884 - jaccard: 0.1595 - val_loss: 0.2873 - val_jaccard: 0.1644\n",
      "Epoch 281/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2899 - jaccard: 0.1563 - val_loss: 0.2880 - val_jaccard: 0.1607\n",
      "Epoch 282/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2891 - jaccard: 0.1584 - val_loss: 0.2879 - val_jaccard: 0.1619\n",
      "Epoch 283/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2885 - jaccard: 0.1578 - val_loss: 0.2873 - val_jaccard: 0.1627\n",
      "Epoch 284/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2890 - jaccard: 0.1594 - val_loss: 0.2867 - val_jaccard: 0.1633\n",
      "Epoch 285/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2875 - jaccard: 0.1574 - val_loss: 0.2867 - val_jaccard: 0.1592\n",
      "Epoch 286/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2873 - jaccard: 0.1583 - val_loss: 0.2861 - val_jaccard: 0.1652\n",
      "Epoch 287/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2870 - jaccard: 0.1615 - val_loss: 0.2862 - val_jaccard: 0.1632\n",
      "Epoch 288/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2866 - jaccard: 0.1603 - val_loss: 0.2856 - val_jaccard: 0.1632\n",
      "Epoch 289/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2870 - jaccard: 0.1576 - val_loss: 0.2856 - val_jaccard: 0.1627\n",
      "Epoch 290/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2862 - jaccard: 0.1603 - val_loss: 0.2854 - val_jaccard: 0.1639\n",
      "Epoch 291/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2855 - jaccard: 0.1634 - val_loss: 0.2847 - val_jaccard: 0.1651\n",
      "Epoch 292/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2863 - jaccard: 0.1578 - val_loss: 0.2850 - val_jaccard: 0.1631\n",
      "Epoch 293/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2858 - jaccard: 0.1613 - val_loss: 0.2850 - val_jaccard: 0.1627\n",
      "Epoch 294/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2850 - jaccard: 0.1608 - val_loss: 0.2841 - val_jaccard: 0.1671\n",
      "Epoch 295/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2844 - jaccard: 0.1627 - val_loss: 0.2843 - val_jaccard: 0.1628\n",
      "Epoch 296/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2839 - jaccard: 0.1611 - val_loss: 0.2833 - val_jaccard: 0.1660\n",
      "Epoch 297/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2839 - jaccard: 0.1629 - val_loss: 0.2838 - val_jaccard: 0.1668\n",
      "Epoch 298/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2832 - jaccard: 0.1633 - val_loss: 0.2830 - val_jaccard: 0.1638\n",
      "Epoch 299/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2834 - jaccard: 0.1623 - val_loss: 0.2829 - val_jaccard: 0.1663\n",
      "Epoch 300/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2830 - jaccard: 0.1626 - val_loss: 0.2833 - val_jaccard: 0.1650\n",
      "Epoch 301/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2829 - jaccard: 0.1638 - val_loss: 0.2832 - val_jaccard: 0.1634\n",
      "Epoch 302/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2843 - jaccard: 0.1591 - val_loss: 0.2828 - val_jaccard: 0.1670\n",
      "Epoch 303/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2827 - jaccard: 0.1636 - val_loss: 0.2828 - val_jaccard: 0.1635\n",
      "Epoch 304/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2830 - jaccard: 0.1631 - val_loss: 0.2822 - val_jaccard: 0.1686\n",
      "Epoch 305/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2831 - jaccard: 0.1612 - val_loss: 0.2827 - val_jaccard: 0.1625\n",
      "Epoch 306/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2822 - jaccard: 0.1604 - val_loss: 0.2819 - val_jaccard: 0.1696\n",
      "Epoch 307/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2831 - jaccard: 0.1670 - val_loss: 0.2827 - val_jaccard: 0.1628\n",
      "Epoch 308/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2822 - jaccard: 0.1602 - val_loss: 0.2812 - val_jaccard: 0.1638\n",
      "Epoch 309/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2818 - jaccard: 0.1605 - val_loss: 0.2825 - val_jaccard: 0.1677\n",
      "Epoch 310/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2817 - jaccard: 0.1655 - val_loss: 0.2820 - val_jaccard: 0.1635\n",
      "Epoch 311/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2825 - jaccard: 0.1614 - val_loss: 0.2808 - val_jaccard: 0.1666\n",
      "Epoch 312/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2809 - jaccard: 0.1641 - val_loss: 0.2811 - val_jaccard: 0.1644\n",
      "Epoch 313/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2805 - jaccard: 0.1624 - val_loss: 0.2806 - val_jaccard: 0.1668\n",
      "Epoch 314/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2797 - jaccard: 0.1648 - val_loss: 0.2801 - val_jaccard: 0.1678\n",
      "Epoch 315/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2799 - jaccard: 0.1639 - val_loss: 0.2796 - val_jaccard: 0.1684\n",
      "Epoch 316/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2797 - jaccard: 0.1648 - val_loss: 0.2808 - val_jaccard: 0.1667\n",
      "Epoch 317/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2802 - jaccard: 0.1633 - val_loss: 0.2793 - val_jaccard: 0.1681\n",
      "Epoch 318/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2786 - jaccard: 0.1670 - val_loss: 0.2791 - val_jaccard: 0.1678\n",
      "Epoch 319/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2782 - jaccard: 0.1662 - val_loss: 0.2788 - val_jaccard: 0.1682\n",
      "Epoch 320/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2792 - jaccard: 0.1671 - val_loss: 0.2786 - val_jaccard: 0.1690\n",
      "Epoch 321/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2786 - jaccard: 0.1626 - val_loss: 0.2786 - val_jaccard: 0.1672\n",
      "Epoch 322/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2781 - jaccard: 0.1665 - val_loss: 0.2787 - val_jaccard: 0.1693\n",
      "Epoch 323/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2785 - jaccard: 0.1654 - val_loss: 0.2786 - val_jaccard: 0.1680\n",
      "Epoch 324/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2775 - jaccard: 0.1662 - val_loss: 0.2780 - val_jaccard: 0.1685\n",
      "Epoch 325/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2783 - jaccard: 0.1682 - val_loss: 0.2777 - val_jaccard: 0.1690\n",
      "Epoch 326/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2790 - jaccard: 0.1620 - val_loss: 0.2787 - val_jaccard: 0.1664\n",
      "Epoch 327/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2779 - jaccard: 0.1633 - val_loss: 0.2783 - val_jaccard: 0.1678\n",
      "Epoch 328/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2772 - jaccard: 0.1665 - val_loss: 0.2782 - val_jaccard: 0.1713\n",
      "Epoch 329/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2773 - jaccard: 0.1680 - val_loss: 0.2772 - val_jaccard: 0.1692\n",
      "Epoch 330/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2764 - jaccard: 0.1658 - val_loss: 0.2776 - val_jaccard: 0.1676\n",
      "Epoch 331/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2761 - jaccard: 0.1685 - val_loss: 0.2766 - val_jaccard: 0.1720\n",
      "Epoch 332/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2776 - jaccard: 0.1646 - val_loss: 0.2769 - val_jaccard: 0.1704\n",
      "Epoch 333/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2759 - jaccard: 0.1691 - val_loss: 0.2765 - val_jaccard: 0.1703\n",
      "Epoch 334/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2764 - jaccard: 0.1643 - val_loss: 0.2765 - val_jaccard: 0.1691\n",
      "Epoch 335/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2760 - jaccard: 0.1676 - val_loss: 0.2764 - val_jaccard: 0.1715\n",
      "Epoch 336/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2754 - jaccard: 0.1688 - val_loss: 0.2762 - val_jaccard: 0.1697\n",
      "Epoch 337/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2750 - jaccard: 0.1674 - val_loss: 0.2756 - val_jaccard: 0.1724\n",
      "Epoch 338/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2752 - jaccard: 0.1696 - val_loss: 0.2760 - val_jaccard: 0.1681\n",
      "Epoch 339/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2743 - jaccard: 0.1675 - val_loss: 0.2754 - val_jaccard: 0.1702\n",
      "Epoch 340/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2739 - jaccard: 0.1683 - val_loss: 0.2755 - val_jaccard: 0.1715\n",
      "Epoch 341/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2743 - jaccard: 0.1708 - val_loss: 0.2752 - val_jaccard: 0.1706\n",
      "Epoch 342/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2736 - jaccard: 0.1683 - val_loss: 0.2747 - val_jaccard: 0.1711\n",
      "Epoch 343/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2735 - jaccard: 0.1698 - val_loss: 0.2761 - val_jaccard: 0.1716\n",
      "Epoch 344/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2739 - jaccard: 0.1698 - val_loss: 0.2742 - val_jaccard: 0.1738\n",
      "Epoch 345/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2734 - jaccard: 0.1696 - val_loss: 0.2744 - val_jaccard: 0.1711\n",
      "Epoch 346/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2734 - jaccard: 0.1691 - val_loss: 0.2752 - val_jaccard: 0.1693\n",
      "Epoch 347/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2728 - jaccard: 0.1693 - val_loss: 0.2741 - val_jaccard: 0.1731\n",
      "Epoch 348/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2728 - jaccard: 0.1703 - val_loss: 0.2735 - val_jaccard: 0.1722\n",
      "Epoch 349/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2723 - jaccard: 0.1716 - val_loss: 0.2739 - val_jaccard: 0.1701\n",
      "Epoch 350/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2720 - jaccard: 0.1691 - val_loss: 0.2733 - val_jaccard: 0.1744\n",
      "Epoch 351/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2716 - jaccard: 0.1717 - val_loss: 0.2735 - val_jaccard: 0.1720\n",
      "Epoch 352/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2723 - jaccard: 0.1712 - val_loss: 0.2729 - val_jaccard: 0.1725\n",
      "Epoch 353/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2724 - jaccard: 0.1681 - val_loss: 0.2732 - val_jaccard: 0.1732\n",
      "Epoch 354/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2712 - jaccard: 0.1725 - val_loss: 0.2729 - val_jaccard: 0.1753\n",
      "Epoch 355/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2718 - jaccard: 0.1723 - val_loss: 0.2736 - val_jaccard: 0.1704\n",
      "Epoch 356/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2717 - jaccard: 0.1687 - val_loss: 0.2725 - val_jaccard: 0.1736\n",
      "Epoch 357/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2713 - jaccard: 0.1715 - val_loss: 0.2727 - val_jaccard: 0.1732\n",
      "Epoch 358/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2707 - jaccard: 0.1713 - val_loss: 0.2718 - val_jaccard: 0.1754\n",
      "Epoch 359/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2703 - jaccard: 0.1722 - val_loss: 0.2724 - val_jaccard: 0.1724\n",
      "Epoch 360/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2709 - jaccard: 0.1724 - val_loss: 0.2727 - val_jaccard: 0.1709\n",
      "Epoch 361/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2712 - jaccard: 0.1672 - val_loss: 0.2724 - val_jaccard: 0.1750\n",
      "Epoch 362/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2706 - jaccard: 0.1739 - val_loss: 0.2721 - val_jaccard: 0.1747\n",
      "Epoch 363/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2709 - jaccard: 0.1695 - val_loss: 0.2719 - val_jaccard: 0.1738\n",
      "Epoch 364/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2702 - jaccard: 0.1720 - val_loss: 0.2716 - val_jaccard: 0.1722\n",
      "Epoch 365/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2694 - jaccard: 0.1719 - val_loss: 0.2713 - val_jaccard: 0.1767\n",
      "Epoch 366/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2694 - jaccard: 0.1728 - val_loss: 0.2713 - val_jaccard: 0.1725\n",
      "Epoch 367/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2689 - jaccard: 0.1731 - val_loss: 0.2705 - val_jaccard: 0.1764\n",
      "Epoch 368/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2690 - jaccard: 0.1732 - val_loss: 0.2713 - val_jaccard: 0.1741\n",
      "Epoch 369/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2687 - jaccard: 0.1743 - val_loss: 0.2709 - val_jaccard: 0.1730\n",
      "Epoch 370/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2683 - jaccard: 0.1733 - val_loss: 0.2703 - val_jaccard: 0.1759\n",
      "Epoch 371/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2681 - jaccard: 0.1733 - val_loss: 0.2705 - val_jaccard: 0.1753\n",
      "Epoch 372/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2687 - jaccard: 0.1717 - val_loss: 0.2715 - val_jaccard: 0.1744\n",
      "Epoch 373/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2684 - jaccard: 0.1755 - val_loss: 0.2699 - val_jaccard: 0.1774\n",
      "Epoch 374/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2697 - jaccard: 0.1679 - val_loss: 0.2714 - val_jaccard: 0.1739\n",
      "Epoch 375/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2710 - jaccard: 0.1747 - val_loss: 0.2735 - val_jaccard: 0.1737\n",
      "Epoch 376/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2703 - jaccard: 0.1673 - val_loss: 0.2711 - val_jaccard: 0.1700\n",
      "Epoch 377/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2691 - jaccard: 0.1722 - val_loss: 0.2714 - val_jaccard: 0.1763\n",
      "Epoch 378/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2692 - jaccard: 0.1709 - val_loss: 0.2699 - val_jaccard: 0.1742\n",
      "Epoch 379/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2688 - jaccard: 0.1737 - val_loss: 0.2695 - val_jaccard: 0.1771\n",
      "Epoch 380/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2678 - jaccard: 0.1739 - val_loss: 0.2700 - val_jaccard: 0.1724\n",
      "Epoch 381/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2687 - jaccard: 0.1693 - val_loss: 0.2697 - val_jaccard: 0.1749\n",
      "Epoch 382/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2671 - jaccard: 0.1755 - val_loss: 0.2693 - val_jaccard: 0.1770\n",
      "Epoch 383/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2668 - jaccard: 0.1741 - val_loss: 0.2689 - val_jaccard: 0.1785\n",
      "Epoch 384/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2673 - jaccard: 0.1745 - val_loss: 0.2700 - val_jaccard: 0.1738\n",
      "Epoch 385/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2679 - jaccard: 0.1768 - val_loss: 0.2684 - val_jaccard: 0.1772\n",
      "Epoch 386/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2673 - jaccard: 0.1672 - val_loss: 0.2697 - val_jaccard: 0.1705\n",
      "Epoch 387/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2663 - jaccard: 0.1752 - val_loss: 0.2695 - val_jaccard: 0.1813\n",
      "Epoch 388/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2662 - jaccard: 0.1787 - val_loss: 0.2689 - val_jaccard: 0.1746\n",
      "Epoch 389/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2664 - jaccard: 0.1711 - val_loss: 0.2679 - val_jaccard: 0.1776\n",
      "Epoch 390/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2661 - jaccard: 0.1766 - val_loss: 0.2685 - val_jaccard: 0.1772\n",
      "Epoch 391/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2650 - jaccard: 0.1743 - val_loss: 0.2679 - val_jaccard: 0.1779\n",
      "Epoch 392/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2650 - jaccard: 0.1788 - val_loss: 0.2685 - val_jaccard: 0.1763\n",
      "Epoch 393/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2654 - jaccard: 0.1742 - val_loss: 0.2674 - val_jaccard: 0.1776\n",
      "Epoch 394/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2652 - jaccard: 0.1757 - val_loss: 0.2675 - val_jaccard: 0.1755\n",
      "Epoch 395/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2655 - jaccard: 0.1756 - val_loss: 0.2672 - val_jaccard: 0.1782\n",
      "Epoch 396/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2649 - jaccard: 0.1753 - val_loss: 0.2674 - val_jaccard: 0.1771\n",
      "Epoch 397/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2641 - jaccard: 0.1755 - val_loss: 0.2669 - val_jaccard: 0.1821\n",
      "Epoch 398/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2638 - jaccard: 0.1798 - val_loss: 0.2676 - val_jaccard: 0.1769\n",
      "Epoch 399/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2643 - jaccard: 0.1780 - val_loss: 0.2664 - val_jaccard: 0.1795\n",
      "Epoch 400/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2637 - jaccard: 0.1762 - val_loss: 0.2671 - val_jaccard: 0.1769\n",
      "Epoch 401/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2638 - jaccard: 0.1762 - val_loss: 0.2666 - val_jaccard: 0.1772\n",
      "Epoch 402/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2646 - jaccard: 0.1763 - val_loss: 0.2662 - val_jaccard: 0.1818\n",
      "Epoch 403/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2633 - jaccard: 0.1794 - val_loss: 0.2664 - val_jaccard: 0.1787\n",
      "Epoch 404/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2630 - jaccard: 0.1750 - val_loss: 0.2656 - val_jaccard: 0.1798\n",
      "Epoch 405/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2633 - jaccard: 0.1780 - val_loss: 0.2657 - val_jaccard: 0.1800\n",
      "Epoch 406/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2631 - jaccard: 0.1764 - val_loss: 0.2663 - val_jaccard: 0.1807\n",
      "Epoch 407/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2637 - jaccard: 0.1794 - val_loss: 0.2655 - val_jaccard: 0.1822\n",
      "Epoch 408/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2628 - jaccard: 0.1793 - val_loss: 0.2659 - val_jaccard: 0.1779\n",
      "Epoch 409/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2623 - jaccard: 0.1767 - val_loss: 0.2652 - val_jaccard: 0.1799\n",
      "Epoch 410/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2622 - jaccard: 0.1792 - val_loss: 0.2660 - val_jaccard: 0.1821\n",
      "Epoch 411/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2618 - jaccard: 0.1806 - val_loss: 0.2656 - val_jaccard: 0.1775\n",
      "Epoch 412/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2616 - jaccard: 0.1782 - val_loss: 0.2649 - val_jaccard: 0.1812\n",
      "Epoch 413/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2617 - jaccard: 0.1792 - val_loss: 0.2660 - val_jaccard: 0.1790\n",
      "Epoch 414/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2619 - jaccard: 0.1797 - val_loss: 0.2652 - val_jaccard: 0.1817\n",
      "Epoch 415/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2627 - jaccard: 0.1752 - val_loss: 0.2655 - val_jaccard: 0.1795\n",
      "Epoch 416/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2613 - jaccard: 0.1833 - val_loss: 0.2650 - val_jaccard: 0.1832\n",
      "Epoch 417/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2619 - jaccard: 0.1794 - val_loss: 0.2653 - val_jaccard: 0.1765\n",
      "Epoch 418/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2622 - jaccard: 0.1756 - val_loss: 0.2645 - val_jaccard: 0.1820\n",
      "Epoch 419/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2611 - jaccard: 0.1776 - val_loss: 0.2648 - val_jaccard: 0.1809\n",
      "Epoch 420/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2603 - jaccard: 0.1816 - val_loss: 0.2643 - val_jaccard: 0.1828\n",
      "Epoch 421/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2607 - jaccard: 0.1821 - val_loss: 0.2639 - val_jaccard: 0.1805\n",
      "Epoch 422/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2602 - jaccard: 0.1789 - val_loss: 0.2641 - val_jaccard: 0.1821\n",
      "Epoch 423/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2609 - jaccard: 0.1801 - val_loss: 0.2644 - val_jaccard: 0.1809\n",
      "Epoch 424/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2607 - jaccard: 0.1818 - val_loss: 0.2639 - val_jaccard: 0.1808\n",
      "Epoch 425/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2607 - jaccard: 0.1740 - val_loss: 0.2634 - val_jaccard: 0.1821\n",
      "Epoch 426/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2610 - jaccard: 0.1819 - val_loss: 0.2642 - val_jaccard: 0.1835\n",
      "Epoch 427/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2607 - jaccard: 0.1824 - val_loss: 0.2643 - val_jaccard: 0.1804\n",
      "Epoch 428/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2608 - jaccard: 0.1772 - val_loss: 0.2650 - val_jaccard: 0.1771\n",
      "Epoch 429/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2614 - jaccard: 0.1788 - val_loss: 0.2632 - val_jaccard: 0.1817\n",
      "Epoch 430/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2598 - jaccard: 0.1791 - val_loss: 0.2631 - val_jaccard: 0.1811\n",
      "Epoch 431/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2595 - jaccard: 0.1786 - val_loss: 0.2632 - val_jaccard: 0.1854\n",
      "Epoch 432/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2591 - jaccard: 0.1850 - val_loss: 0.2632 - val_jaccard: 0.1855\n",
      "Epoch 433/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2587 - jaccard: 0.1812 - val_loss: 0.2627 - val_jaccard: 0.1807\n",
      "Epoch 434/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2591 - jaccard: 0.1800 - val_loss: 0.2643 - val_jaccard: 0.1839\n",
      "Epoch 435/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2595 - jaccard: 0.1816 - val_loss: 0.2632 - val_jaccard: 0.1813\n",
      "Epoch 436/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2584 - jaccard: 0.1818 - val_loss: 0.2623 - val_jaccard: 0.1833\n",
      "Epoch 437/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2585 - jaccard: 0.1826 - val_loss: 0.2621 - val_jaccard: 0.1852\n",
      "Epoch 438/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2585 - jaccard: 0.1829 - val_loss: 0.2626 - val_jaccard: 0.1822\n",
      "Epoch 439/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2584 - jaccard: 0.1828 - val_loss: 0.2625 - val_jaccard: 0.1838\n",
      "Epoch 440/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2575 - jaccard: 0.1810 - val_loss: 0.2617 - val_jaccard: 0.1833\n",
      "Epoch 441/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2587 - jaccard: 0.1786 - val_loss: 0.2630 - val_jaccard: 0.1841\n",
      "Epoch 442/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2582 - jaccard: 0.1870 - val_loss: 0.2625 - val_jaccard: 0.1855\n",
      "Epoch 443/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2583 - jaccard: 0.1809 - val_loss: 0.2625 - val_jaccard: 0.1804\n",
      "Epoch 444/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2571 - jaccard: 0.1828 - val_loss: 0.2616 - val_jaccard: 0.1873\n",
      "Epoch 445/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2586 - jaccard: 0.1822 - val_loss: 0.2622 - val_jaccard: 0.1818\n",
      "Epoch 446/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2566 - jaccard: 0.1854 - val_loss: 0.2620 - val_jaccard: 0.1867\n",
      "Epoch 447/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2567 - jaccard: 0.1826 - val_loss: 0.2614 - val_jaccard: 0.1826\n",
      "Epoch 448/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2579 - jaccard: 0.1832 - val_loss: 0.2614 - val_jaccard: 0.1811\n",
      "Epoch 449/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2576 - jaccard: 0.1817 - val_loss: 0.2616 - val_jaccard: 0.1881\n",
      "Epoch 450/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2564 - jaccard: 0.1855 - val_loss: 0.2621 - val_jaccard: 0.1824\n",
      "Epoch 451/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2564 - jaccard: 0.1835 - val_loss: 0.2614 - val_jaccard: 0.1827\n",
      "Epoch 452/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2569 - jaccard: 0.1827 - val_loss: 0.2611 - val_jaccard: 0.1860\n",
      "Epoch 453/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2562 - jaccard: 0.1852 - val_loss: 0.2613 - val_jaccard: 0.1852\n",
      "Epoch 454/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2562 - jaccard: 0.1833 - val_loss: 0.2615 - val_jaccard: 0.1818\n",
      "Epoch 455/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2562 - jaccard: 0.1840 - val_loss: 0.2603 - val_jaccard: 0.1873\n",
      "Epoch 456/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2557 - jaccard: 0.1874 - val_loss: 0.2607 - val_jaccard: 0.1869\n",
      "Epoch 457/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2550 - jaccard: 0.1829 - val_loss: 0.2605 - val_jaccard: 0.1848\n",
      "Epoch 458/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2558 - jaccard: 0.1874 - val_loss: 0.2609 - val_jaccard: 0.1867\n",
      "Epoch 459/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2545 - jaccard: 0.1848 - val_loss: 0.2607 - val_jaccard: 0.1838\n",
      "Epoch 460/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2547 - jaccard: 0.1865 - val_loss: 0.2598 - val_jaccard: 0.1890\n",
      "Epoch 461/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2541 - jaccard: 0.1870 - val_loss: 0.2602 - val_jaccard: 0.1849\n",
      "Epoch 462/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2564 - jaccard: 0.1845 - val_loss: 0.2602 - val_jaccard: 0.1856\n",
      "Epoch 463/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2550 - jaccard: 0.1835 - val_loss: 0.2609 - val_jaccard: 0.1863\n",
      "Epoch 464/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2546 - jaccard: 0.1870 - val_loss: 0.2598 - val_jaccard: 0.1876\n",
      "Epoch 465/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2545 - jaccard: 0.1845 - val_loss: 0.2597 - val_jaccard: 0.1876\n",
      "Epoch 466/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2543 - jaccard: 0.1896 - val_loss: 0.2603 - val_jaccard: 0.1853\n",
      "Epoch 467/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2547 - jaccard: 0.1846 - val_loss: 0.2597 - val_jaccard: 0.1853\n",
      "Epoch 468/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2551 - jaccard: 0.1848 - val_loss: 0.2600 - val_jaccard: 0.1873\n",
      "Epoch 469/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2546 - jaccard: 0.1852 - val_loss: 0.2599 - val_jaccard: 0.1847\n",
      "Epoch 470/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2545 - jaccard: 0.1869 - val_loss: 0.2597 - val_jaccard: 0.1866\n",
      "Epoch 471/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2549 - jaccard: 0.1872 - val_loss: 0.2597 - val_jaccard: 0.1851\n",
      "Epoch 472/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2538 - jaccard: 0.1856 - val_loss: 0.2595 - val_jaccard: 0.1851\n",
      "Epoch 473/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2554 - jaccard: 0.1859 - val_loss: 0.2592 - val_jaccard: 0.1863\n",
      "Epoch 474/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2536 - jaccard: 0.1849 - val_loss: 0.2598 - val_jaccard: 0.1822\n",
      "Epoch 475/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2542 - jaccard: 0.1860 - val_loss: 0.2590 - val_jaccard: 0.1914\n",
      "Epoch 476/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2538 - jaccard: 0.1851 - val_loss: 0.2589 - val_jaccard: 0.1843\n",
      "Epoch 477/1000\n",
      "320/320 [==============================] - 6s - loss: 0.2541 - jaccard: 0.1844 - val_loss: 0.2593 - val_jaccard: 0.1906\n",
      "Epoch 478/1000\n",
      "240/320 [=====================>........] - ETA: 1s - loss: 0.2488 - jaccard: 0.1861"
     ]
    }
   ],
   "source": [
    "run += 1\n",
    "def train_and_predict(model,fit=True,use_existing=False):\n",
    "    print('This is run number: '+ str(run) + '...')\n",
    "    \n",
    "    if use_existing:\n",
    "        model.load_weights('./data/model_weights_class_3_run_'+str(run)+'.hdf5')\n",
    "        \n",
    "    if fit:\n",
    "        \n",
    "        print('Fitting model...')\n",
    "        print('-'*30)\n",
    "        \n",
    "        quitter = EarlyStopping(monitor='loss', min_delta=0.001, patience=100, verbose=1, mode='auto')\n",
    "        lrreducer = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, verbose=1, mode='auto', epsilon=0.001, cooldown=2, min_lr=0)\n",
    "        model_checkpoint = ModelCheckpoint('./data/model_weights_class_3_run_'+str(run)+'.hdf5', monitor='loss', save_best_only=True)\n",
    "        csvlogger = CSVLogger('./data/training_log_run_'+str(run), separator=',', append=False)\n",
    "\n",
    "        #tensorboard = TensorBoard(log_dir='./logs/'+'run_'+str(run), histogram_freq=0, write_graph=True, write_images=False)\n",
    "        '''\n",
    "        screen -S tensorboard\n",
    "        tensorboard --logdir=logs\n",
    "        <ctrl + a,d to exit>\n",
    "        screen -r tensorboard\n",
    "        '''\n",
    "        \n",
    "        model.fit(x, y_oneclass,\n",
    "                  batch_size=80,\n",
    "                  nb_epoch=1000,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  callbacks=[model_checkpoint,csvlogger],\n",
    "                  validation_split=0.2,\n",
    "                  initial_epoch=0)\n",
    "    \n",
    "        print('Predicting masks on test data...')\n",
    "        print('-'*30)\n",
    "        \n",
    "    imgs_mask_test = model.predict(x, verbose=1)\n",
    "    np.save('jaccard_preds_all_data.npy', imgs_mask_test)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = train_and_predict(model,fit=True,use_existing=False)\n",
    "push('Training is done',\n",
    "     'Train loss: %f, train jaccard: %f, val loss %f, val jaccard%f' %(model.history.history['loss'][-1],model.history.history['jaccard'][-1],model.history.history['val_loss'][-1],model.history.history['val_jaccard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.load('jaccard_preds_all_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(x[120:150,...],y_oneclass[120:150,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm = False\n",
    "if norm:\n",
    "    preds = norm_preds(preds)\n",
    "i = np.random.choice(range(120),1)[0]\n",
    "print(i)\n",
    "plot_all(i,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_everything():\n",
    "    fig = plt.figure(figsize=(12,36))\n",
    "    #num = np.random.randint(10,25,10)\n",
    "    num = np.arange(0,10)\n",
    "    for i in range(1,10):\n",
    "        ax1 = fig.add_subplot(10,8,8*i-7)\n",
    "        ax1.imshow(preds[num[i],0,...],cmap='spectral')\n",
    "        \n",
    "        ax2 = fig.add_subplot(10,8,8*i-6)\n",
    "        ax2.imshow(y_oneclass[num[i],0,...],cmap='spectral')\n",
    "        \n",
    "        ax3 = fig.add_subplot(10,8,8*i-5)\n",
    "        ax3.imshow(preds[num[i],1,...],cmap='spectral')\n",
    "        \n",
    "        ax4 = fig.add_subplot(10,8,8*i-4)\n",
    "        ax4.imshow(y_oneclass[num[i],1,...],cmap='spectral')\n",
    "        \n",
    "        ax5 = fig.add_subplot(10,8,8*i-3)\n",
    "        ax5.imshow(preds[num[i],2,...],cmap='spectral')\n",
    "        \n",
    "        ax6 = fig.add_subplot(10,8,8*i-2)\n",
    "        ax6.imshow(y_oneclass[num[i],2,...],cmap='spectral')\n",
    "        \n",
    "        ax7 = fig.add_subplot(10,8,8*i-1)\n",
    "        ax7.imshow(preds[num[i],3,...],cmap='spectral')\n",
    "        \n",
    "        ax8 = fig.add_subplot(10,8,8*i)\n",
    "        ax8.imshow(y_oneclass[num[i],3,...],cmap='spectral')\n",
    "        \n",
    "    plt.show()\n",
    "plot_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_classifier(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defunct bits and pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_cancelled(model):\n",
    "    model.load_weights('./data/model_weights_class_3_run_66.hdf5')\n",
    "    imgs_mask_test = model.predict(x, verbose=1)\n",
    "    return imgs_mask_test\n",
    "\n",
    "preds = load_cancelled(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_classifier(model):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(model.history.history['loss'], \"-\",color='blue',label=\"Training final loss: \"+str(round(model.history.history['loss'][-1],4)))\n",
    "    ax.plot(model.history.history['jaccard'], \"-\",color='orange',label=\"testing final loss: \"+str(round(model.history.history['jaccard'][-1],4)))\n",
    "    #ax.set_xlim([0, epochs])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Jaccard / Loss')\n",
    "    plt.legend(loc='best')\n",
    "    ax.set_title('Metrics vs time (epochs)')\n",
    "    \n",
    "    '''ax = fig.add_subplot(122)\n",
    "    ax.plot(model.history['acc'], \"-\",color='blue',label=\"Training final acc: \"+str(round(model.history['acc'][-1],4)))\n",
    "    ax.plot(model.history['val_acc'], \"-\",color='orange',label=\"testing final acc: \"+str(round(model.history['val_acc'][-1],4)))\n",
    "    #ax.set_xlim([0, epochs])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    ax.set_title('Accuracy vs time (epochs)')    \n",
    "    '''\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the training data\n",
    "def import_data(load_whitened = False, normalize = True):\n",
    "    if load_whitened:\n",
    "        with open('./data/x_whitened_resized_array.pickle','rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            x = x.astype('float32')\n",
    "    else:\n",
    "        with open('./data/x_resized_array.pickle','rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            x = x.astype('float32')\n",
    "            # Normalize data to max values\n",
    "            for i in range(x.shape[0]):\n",
    "                for j in range(x.shape[1]):\n",
    "                    x[i,j,:,:] *= 1/x[i,j,:,:].max()\n",
    "                    \n",
    "    # Normalize values between 1 and 0\n",
    "    if normalize:\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                #x[i,j,:,:] /= x[i,j,:,:].max()\n",
    "                x[i,j,:,:] = (x[i,j,:,:].min() - x[i,j,:,:])/(x[i,j,:,:].min() - x[i,j,:,:].max())\n",
    "\n",
    "    with open('./data/y_resized_raster.pickle','rb') as f:\n",
    "        y = pickle.load(f)\n",
    "        y = y.astype(np.float32)\n",
    "        \n",
    "    y_oneclass = y[:,1:6,...]\n",
    "    \n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''\n",
    "    \n",
    "    return x, y, y_oneclass\n",
    "    \n",
    "    # Just use a single class: roads\n",
    "    #y = y[:,4,:,:]\n",
    "    #y = y[:,np.newaxis,:,:]\n",
    "\n",
    "    # y = y.reshape(y.shape[0],-1)\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def np_jaccard(y_true,y_pred,smooth=1.):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true * np.rint(y_pred))\n",
    "    return (intersection+smooth) / (np.sum(y_true) + np.sum(y_pred) - intersection+smooth)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
