{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model / scratch pad\n",
    "\n",
    "- Simplify data and attempt to overfit with a single class\n",
    "- Make jaccard cost function to assess model performance in a way that matches submission ranking\n",
    "- Use datagenerator to randomly change images and increase the training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
    "from keras.layers import Activation, Dense, Reshape, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Deconvolution2D, UpSampling2D, ZeroPadding2D,Cropping2D#, Unpooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from keras.backend import set_image_dim_ordering\n",
    "set_image_dim_ordering('th')\n",
    "\n",
    "from shapely.ops import cascaded_union\n",
    "from shapely.geometry import MultiPolygon,asShape, Polygon\n",
    "from shapely.geometry.polygon import Polygon,asPolygon\n",
    "#from rasterio.features import shapes,rasterize\n",
    "from rasterio import features\n",
    "from shapely.affinity import scale # scales the wkt values using the description in the tutorial\n",
    "\n",
    "from shapely import speedups\n",
    "speedups.available\n",
    "speedups.enable()\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the components of the pushbullet API\n",
    "\n",
    "from pushbullet import Pushbullet\n",
    "with open('../pushbullet.api','r') as f:\n",
    "    api = f.readline().strip()   \n",
    "pb = Pushbullet(api)\n",
    "phone = pb.devices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell after anything you want to be notified about!\n",
    "def push(title='Done!',text='Whatever it was, it\\'s done'):\n",
    "    phone.push_note(title,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the training data\n",
    "# To do: train_test_split the data first\n",
    "with open('./data/x_resized_array.pickle','rb') as f:\n",
    "    x = pickle.load(f)\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        x[i,j,:,:] *= 1/x[i,j,:,:].max()\n",
    "        \n",
    "with open('./data/y_resized_raster.pickle','rb') as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "# Just use a single class: roads\n",
    "#y = y[:,4,:,:]\n",
    "#y = y[:,np.newaxis,:,:]\n",
    "\n",
    "# y = y.reshape(y.shape[0],-1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=42)\n",
    "\n",
    "n_train, n_channels, height, width = x_train.shape\n",
    "n_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 136, 136)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make training data with only one class\n",
    "# Remember to change the last layer in the network to ahve a single output\n",
    "# Also change the range(10) in the polygonize_data() list comprehension to range(1)\n",
    "# Finally, obviously feed the correct training data to the model.fit() method\n",
    "\n",
    "y_one = y[:,3,:,:]\n",
    "y_one = y_one[:,np.newaxis,:,:]\n",
    "x_one_train, x_one_test, y_one_train, y_one_test = train_test_split(x,y_one,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "See https://github.com/fchollet/keras/issues/1287\n",
    "\n",
    "This seems to be working well as an autoencoder\n",
    "'''\n",
    "\n",
    "lr = 0.0001\n",
    "decay = 1e-3\n",
    "n_conv = 3\n",
    "n_filters = 32\n",
    "batch_size = 1\n",
    "\n",
    "leaky = LeakyReLU()\n",
    "sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "adagrad = Adagrad(lr=0.001, epsilon=1e-08, decay=0.0)\n",
    "adadelta = Adadelta()\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=decay)\n",
    "act = 'relu'\n",
    "init = 'he_normal'\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#############################\n",
    "### Convolution layer 1   ###\n",
    "#############################\n",
    "\n",
    "model.add(ZeroPadding2D(padding=(28,28),input_shape=(1,136,136)))\n",
    "model.add(Convolution2D(8,n_conv,n_conv,border_mode='same',init=init,input_shape=(1,136,136)))\n",
    "model.add(Activation(act))\n",
    "\n",
    "model.add(Convolution2D(8,n_conv,n_conv,border_mode='same',init=init))\n",
    "model.add(Activation(act))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),border_mode='same'))\n",
    "\n",
    "#############################\n",
    "### Convolution layer 2   ###\n",
    "#############################\n",
    "\n",
    "model.add(Convolution2D(16,n_conv,n_conv,border_mode='same',init=init))\n",
    "model.add(Activation(act))\n",
    "\n",
    "model.add(Convolution2D(16,n_conv,n_conv,border_mode='same',init=init))\n",
    "model.add(Activation(act))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),border_mode='same'))\n",
    "\n",
    "#############################\n",
    "##  Deconvolution layer 1  ##\n",
    "#############################\n",
    "\n",
    "model.add(Convolution2D(8,n_conv,n_conv,border_mode='same',init=init))\n",
    "model.add(Activation(act))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(8,n_conv,n_conv,border_mode='same',init=init))\n",
    "model.add(Activation(act))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "#############################\n",
    "##  Deconvolution layer 2  ##\n",
    "#############################\n",
    "\n",
    "model.add(Convolution2D(16,n_conv,n_conv,border_mode='same',init=init))\n",
    "model.add(Activation(act))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(16,n_conv,n_conv,border_mode='same',init=init))\n",
    "model.add(Activation(act))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "#############################\n",
    "###    Output layer 1     ###\n",
    "#############################\n",
    "\n",
    "model.add(Convolution2D(1,1,1,border_mode='same',init=init))\n",
    "model.add(Activation(act))\n",
    "\n",
    "model.add(Cropping2D(cropping=((28,28), (28,28))))\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Execute this in order to use a data generator to expand the number of images.\n",
    "Note: adjust the variables to actually create random images\n",
    "'''\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True)  # randomly flip images\n",
    "\n",
    "datagen.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2s - loss: 0.5907\n",
      "Epoch 2/100\n",
      "2s - loss: 0.5778\n",
      "Epoch 3/100\n",
      "3s - loss: 0.5669\n",
      "Epoch 4/100\n",
      "2s - loss: 0.5658\n",
      "Epoch 5/100\n",
      "3s - loss: 0.5658\n",
      "Epoch 6/100\n",
      "3s - loss: 0.5681\n",
      "Epoch 7/100\n",
      "3s - loss: 0.5629\n",
      "Epoch 8/100\n",
      "3s - loss: 0.5636\n",
      "Epoch 9/100\n",
      "3s - loss: 0.5594\n",
      "Epoch 10/100\n",
      "3s - loss: 0.5627\n",
      "Epoch 11/100\n",
      "2s - loss: 0.5646\n",
      "Epoch 12/100\n",
      "3s - loss: 0.5546\n",
      "Epoch 13/100\n",
      "3s - loss: 0.5567\n",
      "Epoch 14/100\n",
      "3s - loss: 0.5534\n",
      "Epoch 15/100\n",
      "3s - loss: 0.5553\n",
      "Epoch 16/100\n",
      "3s - loss: 0.5730\n",
      "Epoch 17/100\n",
      "3s - loss: 0.5506\n",
      "Epoch 18/100\n",
      "3s - loss: 0.5505\n",
      "Epoch 19/100\n",
      "3s - loss: 0.5503\n",
      "Epoch 20/100\n",
      "2s - loss: 0.5482\n",
      "Epoch 21/100\n",
      "3s - loss: 0.5554\n",
      "Epoch 22/100\n",
      "3s - loss: 0.5456\n",
      "Epoch 23/100\n",
      "3s - loss: 0.5465\n",
      "Epoch 24/100\n",
      "2s - loss: 0.5445\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 11\n",
    "classType = 4\n",
    "\n",
    "'''outputs = model.fit_generator(datagen.flow(y, y,\n",
    "                        batch_size=batch_size),\n",
    "                        samples_per_epoch=128,\n",
    "                        nb_epoch=n_epochs)'''\n",
    "\n",
    "# Uncomment this to fit without the image generator\n",
    "outputs = model.fit(y,\n",
    "                    y,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=n_epochs,\n",
    "                   verbose=2)\n",
    "                   \n",
    "training_outcomes = model.predict(y[:,classType,:,:],batch_size=batch_size)\n",
    "push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_all(classType,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict off train data and plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_outcomes = model.predict(y,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.067313567"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(training_outcomes[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_all(classType,i):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.imshow(training_outcomes[i,classType,:,:],cmap='spectral')\n",
    "\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.imshow(training_outcomes[i,classType,:,:]>np.median(training_outcomes[5,0,:,:]),cmap='spectral')\n",
    "\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax3.imshow(x[i,16,:,:],cmap='spectral')\n",
    "\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    ax4.imshow(y[i,classType,:,:],cmap='spectral')\n",
    "plot_all(classType,5)\n",
    "#push('PICTURES!','The plots are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the testing data\n",
    "# This is used for generating a submission\n",
    "with open('./data/submission_resized_array.pickle','rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the testing data into values and keys\n",
    "# It's currently a dict\n",
    "test_values = np.array(list(test_data.values()))\n",
    "test_ids = np.array(list(test_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict the rasters for the testing data\n",
    "# Save the prediction results immediately so if something happens I don't ahve to train the model again\n",
    "\n",
    "predictions = model.predict(test_values)\n",
    "with open('./data/predictions.pickle','wb') as f:\n",
    "    pickle.dump(predictions,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the predictions from file\n",
    "with open('./data/predictions.pickle','rb') as f:\n",
    "    predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load all the various train_wkt files from kaggle...\n",
    "\n",
    "train_wkt = pd.read_csv('./data/train_wkt_v2.csv',names=['imageID','feature','wkt'],skiprows=1)\n",
    "train_wkt_old = pd.read_csv('./data/train_wkt.csv',names=['imageID','feature','wkt'],skiprows=1)\n",
    "train_wkt_v3 = pd.read_csv('./data/train_wkt_v3.csv',names=['imageID','feature','wkt'],skiprows=1)\n",
    "\n",
    "grid_sizes = pd.read_csv('./data/grid_sizes.csv',names=['imageID','xmax','ymin'],skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polygonize_data():\n",
    "    def polygonize(raster,feature,imageID):\n",
    "        width_height_prime = 136*(136/(136+1))\n",
    "        mask = raster != 0\n",
    "        shape = cascaded_union([asShape(g) for g,_ in features.shapes(raster,mask=mask)])\n",
    "        xmax = grid_sizes.loc[grid_sizes.imageID==imageID].xmax.values[0]\n",
    "        ymin = grid_sizes.loc[grid_sizes.imageID==imageID].ymin.values[0]\n",
    "        #print(feature,imageID)\n",
    "        return [imageID,feature+1,scale(shape,xfact=xmax/width_height_prime, yfact=ymin/width_height_prime,origin=(0,0)).wkt]\n",
    "    \n",
    "    polygons = [polygonize(raster[feature,:,:].astype('float32'),feature,test_ids[n]) for n,raster in enumerate([predictions[n,:,:,:] for n in range(predictions.shape[0])]) for feature in range(10)]\n",
    "\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call the polygonize_data() function to generate the polygons from the predictions\n",
    "\n",
    "submission_shapes = pd.DataFrame(polygonize_data(),columns=['ImageId','ClassType','MultipolygonWKT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove the imageIDs not found in train_wkt_old\n",
    "\n",
    "submission_shapes = submission_shapes[~submission_shapes.ImageId.isin(train_wkt_old.imageID.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert column MultipolygonWKT to string\n",
    "\n",
    "submission_shapes['MultipolygonWKT'] = submission_shapes['MultipolygonWKT'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace text for empty polygons with that required in the submission guidelines\n",
    "\n",
    "submission_shapes.replace(to_replace='GEOMETRYCOLLECTION EMPTY', value='MULTIPOLYGON EMPTY',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_shapes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the submission file in compressed format\n",
    "\n",
    "submission_shapes.to_csv('./data/submission.gz',index=False,compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
