{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rasterize WKTs and load images\n",
    "\n",
    "This is heavily incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import io\n",
    "import tifffile as tiff\n",
    "import zipfile\n",
    "\n",
    "#import skimage.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First get all images resized to 160x160 and stored in a single array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_ids = pd.read_csv('./data/sample_submission.csv')\n",
    "training_ids = pd.read_csv('./data/train_wkt_v4.csv',names=['imageID','feature','wkt'],skiprows=1)\n",
    "grid_sizes = pd.read_csv('./data/grid_sizes.csv',names=['imageID','xmax','ymin'],skiprows=1)\n",
    "three = zipfile.ZipFile('./data/three_band.zip')\n",
    "sixteen = zipfile.ZipFile('./data/sixteen_band.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This just normalizes between the 5th and 95th percentile, and sets anything outside that to 0,1\n",
    "\n",
    "def normalize(img, l_percentile=5, h_percentile=95):\n",
    "    output = np.zeros_like(img).astype(np.float32) # return array of zeros with same dimensions as given array\n",
    "    for i in range(img.shape[0]): # For each band\n",
    "        min_ = np.percentile(img[i,...], l_percentile)\n",
    "        max_ = np.percentile(img[i,..., h_percentile)\n",
    "        norm = (img[i,...] - min_)/ (max_ - min_)\n",
    "        norm[norm < 0] = 0\n",
    "        norm[norm > 1] = 1\n",
    "        output[i,...] = norm\n",
    "\n",
    "    return output.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_polygons(imageId, class_):\n",
    "    img = training_ids[training_ids.imageId == imageId]\n",
    "    img_wkt = img[img.feature == class_].wkt\n",
    "    polygon = None\n",
    "    if len(img_wkt) > 0:\n",
    "        assert len(img_wkt) == 1 # test condition, trigger error if it's false, i.e. there are multiple wkts here\n",
    "        polygon = wkt_loads(img_wkt.values[0]) # from shapely.wkt import loads as wkt_loads\n",
    "    return polygon\n",
    "\n",
    "def get_coordinates(polygons, raster_img_size, xmax, ymin):\n",
    "    perimeters = []\n",
    "    interiors = []\n",
    "    \n",
    "    if polygons is None:\n",
    "        return None\n",
    "    \n",
    "    for k in range(len(polygons)):\n",
    "        # Finds the outer coords of the polygon\n",
    "        perimeter = np.array(list(polygons[k].exterior.coords))\n",
    "        # Converts the coords using the scaling factor\n",
    "        perimeter_contour = convert_coords(perim, raster_img_size, xmax, ymin)\n",
    "        # Appends the outer coords to a list\n",
    "        perimeters.append(perimeter_contour)\n",
    "        # Then for each interior in that polygon...\n",
    "        for i in polygons[k].interiors:\n",
    "            # Finds the interior coords\n",
    "            interior = np.array(list(i.coords))\n",
    "            # Converts the coords using the scaling factor \n",
    "            interior_contour = convert_coords(interior, raster_img_size, xmax, ymin)\n",
    "            # Appends them to a list\n",
    "            interiors.append(interior_contour)\n",
    "    return perimeters, interiors\n",
    "\n",
    "def convert_coords(coords, img_size, xmax, ymin):\n",
    "    img_h, img_w = img_size\n",
    "    width_prime = img_w*(img_w / (img_w + 1))\n",
    "    height_prime = img_h*(img_h / (img_h + 1))\n",
    "    \n",
    "    xf = width_prime / xmax\n",
    "    yf = height_prime / ymax\n",
    "    \n",
    "    coords[:, 1] *= yf\n",
    "    coords[:, 0] *= xf\n",
    "    coords = np.round(coords).astype(np.int32)\n",
    "    return coords\n",
    "\n",
    "def plot_mask(raster_img_size, coordinates):\n",
    "    msk = np.zeros(raster_img_size, np.uint8)\n",
    "    \n",
    "    if coordinates is None:\n",
    "        return img_mask\n",
    "    \n",
    "    perim_list, interior_list = coordinates\n",
    "    cv2.fillPoly(msk, perim_list, 1)\n",
    "    cv2.fillPoly(msk, interior_list, 0)\n",
    "    return msk\n",
    "\n",
    "def make_masks(raster_size=(160,160), imageId, class_):\n",
    "    # First get the xmax and ymax\n",
    "    xmax, ymin = grid_sizes[grid_sizes.ImageId == imageId].iloc[0, 1:].astype(float)\n",
    "    \n",
    "    # Next, get the polygon list from the train_wkts file\n",
    "    polygons = get_polygons(imageID,class_)\n",
    "    \n",
    "    # Convert polygons to contours and re-factor the coords\n",
    "    coordinates = get_coordinates(polygons, raster_size, xmax, ymin)\n",
    "    \n",
    "    # Makes actual image masks from the coords\n",
    "    mask = plot_mask(raster_size, coordinates)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make training images and masks\n",
    "\n",
    "train_imgs = {}\n",
    "train_msks = {}\n",
    "\n",
    "for id in sorted(training_ids['Image_ID'].unique()):\n",
    "    \n",
    "    image_id_image = []\n",
    "    image_id_msks = []\n",
    "    # Open the threeband zipfile and get the image\n",
    "    with three_band.open('sixteen_band/{}.tif'.format(image_id,k)) as f:\n",
    "        img = tiff.imread(io.BytesIO(f.read())).astype(np.float32)\n",
    "        img = skimage.transform.resize(img,(img.shape[0],160,160))\n",
    "        img = normalize(img)\n",
    "        image_id_image.append(img)\n",
    "    \n",
    "    # Open the sixteen band zipfile and get the images\n",
    "    for k in ['A','M','P']:\n",
    "        with sixteen_band.open('sixteen_band/{}_{}.tif'.format(image_id,k)) as f:\n",
    "            img = tiff.imread(io.BytesIO(f.read())).astype(np.float32)\n",
    "        \n",
    "            if k == 'P':\n",
    "                img = img[np.newaxis,...]\n",
    "\n",
    "            img = skimage.transform.resize(img,(img.shape[0],160,160))\n",
    "            img = normalize(img)\n",
    "            image_id_image.append(img)\n",
    "        \n",
    "    for class_ in range(10):\n",
    "        image_id_msks.append(make_masks((160,160,)id,class_)) # NB THIS HAS TO BE DONE STILL\n",
    "    \n",
    "    train_imgs[id] = np.array(image_id_image)\n",
    "    train_msks[id] = np.array(image_id_msks)\n",
    "\n",
    "np.save('./data/train_imgs.npy',train_imgs)\n",
    "np.save('./data/train_msks.npy',np.array(train_msks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now make a submission\n",
    "\n",
    "submission_images = {}\n",
    "\n",
    "for id in sorted(submission_ids['Image_ID'].unique()):\n",
    "    \n",
    "    image_id_image = []\n",
    "    \n",
    "    # Open the threeband zipfile and get the image\n",
    "    with three_band.open('sixteen_band/{}.tif'.format(image_id,k)) as f:\n",
    "        img = tiff.imread(io.BytesIO(f.read())).astype(np.float32)\n",
    "        img = skimage.transform.resize(img,(img.shape[0],160,160))\n",
    "        img = normalize(img)\n",
    "        image_id_image.append(img)\n",
    "    \n",
    "    # Open the sixteen band zipfile and get the images\n",
    "    for k in ['A','M','P']:\n",
    "        with sixteen_band.open('sixteen_band/{}_{}.tif'.format(image_id,k)) as f:\n",
    "            img = tiff.imread(io.BytesIO(f.read())).astype(np.float32)\n",
    "        \n",
    "            if k == 'P':\n",
    "                img = img[np.newaxis,...]\n",
    "\n",
    "            img = skimage.transform.resize(img,(img.shape[0],160,160))\n",
    "            img = normalize(img)\n",
    "            image_id_image.append(img)\n",
    "        \n",
    "    submission_images[id] = np.array(image_id_image)\n",
    "\n",
    "np.save('./data/submission_images.npy',train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old stuff. This worked. For reference.\n",
    "for file in three.namelist():\n",
    "    print(file)\n",
    "    with three.open(file) as f:\n",
    "        img = tiff.imread(io.BytesIO(f.read())).astype(np.float32)\n",
    "        if file[-5:-4] == 'P':\n",
    "            img = img[np.newaxis,...]\n",
    "        img = skimage.transform.resize(img,(img.shape[0],160,160))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
