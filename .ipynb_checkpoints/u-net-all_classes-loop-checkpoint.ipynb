{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U-Net architecture\n",
    "\n",
    "See [here](https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py#L19) for code and [here](https://arxiv.org/pdf/1505.04597.pdf) for the original literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def restart_kernel(restart=False):\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    if restart:\n",
    "        app.kernel.do_shutdown(True)\n",
    "\n",
    "restart_kernel(False)\n",
    "\n",
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=gpu,lib.cnmem=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Reshape, Input, merge, Activation, Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "# \"tf\" assumes (rows, cols, channels) while \"th\" assumes (channels, rows, cols)\n",
    "# Possibly change this around natively in the data so the backend doesn't have to switch them\n",
    "# Only necessary if I use TF!\n",
    "\n",
    "import skimage.exposure\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pushbullet import Pushbullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run # 126\n"
     ]
    }
   ],
   "source": [
    "# Pushbullet notifier\n",
    "def push(title='Done!',text=''):\n",
    "    Pushbullet('o.YFPNNPfGRekivaCGHa4qMSgjZt8zJ6FL').devices[0].push_note(title,text)\n",
    "    \n",
    "# Import the training data\n",
    "def import_data(img_h,img_w):\n",
    "    \n",
    "    x_train = np.load('./data/training-data/tiled_patched_images_576_576_train.npy')\n",
    "    x_test = np.load('./data/training-data/tiled_patched_images_576_576_test.npy')\n",
    "    y_train = np.load('./data/training-data/tiled_patched_masks_576_576_train.npy')\n",
    "    y_test = np.load('./data/training-data/tiled_patched_masks_576_576_test.npy')\n",
    "    \n",
    "    #train_imgs = np.load('./data/training-data/resized_data_{}x{}_imgs.npy'.format(img_w,img_h))[()]\n",
    "    #train_msks = np.load('./data/training-data/resized_data_{}x{}_msks.npy'.format(img_w,img_h))[()]\n",
    "    \n",
    "    #x = np.rollaxis(np.concatenate([i[np.newaxis,...] for i in train_imgs.values()],axis=0),3,1)\n",
    "    #y = np.rollaxis(np.concatenate([i[np.newaxis,...] for i in train_msks.values()],axis=0),3,1).astype(np.float32)\n",
    "    #y = y.reshape((y.shape[0],y.shape[1],y.shape[2]*y.shape[3]))\n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''    \n",
    "    return x_train,x_test, y_train,y_test\n",
    "\n",
    "img_h,img_w = 160,160\n",
    "x_train,x_test, y_train,y_test = import_data(img_h,img_w)\n",
    "\n",
    "# Increment the counter\n",
    "def counter():\n",
    "    run = np.load('./data/misc/run_counter.npy')\n",
    "    run += 1\n",
    "    np.save('./data/misc/run_counter.npy',run)\n",
    "    return run\n",
    "run = counter()\n",
    "\n",
    "# Set the counter to a specific value\n",
    "def set_counter(run):\n",
    "    run = run\n",
    "    np.save('./data/misc/run_counter.npy',run)\n",
    "    return run\n",
    "# Uncomment the next line to manually set the counter if something goes wrong\n",
    "run = set_counter(126)\n",
    "print('This is run # %i' %run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 133 ms, total: 3.19 s\n",
      "Wall time: 6.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def compiler(img_rows = x_train.shape[2],img_cols = x_train.shape[3],\n",
    "            nfilters = 64,activation = 'relu',init = 'he_normal',\n",
    "            lr=0.001,decay=0.0,p=[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2]):\n",
    "    \n",
    "    def jaccard(y_true, y_pred,smooth=1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "    \n",
    "    def Conv2DReluBatchNorm(n_filter, w_filter, h_filter, inputs, activation, init='he_uniform',dropout=0.2):\n",
    "        return BatchNormalization(mode=2, axis=1)(Activation(activation=activation)((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init)(inputs))))\n",
    "        \n",
    "    def up_conv(nfilters,filter_factor,inputs,init=init,activation=activation):\n",
    "        return BatchNormalization(mode=2, axis=1)(Activation(activation=activation)(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init)(UpSampling2D(size=(2, 2))(inputs))))\n",
    "\n",
    "    inputs = Input((20, img_rows, img_cols))\n",
    "    \n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, inputs, activation=activation,init=init)\n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, conv1, activation=activation,init=init)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(p=p[0])(pool1)\n",
    "\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, pool1, activation=activation,init=init)\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv2, activation=activation,init=init)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(p=p[1])(pool2)\n",
    "\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, pool2, activation=activation,init=init)\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv3, activation=activation,init=init)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(p=p[2])(pool3)\n",
    "\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, pool3, activation=activation,init=init)\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv4, activation=activation,init=init)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = Dropout(p=p[3])(pool4)\n",
    "\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, pool4, activation=activation,init=init)\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, conv5, activation=activation,init=init)\n",
    "    conv5 = Dropout(p=p[4])(conv5)\n",
    "        \n",
    "    up6 = merge([up_conv(nfilters,8,conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, up6, activation=activation,init=init)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv6, activation=activation,init=init)\n",
    "    conv6 = Dropout(p=p[5])(conv6)\n",
    "\n",
    "    up7 = merge([up_conv(nfilters,4,conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, up7, activation=activation,init=init)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv7, activation=activation,init=init)\n",
    "    conv7 = Dropout(p=p[6])(conv7)\n",
    "\n",
    "    up8 = merge([up_conv(nfilters,2,conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, up8, activation=activation,init=init)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv8, activation=activation,init=init)\n",
    "    conv8 = Dropout(p=p[7])(conv8)\n",
    "\n",
    "    up9 = merge([up_conv(nfilters,1,conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, up9, activation=activation,init=init)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, conv9, activation=activation,init=init)\n",
    "    conv9 = Dropout(p=p[8])(conv9)\n",
    "    \n",
    "    conv10 = Conv2DReluBatchNorm(1, 1, 1, conv9, activation='relu',init=init)\n",
    "    output = Activation(activation='sigmoid')(conv10)\n",
    "    \n",
    "    model = Model(input=inputs, output=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=lr,decay=decay), loss='binary_crossentropy',metrics=[jaccard])\n",
    "\n",
    "    return model\n",
    "\n",
    "#p = [0]*9\n",
    "p=[0.1,0.2,0.3,0.4,0.5,0.4,0.3,0.2,0.1] # current version\n",
    "#p=[0.2,0.3,0.4,0.5,0.5,0.5,0.4,0.3,0.2] # symmetric but more dropout\n",
    "#p=[0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6] # increasing\n",
    "\n",
    "model = compiler(img_rows=x_train.shape[2],img_cols=x_train.shape[3],\n",
    "            nfilters=64,activation='relu',init='he_normal',\n",
    "            lr=0.001,p=p)\n",
    "model.save_weights('./data/misc/initial_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run # 126 for class 3\n",
      "Train on 260 samples, validate on 64 samples\n",
      "Epoch 1/1000\n",
      "260/260 [==============================] - 60s - loss: 0.7145 - jaccard: 0.0690 - val_loss: 0.6785 - val_jaccard: 0.0732\n",
      "Epoch 2/1000\n",
      "260/260 [==============================] - 63s - loss: 0.6637 - jaccard: 0.0709 - val_loss: 0.6536 - val_jaccard: 0.0729\n",
      "Epoch 3/1000\n",
      "260/260 [==============================] - 63s - loss: 0.6403 - jaccard: 0.0715 - val_loss: 0.6269 - val_jaccard: 0.0727\n",
      "Epoch 4/1000\n",
      "260/260 [==============================] - 63s - loss: 0.6176 - jaccard: 0.0715 - val_loss: 0.6067 - val_jaccard: 0.0748\n",
      "Epoch 5/1000\n",
      "260/260 [==============================] - 63s - loss: 0.5966 - jaccard: 0.0717 - val_loss: 0.5869 - val_jaccard: 0.0771\n",
      "Epoch 6/1000\n",
      "260/260 [==============================] - 63s - loss: 0.5770 - jaccard: 0.0723 - val_loss: 0.5694 - val_jaccard: 0.0735\n",
      "Epoch 7/1000\n",
      "260/260 [==============================] - 63s - loss: 0.5610 - jaccard: 0.0729 - val_loss: 0.5527 - val_jaccard: 0.0771\n",
      "Epoch 8/1000\n",
      "260/260 [==============================] - 63s - loss: 0.5454 - jaccard: 0.0728 - val_loss: 0.5343 - val_jaccard: 0.0743\n",
      "Epoch 9/1000\n",
      "260/260 [==============================] - 63s - loss: 0.5292 - jaccard: 0.0746 - val_loss: 0.5254 - val_jaccard: 0.0757\n",
      "Epoch 10/1000\n",
      "260/260 [==============================] - 63s - loss: 0.5134 - jaccard: 0.0727 - val_loss: 0.5102 - val_jaccard: 0.0854\n",
      "Epoch 11/1000\n",
      "260/260 [==============================] - 60s - loss: 0.5055 - jaccard: 0.0729 - val_loss: 0.5116 - val_jaccard: 0.0830\n",
      "Epoch 12/1000\n",
      "260/260 [==============================] - 63s - loss: 0.4892 - jaccard: 0.0708 - val_loss: 0.4821 - val_jaccard: 0.0800\n",
      "Epoch 13/1000\n",
      "260/260 [==============================] - 63s - loss: 0.4755 - jaccard: 0.0735 - val_loss: 0.4728 - val_jaccard: 0.0856\n",
      "Epoch 14/1000\n",
      "260/260 [==============================] - 63s - loss: 0.4631 - jaccard: 0.0723 - val_loss: 0.4646 - val_jaccard: 0.0738\n",
      "Epoch 15/1000\n",
      "260/260 [==============================] - 63s - loss: 0.4507 - jaccard: 0.0736 - val_loss: 0.4550 - val_jaccard: 0.0835\n",
      "Epoch 16/1000\n",
      "260/260 [==============================] - 63s - loss: 0.4421 - jaccard: 0.0722 - val_loss: 0.4429 - val_jaccard: 0.0704\n",
      "Epoch 17/1000\n",
      "260/260 [==============================] - 63s - loss: 0.4315 - jaccard: 0.0697 - val_loss: 0.4268 - val_jaccard: 0.0712\n",
      "Epoch 18/1000\n",
      "260/260 [==============================] - 60s - loss: 0.4195 - jaccard: 0.0711 - val_loss: 0.4335 - val_jaccard: 0.0887\n",
      "Epoch 19/1000\n",
      "260/260 [==============================] - 63s - loss: 0.4100 - jaccard: 0.0743 - val_loss: 0.4225 - val_jaccard: 0.0843\n",
      "Epoch 20/1000\n",
      "260/260 [==============================] - 63s - loss: 0.4032 - jaccard: 0.0724 - val_loss: 0.4041 - val_jaccard: 0.0726\n",
      "Epoch 21/1000\n",
      "260/260 [==============================] - 63s - loss: 0.3911 - jaccard: 0.0745 - val_loss: 0.3931 - val_jaccard: 0.0856\n",
      "Epoch 22/1000\n",
      "260/260 [==============================] - 60s - loss: 0.3826 - jaccard: 0.0752 - val_loss: 0.3947 - val_jaccard: 0.0909\n",
      "Epoch 23/1000\n",
      "260/260 [==============================] - 59s - loss: 0.3756 - jaccard: 0.0725 - val_loss: 0.4012 - val_jaccard: 0.0869\n",
      "Epoch 24/1000\n",
      "260/260 [==============================] - 63s - loss: 0.3688 - jaccard: 0.0738 - val_loss: 0.3708 - val_jaccard: 0.0834\n",
      "Epoch 25/1000\n",
      "260/260 [==============================] - 60s - loss: 0.3617 - jaccard: 0.0716 - val_loss: 0.3952 - val_jaccard: 0.0978\n",
      "Epoch 26/1000\n",
      "260/260 [==============================] - 60s - loss: 0.3567 - jaccard: 0.0737 - val_loss: 0.3914 - val_jaccard: 0.0847\n",
      "Epoch 27/1000\n",
      "260/260 [==============================] - 63s - loss: 0.3478 - jaccard: 0.0763 - val_loss: 0.3643 - val_jaccard: 0.0718\n",
      "Epoch 28/1000\n",
      "260/260 [==============================] - 63s - loss: 0.3422 - jaccard: 0.0729 - val_loss: 0.3581 - val_jaccard: 0.0948\n",
      "Epoch 29/1000\n",
      "260/260 [==============================] - 60s - loss: 0.3373 - jaccard: 0.0721 - val_loss: 0.3729 - val_jaccard: 0.0984\n",
      "Epoch 30/1000\n",
      "260/260 [==============================] - 63s - loss: 0.3313 - jaccard: 0.0731 - val_loss: 0.3447 - val_jaccard: 0.0879\n",
      "Epoch 31/1000\n",
      "260/260 [==============================] - 63s - loss: 0.3323 - jaccard: 0.0702 - val_loss: 0.3290 - val_jaccard: 0.0792\n",
      "Epoch 32/1000\n",
      "260/260 [==============================] - 60s - loss: 0.3225 - jaccard: 0.0732 - val_loss: 0.3453 - val_jaccard: 0.0992\n",
      "Epoch 33/1000\n",
      "260/260 [==============================] - 59s - loss: 0.3156 - jaccard: 0.0741 - val_loss: 0.3375 - val_jaccard: 0.0970\n",
      "Epoch 34/1000\n",
      "260/260 [==============================] - 60s - loss: 0.3096 - jaccard: 0.0797 - val_loss: 0.3293 - val_jaccard: 0.0945\n",
      "Epoch 35/1000\n",
      "260/260 [==============================] - 63s - loss: 0.3123 - jaccard: 0.0748 - val_loss: 0.3232 - val_jaccard: 0.0970\n",
      "Epoch 36/1000\n",
      "260/260 [==============================] - 60s - loss: 0.3050 - jaccard: 0.0716 - val_loss: 0.3397 - val_jaccard: 0.1062\n",
      "Epoch 37/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2989 - jaccard: 0.0748 - val_loss: 0.3317 - val_jaccard: 0.1082\n",
      "Epoch 38/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2983 - jaccard: 0.0753 - val_loss: 0.2988 - val_jaccard: 0.0802\n",
      "Epoch 39/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2930 - jaccard: 0.0734 - val_loss: 0.3106 - val_jaccard: 0.1010\n",
      "Epoch 40/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2928 - jaccard: 0.0763 - val_loss: 0.3027 - val_jaccard: 0.0930\n",
      "Epoch 41/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2875 - jaccard: 0.0770 - val_loss: 0.2924 - val_jaccard: 0.1027\n",
      "Epoch 42/1000\n",
      "260/260 [==============================] - 60s - loss: 0.2813 - jaccard: 0.0776 - val_loss: 0.2980 - val_jaccard: 0.1068\n",
      "Epoch 43/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2776 - jaccard: 0.0813 - val_loss: 0.2880 - val_jaccard: 0.0936\n",
      "Epoch 44/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2752 - jaccard: 0.0786 - val_loss: 0.2777 - val_jaccard: 0.0904\n",
      "Epoch 45/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2769 - jaccard: 0.0749 - val_loss: 0.2791 - val_jaccard: 0.0806\n",
      "Epoch 46/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2709 - jaccard: 0.0754 - val_loss: 0.2827 - val_jaccard: 0.1040\n",
      "Epoch 47/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2692 - jaccard: 0.0749 - val_loss: 0.2816 - val_jaccard: 0.1061\n",
      "Epoch 48/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2649 - jaccard: 0.0780 - val_loss: 0.2732 - val_jaccard: 0.0993\n",
      "Epoch 49/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2625 - jaccard: 0.0789 - val_loss: 0.2615 - val_jaccard: 0.0922\n",
      "Epoch 50/1000\n",
      "260/260 [==============================] - 60s - loss: 0.2593 - jaccard: 0.0811 - val_loss: 0.2669 - val_jaccard: 0.0995\n",
      "Epoch 51/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2560 - jaccard: 0.0803 - val_loss: 0.2666 - val_jaccard: 0.1067\n",
      "Epoch 52/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2546 - jaccard: 0.0826 - val_loss: 0.2573 - val_jaccard: 0.0984\n",
      "Epoch 53/1000\n",
      "260/260 [==============================] - 60s - loss: 0.2528 - jaccard: 0.0829 - val_loss: 0.2588 - val_jaccard: 0.0957\n",
      "Epoch 54/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2513 - jaccard: 0.0807 - val_loss: 0.2632 - val_jaccard: 0.1021\n",
      "Epoch 55/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2470 - jaccard: 0.0862 - val_loss: 0.2601 - val_jaccard: 0.1043\n",
      "Epoch 56/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2465 - jaccard: 0.0857 - val_loss: 0.2573 - val_jaccard: 0.1014\n",
      "Epoch 57/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2432 - jaccard: 0.0899 - val_loss: 0.2581 - val_jaccard: 0.0987\n",
      "Epoch 58/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2473 - jaccard: 0.0787 - val_loss: 0.2532 - val_jaccard: 0.1031\n",
      "Epoch 59/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2433 - jaccard: 0.0829 - val_loss: 0.2473 - val_jaccard: 0.1069\n",
      "Epoch 60/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2399 - jaccard: 0.0864 - val_loss: 0.2463 - val_jaccard: 0.0992\n",
      "Epoch 61/1000\n",
      "260/260 [==============================] - 60s - loss: 0.2407 - jaccard: 0.0833 - val_loss: 0.2469 - val_jaccard: 0.0998\n",
      "Epoch 62/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2379 - jaccard: 0.0847 - val_loss: 0.2410 - val_jaccard: 0.1036\n",
      "Epoch 63/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2360 - jaccard: 0.0897 - val_loss: 0.2417 - val_jaccard: 0.1032\n",
      "Epoch 64/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2335 - jaccard: 0.0919 - val_loss: 0.2412 - val_jaccard: 0.1091\n",
      "Epoch 65/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2368 - jaccard: 0.0966 - val_loss: 0.2412 - val_jaccard: 0.1029\n",
      "Epoch 66/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2356 - jaccard: 0.0981 - val_loss: 0.2371 - val_jaccard: 0.1012\n",
      "Epoch 67/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2302 - jaccard: 0.0998 - val_loss: 0.2347 - val_jaccard: 0.1014\n",
      "Epoch 68/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2291 - jaccard: 0.0960 - val_loss: 0.2331 - val_jaccard: 0.0969\n",
      "Epoch 69/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2283 - jaccard: 0.0937 - val_loss: 0.2310 - val_jaccard: 0.0956\n",
      "Epoch 70/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2264 - jaccard: 0.0946 - val_loss: 0.2316 - val_jaccard: 0.0925\n",
      "Epoch 71/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2244 - jaccard: 0.0956 - val_loss: 0.2314 - val_jaccard: 0.0919\n",
      "Epoch 72/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2230 - jaccard: 0.0955 - val_loss: 0.2264 - val_jaccard: 0.0999\n",
      "Epoch 73/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2223 - jaccard: 0.0959 - val_loss: 0.2275 - val_jaccard: 0.0992\n",
      "Epoch 74/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2233 - jaccard: 0.0914 - val_loss: 0.2250 - val_jaccard: 0.1012\n",
      "Epoch 75/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2204 - jaccard: 0.0949 - val_loss: 0.2245 - val_jaccard: 0.0963\n",
      "Epoch 76/1000\n",
      "260/260 [==============================] - 60s - loss: 0.2203 - jaccard: 0.0945 - val_loss: 0.2253 - val_jaccard: 0.0952\n",
      "Epoch 77/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2193 - jaccard: 0.0952 - val_loss: 0.2239 - val_jaccard: 0.0977\n",
      "Epoch 78/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2190 - jaccard: 0.0963 - val_loss: 0.2238 - val_jaccard: 0.0997\n",
      "Epoch 79/1000\n",
      "260/260 [==============================] - 60s - loss: 0.2178 - jaccard: 0.0974 - val_loss: 0.2238 - val_jaccard: 0.0970\n",
      "Epoch 80/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2169 - jaccard: 0.0980 - val_loss: 0.2214 - val_jaccard: 0.0980\n",
      "Epoch 81/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2142 - jaccard: 0.1016 - val_loss: 0.2193 - val_jaccard: 0.1066\n",
      "Epoch 82/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2164 - jaccard: 0.0973 - val_loss: 0.2185 - val_jaccard: 0.1023\n",
      "Epoch 83/1000\n",
      "260/260 [==============================] - 60s - loss: 0.2133 - jaccard: 0.1019 - val_loss: 0.2198 - val_jaccard: 0.1010\n",
      "Epoch 84/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2131 - jaccard: 0.1051 - val_loss: 0.2191 - val_jaccard: 0.1009\n",
      "Epoch 85/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2130 - jaccard: 0.1019 - val_loss: 0.2186 - val_jaccard: 0.1009\n",
      "Epoch 86/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2142 - jaccard: 0.1011 - val_loss: 0.2201 - val_jaccard: 0.0947\n",
      "Epoch 87/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2142 - jaccard: 0.0996 - val_loss: 0.2171 - val_jaccard: 0.1037\n",
      "Epoch 88/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2131 - jaccard: 0.0986 - val_loss: 0.2188 - val_jaccard: 0.0950\n",
      "Epoch 89/1000\n",
      "260/260 [==============================] - 59s - loss: 0.2109 - jaccard: 0.1031 - val_loss: 0.2172 - val_jaccard: 0.1007\n",
      "Epoch 90/1000\n",
      "260/260 [==============================] - 63s - loss: 0.2110 - jaccard: 0.1012 - val_loss: 0.2143 - val_jaccard: 0.1064\n",
      "Epoch 91/1000\n",
      " 75/260 [=======>......................] - ETA: 40s - loss: 0.2020 - jaccard: 0.1043"
     ]
    }
   ],
   "source": [
    "def trainer(model,class_,nb_epochs=1000,fit=True,use_existing=False):\n",
    "    print('This is run # {} for class {}'.format(run,class_))\n",
    "    \n",
    "    if use_existing:\n",
    "        model.load_weights('./data/weights/model_weights_class_{}_run_{}.hdf5'.format(class_,run))\n",
    "        \n",
    "    if fit:\n",
    "        quitter = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=50, verbose=1, mode='auto')\n",
    "        lrreducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, verbose=1, mode='auto', epsilon=0.001, cooldown=2, min_lr=0)\n",
    "        model_checkpoint = ModelCheckpoint('./data/weights/model_weights_run_{}_class_{}.hdf5'.format(run,class_), monitor='val_loss', save_best_only=True)\n",
    "        csvlogger = CSVLogger('./data/logs/training_log_run_{}_class_{}'.format(run,class_), separator=',', append=True)\n",
    "        # tensorboard = TensorBoard(log_dir='./data/logs/'+'tensorboard_all-classes-run_{:04d}'.format(run), histogram_freq=0, write_graph=True, write_images=False)\n",
    "        # tensorboard --logdir=data/logs\n",
    "        \n",
    "        model.fit(x_train, y_train[:,class_:class_+1,:,:],\n",
    "                  batch_size=5,\n",
    "                  nb_epoch=nb_epochs,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  callbacks=[model_checkpoint,csvlogger,quitter,lrreducer],\n",
    "                  validation_data=(x_test,y_test[:,class_:class_+1,:,:]),\n",
    "                  initial_epoch=0)\n",
    "            \n",
    "    preds = model.predict(x_train, verbose=1)\n",
    "    np.save('./data/predictions/predictions_run_{}_class_{}.npy'.format(run,class_), preds)\n",
    "    \n",
    "    return model\n",
    "\n",
    "for class_ in range(3,10):\n",
    "    model.load_weights('./data/misc/initial_weights.hdf5')\n",
    "    model = trainer(model,class_,fit=True,use_existing=False)\n",
    "    model.save('./data/models/u-net-complete-model-run_{}_class_{}.hdf5'.format(run,class_))\n",
    "    push('Training on class {} is done'.format(class_),\n",
    "     'Train loss: %f, train jaccard: %f, val loss %f, val jaccard%f' %(model.history.history['loss'][-1],model.history.history['jaccard'][-1],model.history.history['val_loss'][-1],model.history.history['val_jaccard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
