{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and thoughts\n",
    "\n",
    "#### Current best ideas / runs that work / etc.\n",
    "- Batch size = ~1/6th of training data size seems to make all the difference\n",
    "- nfilters is only 16 for a single class\n",
    "- Perhaps the best method is to train 10 models, one for each class\n",
    "\n",
    "#### Ideas\n",
    "- Weight map: sum the total area of all classes in y, then calculate each class' proportion of the total and use `1-value` in place of 1 in the binary mask. This will cause low frequency classes to contribute more to the total loss, i.e. penalizing the model when it fails to predict low frequency classes.\n",
    "\n",
    "#### Data augmentation / image manipulation\n",
    "- [Histogram Equalization](http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html#sphx-glr-auto-examples-color-exposure-plot-equalize-py) (also see [here](https://www.kaggle.com/gabrielaltay/dstl-satellite-imagery-feature-detection/exploring-color-scaling-images/discussion), [here](http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/) and [here](https://medium.com/@vivek.yadav/improved-performance-of-deep-learning-neural-network-models-on-traffic-sign-classification-using-6355346da2dc#.x9nidcsh6))\n",
    "- Rotation (with reflection): see data-augmentation.ipynb\n",
    "- Image normalization: see image-preprocessing-new.ipynb\n",
    "\n",
    "#### Overfitting solutions\n",
    "- CNNs are supposed to be more robust to this because of the shared weight matrix of each filter\n",
    "- **Data augmentation!**\n",
    "- L2 regularization (added into the layers via `W_regularizer=l2(l=0.01)` parameter)\n",
    "- Dropout hurts the model in my experience..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U-Net architecture\n",
    "\n",
    "See [here](https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py#L19) for code and [here](https://arxiv.org/pdf/1505.04597.pdf) for the original literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? \n",
      "Nothing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Activation, Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D, BatchNormalization\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "# \"tf\" assumes (rows, cols, channels) while \"th\" assumes (channels, rows, cols)\n",
    "# Possibly change this around natively in the data so the backend doesn't have to switch them\n",
    "# Only necessary if I use TF!\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_preds(preds):\n",
    "    for i in range(preds.shape[0]):\n",
    "        preds[i,0,:,:] = (preds[i,0,:,:].min() - preds[i,0,:,:])/(preds[i,0,:,:].min()-preds[i,0,:,:].max())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_all(i,classType):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.imshow(preds[i,0,...],cmap='spectral')\n",
    "\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.imshow(np.rint(preds[i,0,...]),cmap='spectral')\n",
    "\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax3.imshow(x[i,17,...],cmap='Greys')\n",
    "\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    ax4.imshow(y_oneclass[i,classType,:,:],cmap='spectral')\n",
    "    \n",
    "    plt.show()\n",
    "#plot_all(2,2,classType,0.5)\n",
    "#push('PICTURES!','The plots are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the components of the pushbullet API\n",
    "from pushbullet import Pushbullet\n",
    "\n",
    "pb = Pushbullet('o.YFPNNPfGRekivaCGHa4qMSgjZt8zJ6FL')\n",
    "phone = pb.devices[0]\n",
    "\n",
    "# Run this cell after anything you want to be notified about!\n",
    "def push(title='Done!',text='Whatever it was, it\\'s done'):\n",
    "    phone.push_note(title,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the training data\n",
    "def import_data():\n",
    "    x = np.load('./data/x_augmented.npy','r')\n",
    "    y = np.load('./data/y_augmented.npy','r')\n",
    "    \n",
    "    '''with open('./data/x_resized_array.pickle','rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        x = x.astype(np.float32)\n",
    "        \n",
    "    with open('./data/y_resized_raster.pickle','rb') as f:\n",
    "        y = pickle.load(f)\n",
    "        y = y.astype(np.float32)'''\n",
    "    \n",
    "    y_oneclass = y[:,3:4,...]\n",
    "    \n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''\n",
    "    \n",
    "    return x, y, y_oneclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, y_oneclass = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 s, sys: 33.9 ms, total: 1.07 s\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "V2.0 U-Net with batchnorm\n",
    "'''\n",
    "def builder(img_rows = x.shape[2],img_cols = x.shape[3],\n",
    "            nfilters = 32,activation = 'relu',init = 'he_normal',\n",
    "            lr=1.0,decay=0.0,momentum=0.0, nesterov=False,reg=0.01):\n",
    "    \n",
    "    def jaccard(y_true, y_pred,smooth=1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "    \n",
    "    def Conv2DReluBatchNorm(n_filter, w_filter, h_filter, inputs, activation='relu',init='he_uniform',dropout=0.2):\n",
    "        return BatchNormalization(mode=0, axis=1)(Activation(activation=activation)(Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg))(inputs)))\n",
    "        \n",
    "    def up_conv(nfilters,filter_factor,inputs,init=init,activation=activation):\n",
    "        return Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,activation=activation,W_regularizer=l2(reg))(UpSampling2D(size=(2, 2))(inputs))\n",
    "    \n",
    "    inputs = Input((20, img_rows, img_cols))\n",
    "    padded = ZeroPadding2D(padding=(12,12))(inputs)\n",
    "    \n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, padded, activation=activation,init=init)\n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, conv1, activation=activation,init=init)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, pool1, activation=activation,init=init)\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv2, activation=activation,init=init)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, pool2, activation=activation,init=init)\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv3, activation=activation,init=init)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, pool3, activation=activation,init=init)\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv4, activation=activation,init=init)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, pool4, activation=activation,init=init)\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, conv5, activation=activation,init=init)\n",
    "        \n",
    "    up6 = merge([up_conv(nfilters,8,conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, up6, activation=activation,init=init)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv6, activation=activation,init=init)\n",
    "\n",
    "    up7 = merge([up_conv(nfilters,4,conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, up7, activation=activation,init=init)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv7, activation=activation,init=init)\n",
    "\n",
    "    up8 = merge([up_conv(nfilters,2,conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, up8, activation=activation,init=init)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv8, activation=activation,init=init)\n",
    "\n",
    "    up9 = merge([up_conv(nfilters,1,conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, up9, activation=activation,init=init)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, conv9, activation=activation,init=init)\n",
    "\n",
    "    conv10 = Conv2DReluBatchNorm(1, 1, 1, conv9, activation='relu',init=init)\n",
    "    cropped = Cropping2D(cropping=((12,12), (12,12)))(conv10)\n",
    "    output = Activation(activation='sigmoid')(cropped)\n",
    "    \n",
    "    model = Model(input=inputs, output=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=lr,decay=decay), loss='binary_crossentropy', metrics=[jaccard])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = builder(img_rows=x.shape[2],img_cols=x.shape[3],\n",
    "            nfilters=8,activation='relu',init='he_normal',\n",
    "            lr=0.001,decay=0,momentum=0,reg=0.00001)\n",
    "\n",
    "#push('The model is compiled','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run number: 66...\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/2000\n",
      "120/120 [==============================] - 4s - loss: 0.7991 - jaccard: 0.1048 - val_loss: 1.0121 - val_jaccard: 0.1349\n",
      "Epoch 2/2000\n",
      "120/120 [==============================] - 4s - loss: 0.6115 - jaccard: 0.1016 - val_loss: 1.4650 - val_jaccard: 0.1406\n",
      "Epoch 3/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5550 - jaccard: 0.0982 - val_loss: 2.1783 - val_jaccard: 0.1428\n",
      "Epoch 4/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5376 - jaccard: 0.0966 - val_loss: 2.3000 - val_jaccard: 0.1427\n",
      "Epoch 5/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5305 - jaccard: 0.0959 - val_loss: 1.6183 - val_jaccard: 0.1404\n",
      "Epoch 6/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5260 - jaccard: 0.0959 - val_loss: 1.0542 - val_jaccard: 0.1346\n",
      "Epoch 7/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5231 - jaccard: 0.0956 - val_loss: 0.8208 - val_jaccard: 0.1287\n",
      "Epoch 8/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5205 - jaccard: 0.0955 - val_loss: 0.6864 - val_jaccard: 0.1232\n",
      "Epoch 9/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5176 - jaccard: 0.0954 - val_loss: 0.6432 - val_jaccard: 0.1205\n",
      "Epoch 10/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5151 - jaccard: 0.0953 - val_loss: 0.6292 - val_jaccard: 0.1196\n",
      "Epoch 11/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5131 - jaccard: 0.0948 - val_loss: 0.6212 - val_jaccard: 0.1193\n",
      "Epoch 12/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5107 - jaccard: 0.0951 - val_loss: 0.6170 - val_jaccard: 0.1191\n",
      "Epoch 13/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5080 - jaccard: 0.0970 - val_loss: 0.6153 - val_jaccard: 0.1196\n",
      "Epoch 14/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5051 - jaccard: 0.0993 - val_loss: 0.6156 - val_jaccard: 0.1220\n",
      "Epoch 15/2000\n",
      "120/120 [==============================] - 4s - loss: 0.5025 - jaccard: 0.1024 - val_loss: 0.6084 - val_jaccard: 0.1232\n",
      "Epoch 16/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4999 - jaccard: 0.1034 - val_loss: 0.6155 - val_jaccard: 0.1261\n",
      "Epoch 17/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4963 - jaccard: 0.1062 - val_loss: 0.5955 - val_jaccard: 0.1233\n",
      "Epoch 18/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4924 - jaccard: 0.1082 - val_loss: 0.5949 - val_jaccard: 0.1252\n",
      "Epoch 19/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4901 - jaccard: 0.1104 - val_loss: 0.5879 - val_jaccard: 0.1253\n",
      "Epoch 20/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4868 - jaccard: 0.1136 - val_loss: 0.5791 - val_jaccard: 0.1244\n",
      "Epoch 21/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4856 - jaccard: 0.1132 - val_loss: 0.5662 - val_jaccard: 0.1206\n",
      "Epoch 22/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4832 - jaccard: 0.1137 - val_loss: 0.5585 - val_jaccard: 0.1183\n",
      "Epoch 23/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4830 - jaccard: 0.1099 - val_loss: 0.5527 - val_jaccard: 0.1152\n",
      "Epoch 24/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4780 - jaccard: 0.1130 - val_loss: 0.5511 - val_jaccard: 0.1191\n",
      "Epoch 25/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4755 - jaccard: 0.1175 - val_loss: 0.5495 - val_jaccard: 0.1204\n",
      "Epoch 26/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4737 - jaccard: 0.1170 - val_loss: 0.5426 - val_jaccard: 0.1184\n",
      "Epoch 27/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4707 - jaccard: 0.1178 - val_loss: 0.5399 - val_jaccard: 0.1183\n",
      "Epoch 28/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4683 - jaccard: 0.1199 - val_loss: 0.5384 - val_jaccard: 0.1198\n",
      "Epoch 29/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4660 - jaccard: 0.1206 - val_loss: 0.5345 - val_jaccard: 0.1198\n",
      "Epoch 30/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4649 - jaccard: 0.1200 - val_loss: 0.5366 - val_jaccard: 0.1235\n",
      "Epoch 31/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4625 - jaccard: 0.1211 - val_loss: 0.5316 - val_jaccard: 0.1216\n",
      "Epoch 32/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4608 - jaccard: 0.1211 - val_loss: 0.5348 - val_jaccard: 0.1246\n",
      "Epoch 33/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4577 - jaccard: 0.1243 - val_loss: 0.5257 - val_jaccard: 0.1225\n",
      "Epoch 34/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4567 - jaccard: 0.1221 - val_loss: 0.5286 - val_jaccard: 0.1238\n",
      "Epoch 35/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4547 - jaccard: 0.1253 - val_loss: 0.5490 - val_jaccard: 0.1325\n",
      "Epoch 36/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4534 - jaccard: 0.1242 - val_loss: 0.5358 - val_jaccard: 0.1295\n",
      "Epoch 37/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4498 - jaccard: 0.1257 - val_loss: 0.5156 - val_jaccard: 0.1220\n",
      "Epoch 38/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4471 - jaccard: 0.1296 - val_loss: 0.5305 - val_jaccard: 0.1294\n",
      "Epoch 39/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4448 - jaccard: 0.1302 - val_loss: 0.5207 - val_jaccard: 0.1267\n",
      "Epoch 40/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4426 - jaccard: 0.1306 - val_loss: 0.5286 - val_jaccard: 0.1294\n",
      "Epoch 41/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4403 - jaccard: 0.1331 - val_loss: 0.5255 - val_jaccard: 0.1299\n",
      "Epoch 42/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4379 - jaccard: 0.1350 - val_loss: 0.5241 - val_jaccard: 0.1299\n",
      "Epoch 43/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4359 - jaccard: 0.1360 - val_loss: 0.5374 - val_jaccard: 0.1317\n",
      "Epoch 44/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4328 - jaccard: 0.1381 - val_loss: 0.5363 - val_jaccard: 0.1324\n",
      "Epoch 45/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4300 - jaccard: 0.1394 - val_loss: 0.5430 - val_jaccard: 0.1348\n",
      "Epoch 46/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4284 - jaccard: 0.1428 - val_loss: 0.5436 - val_jaccard: 0.1336\n",
      "Epoch 47/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4269 - jaccard: 0.1405 - val_loss: 0.5269 - val_jaccard: 0.1317\n",
      "Epoch 48/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4241 - jaccard: 0.1452 - val_loss: 0.5324 - val_jaccard: 0.1343\n",
      "Epoch 49/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4222 - jaccard: 0.1437 - val_loss: 0.5378 - val_jaccard: 0.1353\n",
      "Epoch 50/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4194 - jaccard: 0.1483 - val_loss: 0.5423 - val_jaccard: 0.1349\n",
      "Epoch 51/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4171 - jaccard: 0.1474 - val_loss: 0.5436 - val_jaccard: 0.1363\n",
      "Epoch 52/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4155 - jaccard: 0.1513 - val_loss: 0.5515 - val_jaccard: 0.1367\n",
      "Epoch 53/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4139 - jaccard: 0.1497 - val_loss: 0.5550 - val_jaccard: 0.1388\n",
      "Epoch 54/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4114 - jaccard: 0.1517 - val_loss: 0.5245 - val_jaccard: 0.1342\n",
      "Epoch 55/2000\n",
      "120/120 [==============================] - 3s - loss: 0.4114 - jaccard: 0.1493 - val_loss: 0.5458 - val_jaccard: 0.1409\n",
      "Epoch 56/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4068 - jaccard: 0.1575 - val_loss: 0.5527 - val_jaccard: 0.1396\n",
      "Epoch 57/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4048 - jaccard: 0.1563 - val_loss: 0.5465 - val_jaccard: 0.1392\n",
      "Epoch 58/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4047 - jaccard: 0.1548 - val_loss: 0.5257 - val_jaccard: 0.1366\n",
      "Epoch 59/2000\n",
      "120/120 [==============================] - 4s - loss: 0.4008 - jaccard: 0.1599 - val_loss: 0.5531 - val_jaccard: 0.1408\n",
      "Epoch 60/2000\n",
      "120/120 [==============================] - 3s - loss: 0.4010 - jaccard: 0.1574 - val_loss: 0.5238 - val_jaccard: 0.1360\n",
      "Epoch 61/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3977 - jaccard: 0.1609 - val_loss: 0.5810 - val_jaccard: 0.1438\n",
      "Epoch 62/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3959 - jaccard: 0.1624 - val_loss: 0.5311 - val_jaccard: 0.1373\n",
      "Epoch 63/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3927 - jaccard: 0.1642 - val_loss: 0.5503 - val_jaccard: 0.1405\n",
      "Epoch 64/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3903 - jaccard: 0.1677 - val_loss: 0.5324 - val_jaccard: 0.1379\n",
      "Epoch 65/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3878 - jaccard: 0.1705 - val_loss: 0.5329 - val_jaccard: 0.1400\n",
      "Epoch 66/2000\n",
      "120/120 [==============================] - 3s - loss: 0.3879 - jaccard: 0.1665 - val_loss: 0.5192 - val_jaccard: 0.1367\n",
      "Epoch 67/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3871 - jaccard: 0.1689 - val_loss: 0.5181 - val_jaccard: 0.1368\n",
      "Epoch 68/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3840 - jaccard: 0.1727 - val_loss: 0.5562 - val_jaccard: 0.1405\n",
      "Epoch 69/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3806 - jaccard: 0.1735 - val_loss: 0.5292 - val_jaccard: 0.1380\n",
      "Epoch 70/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3801 - jaccard: 0.1727 - val_loss: 0.5387 - val_jaccard: 0.1405\n",
      "Epoch 71/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3766 - jaccard: 0.1785 - val_loss: 0.5482 - val_jaccard: 0.1377\n",
      "Epoch 72/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3743 - jaccard: 0.1764 - val_loss: 0.5250 - val_jaccard: 0.1385\n",
      "Epoch 73/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3721 - jaccard: 0.1837 - val_loss: 0.5237 - val_jaccard: 0.1384\n",
      "Epoch 74/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3696 - jaccard: 0.1840 - val_loss: 0.5275 - val_jaccard: 0.1369\n",
      "Epoch 75/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3694 - jaccard: 0.1814 - val_loss: 0.5183 - val_jaccard: 0.1390\n",
      "Epoch 76/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3655 - jaccard: 0.1886 - val_loss: 0.5277 - val_jaccard: 0.1393\n",
      "Epoch 77/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3634 - jaccard: 0.1883 - val_loss: 0.5102 - val_jaccard: 0.1357\n",
      "Epoch 78/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3622 - jaccard: 0.1897 - val_loss: 0.5163 - val_jaccard: 0.1383\n",
      "Epoch 79/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3588 - jaccard: 0.1909 - val_loss: 0.5052 - val_jaccard: 0.1376\n",
      "Epoch 80/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3586 - jaccard: 0.1954 - val_loss: 0.5066 - val_jaccard: 0.1377\n",
      "Epoch 81/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3560 - jaccard: 0.1927 - val_loss: 0.5079 - val_jaccard: 0.1372\n",
      "Epoch 82/2000\n",
      "120/120 [==============================] - 3s - loss: 0.3573 - jaccard: 0.1942 - val_loss: 0.5162 - val_jaccard: 0.1383\n",
      "Epoch 83/2000\n",
      "120/120 [==============================] - 3s - loss: 0.3610 - jaccard: 0.1817 - val_loss: 0.5196 - val_jaccard: 0.1402\n",
      "Epoch 84/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3545 - jaccard: 0.1996 - val_loss: 0.5301 - val_jaccard: 0.1408\n",
      "Epoch 85/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3529 - jaccard: 0.1942 - val_loss: 0.5185 - val_jaccard: 0.1413\n",
      "Epoch 86/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3527 - jaccard: 0.1942 - val_loss: 0.5320 - val_jaccard: 0.1437\n",
      "Epoch 87/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3477 - jaccard: 0.2029 - val_loss: 0.5070 - val_jaccard: 0.1366\n",
      "Epoch 88/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3473 - jaccard: 0.1969 - val_loss: 0.4985 - val_jaccard: 0.1409\n",
      "Epoch 89/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3427 - jaccard: 0.2070 - val_loss: 0.5095 - val_jaccard: 0.1414\n",
      "Epoch 90/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3425 - jaccard: 0.2052 - val_loss: 0.4882 - val_jaccard: 0.1369\n",
      "Epoch 91/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3389 - jaccard: 0.2071 - val_loss: 0.5128 - val_jaccard: 0.1398\n",
      "Epoch 92/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3351 - jaccard: 0.2158 - val_loss: 0.4919 - val_jaccard: 0.1375\n",
      "Epoch 93/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3333 - jaccard: 0.2152 - val_loss: 0.4849 - val_jaccard: 0.1371\n",
      "Epoch 94/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3330 - jaccard: 0.2162 - val_loss: 0.4874 - val_jaccard: 0.1364\n",
      "Epoch 95/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3288 - jaccard: 0.2175 - val_loss: 0.4817 - val_jaccard: 0.1359\n",
      "Epoch 96/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3273 - jaccard: 0.2214 - val_loss: 0.4928 - val_jaccard: 0.1369\n",
      "Epoch 97/2000\n",
      "120/120 [==============================] - 3s - loss: 0.3291 - jaccard: 0.2214 - val_loss: 0.4917 - val_jaccard: 0.1382\n",
      "Epoch 98/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3257 - jaccard: 0.2178 - val_loss: 0.4729 - val_jaccard: 0.1340\n",
      "Epoch 99/2000\n",
      "120/120 [==============================] - 3s - loss: 0.3266 - jaccard: 0.2216 - val_loss: 0.4999 - val_jaccard: 0.1400\n",
      "Epoch 100/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3229 - jaccard: 0.2272 - val_loss: 0.4775 - val_jaccard: 0.1310\n",
      "Epoch 101/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3217 - jaccard: 0.2214 - val_loss: 0.4900 - val_jaccard: 0.1410\n",
      "Epoch 102/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3185 - jaccard: 0.2289 - val_loss: 0.4885 - val_jaccard: 0.1354\n",
      "Epoch 103/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3182 - jaccard: 0.2316 - val_loss: 0.4916 - val_jaccard: 0.1365\n",
      "Epoch 104/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3150 - jaccard: 0.2315 - val_loss: 0.4775 - val_jaccard: 0.1345\n",
      "Epoch 105/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3135 - jaccard: 0.2310 - val_loss: 0.4849 - val_jaccard: 0.1368\n",
      "Epoch 106/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3113 - jaccard: 0.2396 - val_loss: 0.4821 - val_jaccard: 0.1359\n",
      "Epoch 107/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3081 - jaccard: 0.2387 - val_loss: 0.4863 - val_jaccard: 0.1370\n",
      "Epoch 108/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3078 - jaccard: 0.2383 - val_loss: 0.4872 - val_jaccard: 0.1383\n",
      "Epoch 109/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3067 - jaccard: 0.2405 - val_loss: 0.4846 - val_jaccard: 0.1372\n",
      "Epoch 110/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3066 - jaccard: 0.2441 - val_loss: 0.5014 - val_jaccard: 0.1390\n",
      "Epoch 111/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3059 - jaccard: 0.2424 - val_loss: 0.4752 - val_jaccard: 0.1348\n",
      "Epoch 112/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3027 - jaccard: 0.2395 - val_loss: 0.5020 - val_jaccard: 0.1423\n",
      "Epoch 113/2000\n",
      "120/120 [==============================] - 4s - loss: 0.3026 - jaccard: 0.2445 - val_loss: 0.4970 - val_jaccard: 0.1379\n",
      "Epoch 114/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2982 - jaccard: 0.2498 - val_loss: 0.4982 - val_jaccard: 0.1395\n",
      "Epoch 115/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2987 - jaccard: 0.2474 - val_loss: 0.5022 - val_jaccard: 0.1412\n",
      "Epoch 116/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2970 - jaccard: 0.2545 - val_loss: 0.4774 - val_jaccard: 0.1306\n",
      "Epoch 117/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2988 - jaccard: 0.2428 - val_loss: 0.4935 - val_jaccard: 0.1396\n",
      "Epoch 118/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2941 - jaccard: 0.2534 - val_loss: 0.5119 - val_jaccard: 0.1403\n",
      "Epoch 119/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2917 - jaccard: 0.2565 - val_loss: 0.5050 - val_jaccard: 0.1410\n",
      "Epoch 120/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2929 - jaccard: 0.2573 - val_loss: 0.4966 - val_jaccard: 0.1365\n",
      "Epoch 121/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2926 - jaccard: 0.2508 - val_loss: 0.4876 - val_jaccard: 0.1392\n",
      "Epoch 122/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2901 - jaccard: 0.2536 - val_loss: 0.5086 - val_jaccard: 0.1409\n",
      "Epoch 123/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2878 - jaccard: 0.2617 - val_loss: 0.4839 - val_jaccard: 0.1384\n",
      "Epoch 124/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2864 - jaccard: 0.2574 - val_loss: 0.4863 - val_jaccard: 0.1394\n",
      "Epoch 125/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2882 - jaccard: 0.2617 - val_loss: 0.5149 - val_jaccard: 0.1408\n",
      "Epoch 126/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2837 - jaccard: 0.2657 - val_loss: 0.4915 - val_jaccard: 0.1395\n",
      "Epoch 127/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2848 - jaccard: 0.2546 - val_loss: 0.5090 - val_jaccard: 0.1442\n",
      "Epoch 128/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2827 - jaccard: 0.2684 - val_loss: 0.4954 - val_jaccard: 0.1382\n",
      "Epoch 129/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2821 - jaccard: 0.2630 - val_loss: 0.5049 - val_jaccard: 0.1436\n",
      "Epoch 130/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2780 - jaccard: 0.2722 - val_loss: 0.4960 - val_jaccard: 0.1380\n",
      "Epoch 131/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2762 - jaccard: 0.2714 - val_loss: 0.4922 - val_jaccard: 0.1404\n",
      "Epoch 132/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2732 - jaccard: 0.2744 - val_loss: 0.4897 - val_jaccard: 0.1384\n",
      "Epoch 133/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2714 - jaccard: 0.2786 - val_loss: 0.4922 - val_jaccard: 0.1391\n",
      "Epoch 134/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2732 - jaccard: 0.2729 - val_loss: 0.4863 - val_jaccard: 0.1387\n",
      "Epoch 135/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2739 - jaccard: 0.2816 - val_loss: 0.4927 - val_jaccard: 0.1348\n",
      "Epoch 136/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2709 - jaccard: 0.2728 - val_loss: 0.4965 - val_jaccard: 0.1429\n",
      "Epoch 137/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2702 - jaccard: 0.2808 - val_loss: 0.5112 - val_jaccard: 0.1401\n",
      "Epoch 138/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2680 - jaccard: 0.2801 - val_loss: 0.4856 - val_jaccard: 0.1379\n",
      "Epoch 139/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2674 - jaccard: 0.2802 - val_loss: 0.4994 - val_jaccard: 0.1391\n",
      "Epoch 140/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2646 - jaccard: 0.2873 - val_loss: 0.4937 - val_jaccard: 0.1383\n",
      "Epoch 141/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2628 - jaccard: 0.2839 - val_loss: 0.4857 - val_jaccard: 0.1393\n",
      "Epoch 142/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2640 - jaccard: 0.2902 - val_loss: 0.4978 - val_jaccard: 0.1380\n",
      "Epoch 143/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2616 - jaccard: 0.2853 - val_loss: 0.5078 - val_jaccard: 0.1419\n",
      "Epoch 144/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2605 - jaccard: 0.2884 - val_loss: 0.5036 - val_jaccard: 0.1385\n",
      "Epoch 145/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2585 - jaccard: 0.2927 - val_loss: 0.5122 - val_jaccard: 0.1429\n",
      "Epoch 146/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2593 - jaccard: 0.2940 - val_loss: 0.5003 - val_jaccard: 0.1382\n",
      "Epoch 147/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2569 - jaccard: 0.2912 - val_loss: 0.4901 - val_jaccard: 0.1412\n",
      "Epoch 148/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2545 - jaccard: 0.2988 - val_loss: 0.4952 - val_jaccard: 0.1385\n",
      "Epoch 149/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2544 - jaccard: 0.2948 - val_loss: 0.5003 - val_jaccard: 0.1400\n",
      "Epoch 150/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2551 - jaccard: 0.3006 - val_loss: 0.4839 - val_jaccard: 0.1365\n",
      "Epoch 151/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2552 - jaccard: 0.2917 - val_loss: 0.5079 - val_jaccard: 0.1451\n",
      "Epoch 152/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2518 - jaccard: 0.3040 - val_loss: 0.4967 - val_jaccard: 0.1342\n",
      "Epoch 153/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2497 - jaccard: 0.2976 - val_loss: 0.4955 - val_jaccard: 0.1406\n",
      "Epoch 154/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2496 - jaccard: 0.3055 - val_loss: 0.4860 - val_jaccard: 0.1344\n",
      "Epoch 155/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2469 - jaccard: 0.3052 - val_loss: 0.4983 - val_jaccard: 0.1388\n",
      "Epoch 156/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2461 - jaccard: 0.3063 - val_loss: 0.4719 - val_jaccard: 0.1341\n",
      "Epoch 157/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2447 - jaccard: 0.3084 - val_loss: 0.4921 - val_jaccard: 0.1375\n",
      "Epoch 158/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2466 - jaccard: 0.3072 - val_loss: 0.5074 - val_jaccard: 0.1392\n",
      "Epoch 159/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2431 - jaccard: 0.3094 - val_loss: 0.4909 - val_jaccard: 0.1370\n",
      "Epoch 160/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2432 - jaccard: 0.3114 - val_loss: 0.4934 - val_jaccard: 0.1353\n",
      "Epoch 161/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2421 - jaccard: 0.3087 - val_loss: 0.4884 - val_jaccard: 0.1389\n",
      "Epoch 162/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2446 - jaccard: 0.3068 - val_loss: 0.5210 - val_jaccard: 0.1399\n",
      "Epoch 163/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2404 - jaccard: 0.3182 - val_loss: 0.4860 - val_jaccard: 0.1359\n",
      "Epoch 164/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2398 - jaccard: 0.3127 - val_loss: 0.4930 - val_jaccard: 0.1368\n",
      "Epoch 165/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2384 - jaccard: 0.3165 - val_loss: 0.5038 - val_jaccard: 0.1379\n",
      "Epoch 166/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2372 - jaccard: 0.3135 - val_loss: 0.4907 - val_jaccard: 0.1383\n",
      "Epoch 167/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2358 - jaccard: 0.3219 - val_loss: 0.4863 - val_jaccard: 0.1358\n",
      "Epoch 168/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2350 - jaccard: 0.3192 - val_loss: 0.4829 - val_jaccard: 0.1373\n",
      "Epoch 169/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2342 - jaccard: 0.3231 - val_loss: 0.4898 - val_jaccard: 0.1385\n",
      "Epoch 170/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2367 - jaccard: 0.3137 - val_loss: 0.4852 - val_jaccard: 0.1389\n",
      "Epoch 171/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2381 - jaccard: 0.3226 - val_loss: 0.5004 - val_jaccard: 0.1365\n",
      "Epoch 172/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2357 - jaccard: 0.3150 - val_loss: 0.4756 - val_jaccard: 0.1354\n",
      "Epoch 173/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2354 - jaccard: 0.3198 - val_loss: 0.4908 - val_jaccard: 0.1374\n",
      "Epoch 174/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2336 - jaccard: 0.3196 - val_loss: 0.4763 - val_jaccard: 0.1331\n",
      "Epoch 175/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2322 - jaccard: 0.3228 - val_loss: 0.4853 - val_jaccard: 0.1363\n",
      "Epoch 176/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2333 - jaccard: 0.3215 - val_loss: 0.4777 - val_jaccard: 0.1344\n",
      "Epoch 177/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2288 - jaccard: 0.3257 - val_loss: 0.4820 - val_jaccard: 0.1355\n",
      "Epoch 178/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2273 - jaccard: 0.3307 - val_loss: 0.4778 - val_jaccard: 0.1342\n",
      "Epoch 179/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2289 - jaccard: 0.3283 - val_loss: 0.4827 - val_jaccard: 0.1334\n",
      "Epoch 180/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2267 - jaccard: 0.3259 - val_loss: 0.4807 - val_jaccard: 0.1385\n",
      "Epoch 181/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2244 - jaccard: 0.3354 - val_loss: 0.4889 - val_jaccard: 0.1303\n",
      "Epoch 182/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2245 - jaccard: 0.3328 - val_loss: 0.4823 - val_jaccard: 0.1380\n",
      "Epoch 183/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2237 - jaccard: 0.3366 - val_loss: 0.4799 - val_jaccard: 0.1327\n",
      "Epoch 184/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2225 - jaccard: 0.3372 - val_loss: 0.4898 - val_jaccard: 0.1368\n",
      "Epoch 185/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2215 - jaccard: 0.3363 - val_loss: 0.4764 - val_jaccard: 0.1342\n",
      "Epoch 186/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2269 - jaccard: 0.3269 - val_loss: 0.4845 - val_jaccard: 0.1382\n",
      "Epoch 187/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2295 - jaccard: 0.3339 - val_loss: 0.5097 - val_jaccard: 0.1370\n",
      "Epoch 188/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2346 - jaccard: 0.3106 - val_loss: 0.4849 - val_jaccard: 0.1399\n",
      "Epoch 189/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2320 - jaccard: 0.3240 - val_loss: 0.5125 - val_jaccard: 0.1397\n",
      "Epoch 190/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2288 - jaccard: 0.3217 - val_loss: 0.4713 - val_jaccard: 0.1305\n",
      "Epoch 191/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2263 - jaccard: 0.3273 - val_loss: 0.4680 - val_jaccard: 0.1361\n",
      "Epoch 192/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2233 - jaccard: 0.3313 - val_loss: 0.4779 - val_jaccard: 0.1337\n",
      "Epoch 193/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2231 - jaccard: 0.3371 - val_loss: 0.4837 - val_jaccard: 0.1353\n",
      "Epoch 194/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2202 - jaccard: 0.3357 - val_loss: 0.4721 - val_jaccard: 0.1330\n",
      "Epoch 195/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2186 - jaccard: 0.3377 - val_loss: 0.4657 - val_jaccard: 0.1304\n",
      "Epoch 196/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2189 - jaccard: 0.3426 - val_loss: 0.4824 - val_jaccard: 0.1346\n",
      "Epoch 197/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2165 - jaccard: 0.3402 - val_loss: 0.4729 - val_jaccard: 0.1313\n",
      "Epoch 198/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2153 - jaccard: 0.3446 - val_loss: 0.4794 - val_jaccard: 0.1353\n",
      "Epoch 199/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2124 - jaccard: 0.3470 - val_loss: 0.4701 - val_jaccard: 0.1328\n",
      "Epoch 200/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2103 - jaccard: 0.3551 - val_loss: 0.4801 - val_jaccard: 0.1352\n",
      "Epoch 201/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2090 - jaccard: 0.3545 - val_loss: 0.4725 - val_jaccard: 0.1337\n",
      "Epoch 202/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2086 - jaccard: 0.3548 - val_loss: 0.4767 - val_jaccard: 0.1341\n",
      "Epoch 203/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2092 - jaccard: 0.3562 - val_loss: 0.4934 - val_jaccard: 0.1356\n",
      "Epoch 204/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2077 - jaccard: 0.3528 - val_loss: 0.4889 - val_jaccard: 0.1385\n",
      "Epoch 205/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2076 - jaccard: 0.3630 - val_loss: 0.4895 - val_jaccard: 0.1314\n",
      "Epoch 206/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2072 - jaccard: 0.3537 - val_loss: 0.4827 - val_jaccard: 0.1379\n",
      "Epoch 207/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2066 - jaccard: 0.3590 - val_loss: 0.4876 - val_jaccard: 0.1333\n",
      "Epoch 208/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2037 - jaccard: 0.3616 - val_loss: 0.4922 - val_jaccard: 0.1375\n",
      "Epoch 209/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2057 - jaccard: 0.3569 - val_loss: 0.5000 - val_jaccard: 0.1393\n",
      "Epoch 210/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2026 - jaccard: 0.3692 - val_loss: 0.4856 - val_jaccard: 0.1319\n",
      "Epoch 211/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2026 - jaccard: 0.3645 - val_loss: 0.4804 - val_jaccard: 0.1343\n",
      "Epoch 212/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2017 - jaccard: 0.3647 - val_loss: 0.4914 - val_jaccard: 0.1364\n",
      "Epoch 213/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2009 - jaccard: 0.3679 - val_loss: 0.4825 - val_jaccard: 0.1336\n",
      "Epoch 214/2000\n",
      "120/120 [==============================] - 4s - loss: 0.2000 - jaccard: 0.3656 - val_loss: 0.4887 - val_jaccard: 0.1376\n",
      "Epoch 215/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2004 - jaccard: 0.3695 - val_loss: 0.5009 - val_jaccard: 0.1386\n",
      "Epoch 216/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1981 - jaccard: 0.3712 - val_loss: 0.4826 - val_jaccard: 0.1339\n",
      "Epoch 217/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2031 - jaccard: 0.3610 - val_loss: 0.4963 - val_jaccard: 0.1399\n",
      "Epoch 218/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1992 - jaccard: 0.3732 - val_loss: 0.4874 - val_jaccard: 0.1330\n",
      "Epoch 219/2000\n",
      "120/120 [==============================] - 3s - loss: 0.2010 - jaccard: 0.3624 - val_loss: 0.4822 - val_jaccard: 0.1386\n",
      "Epoch 220/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1966 - jaccard: 0.3759 - val_loss: 0.4845 - val_jaccard: 0.1353\n",
      "Epoch 221/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1956 - jaccard: 0.3746 - val_loss: 0.4860 - val_jaccard: 0.1364\n",
      "Epoch 222/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1961 - jaccard: 0.3773 - val_loss: 0.4809 - val_jaccard: 0.1319\n",
      "Epoch 223/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1952 - jaccard: 0.3721 - val_loss: 0.4858 - val_jaccard: 0.1354\n",
      "Epoch 224/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1939 - jaccard: 0.3801 - val_loss: 0.4837 - val_jaccard: 0.1343\n",
      "Epoch 225/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1923 - jaccard: 0.3805 - val_loss: 0.4796 - val_jaccard: 0.1324\n",
      "Epoch 226/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1931 - jaccard: 0.3796 - val_loss: 0.4804 - val_jaccard: 0.1338\n",
      "Epoch 227/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1924 - jaccard: 0.3800 - val_loss: 0.4967 - val_jaccard: 0.1377\n",
      "Epoch 228/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1914 - jaccard: 0.3801 - val_loss: 0.4771 - val_jaccard: 0.1340\n",
      "Epoch 229/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1916 - jaccard: 0.3820 - val_loss: 0.4955 - val_jaccard: 0.1400\n",
      "Epoch 230/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1912 - jaccard: 0.3800 - val_loss: 0.4845 - val_jaccard: 0.1367\n",
      "Epoch 231/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1921 - jaccard: 0.3852 - val_loss: 0.4786 - val_jaccard: 0.1336\n",
      "Epoch 232/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1903 - jaccard: 0.3833 - val_loss: 0.4856 - val_jaccard: 0.1383\n",
      "Epoch 233/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1882 - jaccard: 0.3872 - val_loss: 0.4922 - val_jaccard: 0.1370\n",
      "Epoch 234/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1894 - jaccard: 0.3877 - val_loss: 0.4809 - val_jaccard: 0.1330\n",
      "Epoch 235/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1890 - jaccard: 0.3831 - val_loss: 0.4862 - val_jaccard: 0.1395\n",
      "Epoch 236/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1874 - jaccard: 0.3916 - val_loss: 0.4866 - val_jaccard: 0.1348\n",
      "Epoch 237/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1913 - jaccard: 0.3788 - val_loss: 0.5013 - val_jaccard: 0.1406\n",
      "Epoch 238/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1881 - jaccard: 0.3889 - val_loss: 0.4877 - val_jaccard: 0.1315\n",
      "Epoch 239/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1867 - jaccard: 0.3848 - val_loss: 0.4895 - val_jaccard: 0.1399\n",
      "Epoch 240/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1864 - jaccard: 0.3927 - val_loss: 0.4904 - val_jaccard: 0.1325\n",
      "Epoch 241/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1877 - jaccard: 0.3823 - val_loss: 0.4969 - val_jaccard: 0.1402\n",
      "Epoch 242/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1867 - jaccard: 0.3983 - val_loss: 0.4993 - val_jaccard: 0.1326\n",
      "Epoch 243/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1854 - jaccard: 0.3877 - val_loss: 0.4754 - val_jaccard: 0.1335\n",
      "Epoch 244/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1865 - jaccard: 0.3914 - val_loss: 0.4761 - val_jaccard: 0.1308\n",
      "Epoch 245/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1858 - jaccard: 0.3901 - val_loss: 0.4838 - val_jaccard: 0.1329\n",
      "Epoch 246/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1828 - jaccard: 0.3973 - val_loss: 0.4856 - val_jaccard: 0.1364\n",
      "Epoch 247/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1824 - jaccard: 0.3957 - val_loss: 0.4891 - val_jaccard: 0.1337\n",
      "Epoch 248/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1837 - jaccard: 0.3974 - val_loss: 0.4792 - val_jaccard: 0.1344\n",
      "Epoch 249/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1817 - jaccard: 0.3965 - val_loss: 0.4756 - val_jaccard: 0.1350\n",
      "Epoch 250/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1811 - jaccard: 0.3993 - val_loss: 0.4805 - val_jaccard: 0.1315\n",
      "Epoch 251/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1797 - jaccard: 0.4003 - val_loss: 0.4842 - val_jaccard: 0.1395\n",
      "Epoch 252/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1792 - jaccard: 0.4071 - val_loss: 0.4818 - val_jaccard: 0.1297\n",
      "Epoch 253/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1789 - jaccard: 0.4003 - val_loss: 0.4864 - val_jaccard: 0.1409\n",
      "Epoch 254/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1793 - jaccard: 0.4024 - val_loss: 0.4772 - val_jaccard: 0.1273\n",
      "Epoch 255/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1773 - jaccard: 0.4043 - val_loss: 0.4862 - val_jaccard: 0.1368\n",
      "Epoch 256/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1798 - jaccard: 0.3981 - val_loss: 0.4971 - val_jaccard: 0.1404\n",
      "Epoch 257/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1798 - jaccard: 0.4042 - val_loss: 0.4928 - val_jaccard: 0.1377\n",
      "Epoch 258/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1777 - jaccard: 0.4029 - val_loss: 0.4794 - val_jaccard: 0.1358\n",
      "Epoch 259/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1759 - jaccard: 0.4113 - val_loss: 0.4989 - val_jaccard: 0.1325\n",
      "Epoch 260/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1755 - jaccard: 0.4069 - val_loss: 0.4894 - val_jaccard: 0.1381\n",
      "Epoch 261/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1755 - jaccard: 0.4130 - val_loss: 0.4898 - val_jaccard: 0.1324\n",
      "Epoch 262/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1776 - jaccard: 0.4006 - val_loss: 0.4935 - val_jaccard: 0.1415\n",
      "Epoch 263/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1749 - jaccard: 0.4151 - val_loss: 0.4840 - val_jaccard: 0.1302\n",
      "Epoch 264/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1793 - jaccard: 0.4037 - val_loss: 0.4877 - val_jaccard: 0.1322\n",
      "Epoch 265/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1813 - jaccard: 0.3954 - val_loss: 0.4942 - val_jaccard: 0.1389\n",
      "Epoch 266/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1800 - jaccard: 0.4063 - val_loss: 0.4681 - val_jaccard: 0.1209\n",
      "Epoch 267/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1792 - jaccard: 0.3982 - val_loss: 0.4738 - val_jaccard: 0.1345\n",
      "Epoch 268/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1805 - jaccard: 0.4065 - val_loss: 0.4603 - val_jaccard: 0.1186\n",
      "Epoch 269/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1796 - jaccard: 0.3969 - val_loss: 0.4671 - val_jaccard: 0.1307\n",
      "Epoch 270/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1780 - jaccard: 0.4073 - val_loss: 0.4916 - val_jaccard: 0.1346\n",
      "Epoch 271/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1748 - jaccard: 0.4082 - val_loss: 0.4688 - val_jaccard: 0.1299\n",
      "Epoch 272/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1738 - jaccard: 0.4112 - val_loss: 0.4674 - val_jaccard: 0.1293\n",
      "Epoch 273/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1718 - jaccard: 0.4177 - val_loss: 0.4727 - val_jaccard: 0.1289\n",
      "Epoch 274/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1707 - jaccard: 0.4172 - val_loss: 0.4819 - val_jaccard: 0.1306\n",
      "Epoch 275/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1729 - jaccard: 0.4153 - val_loss: 0.4832 - val_jaccard: 0.1329\n",
      "Epoch 276/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1709 - jaccard: 0.4129 - val_loss: 0.4735 - val_jaccard: 0.1283\n",
      "Epoch 277/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1707 - jaccard: 0.4242 - val_loss: 0.4973 - val_jaccard: 0.1377\n",
      "Epoch 278/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1699 - jaccard: 0.4170 - val_loss: 0.4704 - val_jaccard: 0.1252\n",
      "Epoch 279/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1692 - jaccard: 0.4233 - val_loss: 0.4968 - val_jaccard: 0.1368\n",
      "Epoch 280/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1694 - jaccard: 0.4166 - val_loss: 0.4819 - val_jaccard: 0.1338\n",
      "Epoch 281/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1669 - jaccard: 0.4275 - val_loss: 0.4849 - val_jaccard: 0.1337\n",
      "Epoch 282/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1672 - jaccard: 0.4248 - val_loss: 0.4871 - val_jaccard: 0.1316\n",
      "Epoch 283/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1640 - jaccard: 0.4303 - val_loss: 0.4953 - val_jaccard: 0.1371\n",
      "Epoch 284/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1637 - jaccard: 0.4310 - val_loss: 0.4921 - val_jaccard: 0.1311\n",
      "Epoch 285/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1636 - jaccard: 0.4348 - val_loss: 0.4886 - val_jaccard: 0.1321\n",
      "Epoch 286/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1630 - jaccard: 0.4302 - val_loss: 0.4904 - val_jaccard: 0.1365\n",
      "Epoch 287/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1626 - jaccard: 0.4381 - val_loss: 0.4809 - val_jaccard: 0.1279\n",
      "Epoch 288/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1629 - jaccard: 0.4285 - val_loss: 0.5063 - val_jaccard: 0.1411\n",
      "Epoch 289/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1631 - jaccard: 0.4362 - val_loss: 0.5022 - val_jaccard: 0.1334\n",
      "Epoch 290/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1618 - jaccard: 0.4350 - val_loss: 0.4837 - val_jaccard: 0.1329\n",
      "Epoch 291/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1618 - jaccard: 0.4391 - val_loss: 0.4962 - val_jaccard: 0.1309\n",
      "Epoch 292/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1630 - jaccard: 0.4272 - val_loss: 0.4846 - val_jaccard: 0.1362\n",
      "Epoch 293/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1660 - jaccard: 0.4328 - val_loss: 0.5049 - val_jaccard: 0.1317\n",
      "Epoch 294/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1634 - jaccard: 0.4305 - val_loss: 0.4909 - val_jaccard: 0.1328\n",
      "Epoch 295/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1619 - jaccard: 0.4384 - val_loss: 0.4999 - val_jaccard: 0.1351\n",
      "Epoch 296/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1617 - jaccard: 0.4351 - val_loss: 0.4892 - val_jaccard: 0.1300\n",
      "Epoch 297/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1598 - jaccard: 0.4381 - val_loss: 0.4896 - val_jaccard: 0.1332\n",
      "Epoch 298/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1592 - jaccard: 0.4428 - val_loss: 0.4845 - val_jaccard: 0.1293\n",
      "Epoch 299/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1587 - jaccard: 0.4404 - val_loss: 0.4912 - val_jaccard: 0.1340\n",
      "Epoch 300/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1586 - jaccard: 0.4433 - val_loss: 0.4932 - val_jaccard: 0.1345\n",
      "Epoch 301/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1592 - jaccard: 0.4404 - val_loss: 0.4984 - val_jaccard: 0.1348\n",
      "Epoch 302/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1571 - jaccard: 0.4475 - val_loss: 0.4953 - val_jaccard: 0.1320\n",
      "Epoch 303/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1571 - jaccard: 0.4434 - val_loss: 0.4902 - val_jaccard: 0.1320\n",
      "Epoch 304/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1640 - jaccard: 0.4370 - val_loss: 0.5148 - val_jaccard: 0.1367\n",
      "Epoch 305/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1595 - jaccard: 0.4353 - val_loss: 0.4908 - val_jaccard: 0.1329\n",
      "Epoch 306/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1580 - jaccard: 0.4437 - val_loss: 0.5060 - val_jaccard: 0.1360\n",
      "Epoch 307/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1570 - jaccard: 0.4476 - val_loss: 0.4945 - val_jaccard: 0.1298\n",
      "Epoch 308/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1596 - jaccard: 0.4368 - val_loss: 0.4987 - val_jaccard: 0.1362\n",
      "Epoch 309/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1588 - jaccard: 0.4506 - val_loss: 0.5112 - val_jaccard: 0.1331\n",
      "Epoch 310/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1596 - jaccard: 0.4358 - val_loss: 0.4829 - val_jaccard: 0.1321\n",
      "Epoch 311/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1595 - jaccard: 0.4394 - val_loss: 0.5096 - val_jaccard: 0.1371\n",
      "Epoch 312/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1569 - jaccard: 0.4488 - val_loss: 0.5006 - val_jaccard: 0.1341\n",
      "Epoch 313/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1561 - jaccard: 0.4433 - val_loss: 0.4808 - val_jaccard: 0.1287\n",
      "Epoch 314/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1547 - jaccard: 0.4496 - val_loss: 0.5102 - val_jaccard: 0.1380\n",
      "Epoch 315/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1532 - jaccard: 0.4562 - val_loss: 0.4876 - val_jaccard: 0.1283\n",
      "Epoch 316/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1536 - jaccard: 0.4495 - val_loss: 0.4886 - val_jaccard: 0.1326\n",
      "Epoch 317/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1524 - jaccard: 0.4585 - val_loss: 0.4999 - val_jaccard: 0.1317\n",
      "Epoch 318/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1532 - jaccard: 0.4504 - val_loss: 0.4810 - val_jaccard: 0.1264\n",
      "Epoch 319/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1532 - jaccard: 0.4575 - val_loss: 0.5013 - val_jaccard: 0.1357\n",
      "Epoch 320/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1561 - jaccard: 0.4437 - val_loss: 0.4905 - val_jaccard: 0.1336\n",
      "Epoch 321/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1545 - jaccard: 0.4562 - val_loss: 0.5129 - val_jaccard: 0.1359\n",
      "Epoch 322/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1547 - jaccard: 0.4467 - val_loss: 0.5103 - val_jaccard: 0.1357\n",
      "Epoch 323/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1524 - jaccard: 0.4596 - val_loss: 0.5092 - val_jaccard: 0.1367\n",
      "Epoch 324/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1531 - jaccard: 0.4524 - val_loss: 0.5202 - val_jaccard: 0.1357\n",
      "Epoch 325/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1506 - jaccard: 0.4585 - val_loss: 0.4995 - val_jaccard: 0.1331\n",
      "Epoch 326/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1539 - jaccard: 0.4554 - val_loss: 0.5154 - val_jaccard: 0.1338\n",
      "Epoch 327/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1516 - jaccard: 0.4534 - val_loss: 0.5054 - val_jaccard: 0.1370\n",
      "Epoch 328/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1511 - jaccard: 0.4617 - val_loss: 0.5079 - val_jaccard: 0.1328\n",
      "Epoch 329/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1498 - jaccard: 0.4600 - val_loss: 0.4991 - val_jaccard: 0.1313\n",
      "Epoch 330/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1492 - jaccard: 0.4642 - val_loss: 0.5032 - val_jaccard: 0.1338\n",
      "Epoch 331/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1501 - jaccard: 0.4580 - val_loss: 0.5007 - val_jaccard: 0.1328\n",
      "Epoch 332/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1499 - jaccard: 0.4665 - val_loss: 0.5027 - val_jaccard: 0.1327\n",
      "Epoch 333/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1497 - jaccard: 0.4570 - val_loss: 0.4994 - val_jaccard: 0.1343\n",
      "Epoch 334/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1496 - jaccard: 0.4629 - val_loss: 0.5103 - val_jaccard: 0.1339\n",
      "Epoch 335/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1493 - jaccard: 0.4614 - val_loss: 0.5141 - val_jaccard: 0.1363\n",
      "Epoch 336/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1661 - jaccard: 0.4297 - val_loss: 0.4823 - val_jaccard: 0.1263\n",
      "Epoch 337/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1584 - jaccard: 0.4530 - val_loss: 0.5217 - val_jaccard: 0.1349\n",
      "Epoch 338/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1582 - jaccard: 0.4446 - val_loss: 0.4780 - val_jaccard: 0.1263\n",
      "Epoch 339/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1578 - jaccard: 0.4440 - val_loss: 0.4899 - val_jaccard: 0.1329\n",
      "Epoch 340/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1595 - jaccard: 0.4409 - val_loss: 0.4777 - val_jaccard: 0.1243\n",
      "Epoch 341/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1548 - jaccard: 0.4482 - val_loss: 0.4863 - val_jaccard: 0.1326\n",
      "Epoch 342/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1513 - jaccard: 0.4587 - val_loss: 0.4800 - val_jaccard: 0.1242\n",
      "Epoch 343/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1501 - jaccard: 0.4601 - val_loss: 0.4732 - val_jaccard: 0.1226\n",
      "Epoch 344/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1476 - jaccard: 0.4683 - val_loss: 0.4798 - val_jaccard: 0.1277\n",
      "Epoch 345/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1484 - jaccard: 0.4623 - val_loss: 0.4886 - val_jaccard: 0.1299\n",
      "Epoch 346/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1454 - jaccard: 0.4752 - val_loss: 0.4803 - val_jaccard: 0.1233\n",
      "Epoch 347/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1468 - jaccard: 0.4672 - val_loss: 0.4806 - val_jaccard: 0.1267\n",
      "Epoch 348/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1441 - jaccard: 0.4739 - val_loss: 0.4813 - val_jaccard: 0.1274\n",
      "Epoch 349/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1437 - jaccard: 0.4739 - val_loss: 0.4904 - val_jaccard: 0.1306\n",
      "Epoch 350/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1429 - jaccard: 0.4732 - val_loss: 0.4798 - val_jaccard: 0.1273\n",
      "Epoch 351/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1424 - jaccard: 0.4779 - val_loss: 0.4801 - val_jaccard: 0.1264\n",
      "Epoch 352/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1409 - jaccard: 0.4845 - val_loss: 0.4900 - val_jaccard: 0.1286\n",
      "Epoch 353/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1428 - jaccard: 0.4739 - val_loss: 0.4964 - val_jaccard: 0.1371\n",
      "Epoch 354/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1445 - jaccard: 0.4770 - val_loss: 0.4983 - val_jaccard: 0.1265\n",
      "Epoch 355/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1447 - jaccard: 0.4729 - val_loss: 0.5095 - val_jaccard: 0.1362\n",
      "Epoch 356/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1437 - jaccard: 0.4683 - val_loss: 0.4951 - val_jaccard: 0.1311\n",
      "Epoch 357/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1455 - jaccard: 0.4792 - val_loss: 0.5067 - val_jaccard: 0.1319\n",
      "Epoch 358/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1459 - jaccard: 0.4656 - val_loss: 0.4817 - val_jaccard: 0.1260\n",
      "Epoch 359/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1437 - jaccard: 0.4798 - val_loss: 0.5042 - val_jaccard: 0.1346\n",
      "Epoch 360/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1468 - jaccard: 0.4664 - val_loss: 0.5033 - val_jaccard: 0.1334\n",
      "Epoch 361/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1424 - jaccard: 0.4835 - val_loss: 0.5146 - val_jaccard: 0.1361\n",
      "Epoch 362/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1409 - jaccard: 0.4824 - val_loss: 0.5034 - val_jaccard: 0.1300\n",
      "Epoch 363/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1402 - jaccard: 0.4838 - val_loss: 0.5037 - val_jaccard: 0.1329\n",
      "Epoch 364/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1403 - jaccard: 0.4845 - val_loss: 0.4985 - val_jaccard: 0.1309\n",
      "Epoch 365/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1395 - jaccard: 0.4840 - val_loss: 0.5119 - val_jaccard: 0.1359\n",
      "Epoch 366/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1382 - jaccard: 0.4900 - val_loss: 0.4958 - val_jaccard: 0.1261\n",
      "Epoch 367/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1393 - jaccard: 0.4816 - val_loss: 0.5022 - val_jaccard: 0.1337\n",
      "Epoch 368/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1400 - jaccard: 0.4898 - val_loss: 0.5078 - val_jaccard: 0.1296\n",
      "Epoch 369/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1395 - jaccard: 0.4819 - val_loss: 0.4991 - val_jaccard: 0.1304\n",
      "Epoch 370/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1385 - jaccard: 0.4907 - val_loss: 0.5160 - val_jaccard: 0.1352\n",
      "Epoch 371/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1393 - jaccard: 0.4835 - val_loss: 0.4818 - val_jaccard: 0.1208\n",
      "Epoch 372/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1368 - jaccard: 0.4886 - val_loss: 0.4992 - val_jaccard: 0.1371\n",
      "Epoch 373/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1359 - jaccard: 0.4979 - val_loss: 0.5049 - val_jaccard: 0.1265\n",
      "Epoch 374/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1391 - jaccard: 0.4864 - val_loss: 0.5123 - val_jaccard: 0.1376\n",
      "Epoch 375/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1394 - jaccard: 0.4802 - val_loss: 0.4931 - val_jaccard: 0.1253\n",
      "Epoch 376/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1433 - jaccard: 0.4754 - val_loss: 0.4977 - val_jaccard: 0.1299\n",
      "Epoch 377/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1395 - jaccard: 0.4819 - val_loss: 0.4875 - val_jaccard: 0.1269\n",
      "Epoch 378/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1391 - jaccard: 0.4902 - val_loss: 0.5088 - val_jaccard: 0.1330\n",
      "Epoch 379/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1380 - jaccard: 0.4862 - val_loss: 0.4950 - val_jaccard: 0.1303\n",
      "Epoch 380/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1371 - jaccard: 0.4924 - val_loss: 0.5009 - val_jaccard: 0.1310\n",
      "Epoch 381/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1372 - jaccard: 0.4845 - val_loss: 0.4941 - val_jaccard: 0.1286\n",
      "Epoch 382/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1366 - jaccard: 0.4994 - val_loss: 0.5010 - val_jaccard: 0.1307\n",
      "Epoch 383/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1353 - jaccard: 0.4917 - val_loss: 0.4998 - val_jaccard: 0.1331\n",
      "Epoch 384/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1378 - jaccard: 0.4950 - val_loss: 0.5026 - val_jaccard: 0.1316\n",
      "Epoch 385/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1356 - jaccard: 0.4911 - val_loss: 0.4857 - val_jaccard: 0.1257\n",
      "Epoch 386/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1351 - jaccard: 0.5005 - val_loss: 0.5127 - val_jaccard: 0.1312\n",
      "Epoch 387/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1348 - jaccard: 0.4953 - val_loss: 0.5064 - val_jaccard: 0.1325\n",
      "Epoch 388/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1334 - jaccard: 0.5052 - val_loss: 0.4979 - val_jaccard: 0.1263\n",
      "Epoch 389/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1328 - jaccard: 0.5009 - val_loss: 0.5041 - val_jaccard: 0.1306\n",
      "Epoch 390/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1334 - jaccard: 0.5016 - val_loss: 0.5063 - val_jaccard: 0.1331\n",
      "Epoch 391/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1325 - jaccard: 0.5051 - val_loss: 0.5072 - val_jaccard: 0.1322\n",
      "Epoch 392/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1317 - jaccard: 0.5050 - val_loss: 0.5034 - val_jaccard: 0.1284\n",
      "Epoch 393/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1310 - jaccard: 0.5083 - val_loss: 0.5024 - val_jaccard: 0.1298\n",
      "Epoch 394/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1300 - jaccard: 0.5065 - val_loss: 0.4943 - val_jaccard: 0.1257\n",
      "Epoch 395/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1292 - jaccard: 0.5088 - val_loss: 0.5005 - val_jaccard: 0.1323\n",
      "Epoch 396/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1290 - jaccard: 0.5149 - val_loss: 0.5094 - val_jaccard: 0.1296\n",
      "Epoch 397/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1292 - jaccard: 0.5095 - val_loss: 0.5175 - val_jaccard: 0.1352\n",
      "Epoch 398/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1300 - jaccard: 0.5108 - val_loss: 0.5236 - val_jaccard: 0.1298\n",
      "Epoch 399/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1316 - jaccard: 0.4988 - val_loss: 0.5151 - val_jaccard: 0.1396\n",
      "Epoch 400/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1306 - jaccard: 0.5146 - val_loss: 0.5226 - val_jaccard: 0.1325\n",
      "Epoch 401/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1296 - jaccard: 0.5087 - val_loss: 0.5083 - val_jaccard: 0.1308\n",
      "Epoch 402/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1301 - jaccard: 0.5113 - val_loss: 0.5142 - val_jaccard: 0.1332\n",
      "Epoch 403/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1287 - jaccard: 0.5122 - val_loss: 0.5039 - val_jaccard: 0.1284\n",
      "Epoch 404/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1310 - jaccard: 0.5028 - val_loss: 0.5010 - val_jaccard: 0.1309\n",
      "Epoch 405/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1276 - jaccard: 0.5174 - val_loss: 0.5177 - val_jaccard: 0.1300\n",
      "Epoch 406/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1279 - jaccard: 0.5153 - val_loss: 0.5184 - val_jaccard: 0.1312\n",
      "Epoch 407/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1304 - jaccard: 0.5069 - val_loss: 0.5227 - val_jaccard: 0.1367\n",
      "Epoch 408/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1268 - jaccard: 0.5164 - val_loss: 0.5140 - val_jaccard: 0.1316\n",
      "Epoch 409/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1267 - jaccard: 0.5205 - val_loss: 0.5053 - val_jaccard: 0.1289\n",
      "Epoch 410/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1266 - jaccard: 0.5180 - val_loss: 0.5198 - val_jaccard: 0.1337\n",
      "Epoch 411/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1254 - jaccard: 0.5215 - val_loss: 0.5135 - val_jaccard: 0.1299\n",
      "Epoch 412/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1247 - jaccard: 0.5227 - val_loss: 0.5219 - val_jaccard: 0.1345\n",
      "Epoch 413/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1241 - jaccard: 0.5267 - val_loss: 0.5085 - val_jaccard: 0.1280\n",
      "Epoch 414/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1243 - jaccard: 0.5244 - val_loss: 0.5033 - val_jaccard: 0.1302\n",
      "Epoch 415/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1233 - jaccard: 0.5286 - val_loss: 0.5051 - val_jaccard: 0.1281\n",
      "Epoch 416/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1216 - jaccard: 0.5300 - val_loss: 0.5153 - val_jaccard: 0.1345\n",
      "Epoch 417/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1225 - jaccard: 0.5289 - val_loss: 0.5135 - val_jaccard: 0.1289\n",
      "Epoch 418/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1214 - jaccard: 0.5317 - val_loss: 0.5205 - val_jaccard: 0.1346\n",
      "Epoch 419/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1231 - jaccard: 0.5295 - val_loss: 0.5007 - val_jaccard: 0.1227\n",
      "Epoch 420/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1242 - jaccard: 0.5269 - val_loss: 0.5209 - val_jaccard: 0.1338\n",
      "Epoch 421/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1237 - jaccard: 0.5271 - val_loss: 0.5150 - val_jaccard: 0.1280\n",
      "Epoch 422/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1237 - jaccard: 0.5231 - val_loss: 0.5113 - val_jaccard: 0.1323\n",
      "Epoch 423/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1233 - jaccard: 0.5322 - val_loss: 0.5164 - val_jaccard: 0.1280\n",
      "Epoch 424/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1235 - jaccard: 0.5235 - val_loss: 0.5086 - val_jaccard: 0.1307\n",
      "Epoch 425/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1233 - jaccard: 0.5321 - val_loss: 0.5186 - val_jaccard: 0.1273\n",
      "Epoch 426/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1227 - jaccard: 0.5276 - val_loss: 0.5110 - val_jaccard: 0.1318\n",
      "Epoch 427/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1230 - jaccard: 0.5323 - val_loss: 0.5209 - val_jaccard: 0.1301\n",
      "Epoch 428/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1225 - jaccard: 0.5293 - val_loss: 0.5182 - val_jaccard: 0.1334\n",
      "Epoch 429/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1219 - jaccard: 0.5342 - val_loss: 0.5170 - val_jaccard: 0.1285\n",
      "Epoch 430/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1207 - jaccard: 0.5331 - val_loss: 0.5156 - val_jaccard: 0.1327\n",
      "Epoch 431/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1199 - jaccard: 0.5403 - val_loss: 0.5272 - val_jaccard: 0.1288\n",
      "Epoch 432/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1215 - jaccard: 0.5298 - val_loss: 0.5247 - val_jaccard: 0.1365\n",
      "Epoch 433/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1209 - jaccard: 0.5419 - val_loss: 0.5109 - val_jaccard: 0.1214\n",
      "Epoch 434/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1250 - jaccard: 0.5233 - val_loss: 0.5148 - val_jaccard: 0.1336\n",
      "Epoch 435/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1229 - jaccard: 0.5290 - val_loss: 0.5002 - val_jaccard: 0.1184\n",
      "Epoch 436/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1219 - jaccard: 0.5335 - val_loss: 0.5330 - val_jaccard: 0.1349\n",
      "Epoch 437/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1242 - jaccard: 0.5298 - val_loss: 0.5084 - val_jaccard: 0.1235\n",
      "Epoch 438/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1240 - jaccard: 0.5257 - val_loss: 0.5172 - val_jaccard: 0.1344\n",
      "Epoch 439/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1228 - jaccard: 0.5335 - val_loss: 0.5231 - val_jaccard: 0.1274\n",
      "Epoch 440/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1277 - jaccard: 0.5101 - val_loss: 0.5238 - val_jaccard: 0.1400\n",
      "Epoch 441/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1277 - jaccard: 0.5310 - val_loss: 0.5156 - val_jaccard: 0.1230\n",
      "Epoch 442/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1250 - jaccard: 0.5210 - val_loss: 0.5182 - val_jaccard: 0.1311\n",
      "Epoch 443/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1238 - jaccard: 0.5329 - val_loss: 0.5276 - val_jaccard: 0.1318\n",
      "Epoch 444/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1225 - jaccard: 0.5298 - val_loss: 0.5109 - val_jaccard: 0.1255\n",
      "Epoch 445/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1208 - jaccard: 0.5351 - val_loss: 0.5121 - val_jaccard: 0.1286\n",
      "Epoch 446/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1269 - jaccard: 0.5270 - val_loss: 0.5223 - val_jaccard: 0.1283\n",
      "Epoch 447/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1228 - jaccard: 0.5287 - val_loss: 0.5158 - val_jaccard: 0.1323\n",
      "Epoch 448/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1247 - jaccard: 0.5306 - val_loss: 0.5185 - val_jaccard: 0.1226\n",
      "Epoch 449/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1218 - jaccard: 0.5303 - val_loss: 0.5243 - val_jaccard: 0.1347\n",
      "Epoch 450/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1250 - jaccard: 0.5299 - val_loss: 0.4964 - val_jaccard: 0.1075\n",
      "Epoch 451/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1245 - jaccard: 0.5251 - val_loss: 0.5255 - val_jaccard: 0.1358\n",
      "Epoch 452/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1252 - jaccard: 0.5267 - val_loss: 0.5104 - val_jaccard: 0.1222\n",
      "Epoch 453/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1275 - jaccard: 0.5261 - val_loss: 0.5239 - val_jaccard: 0.1322\n",
      "Epoch 454/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1257 - jaccard: 0.5217 - val_loss: 0.5217 - val_jaccard: 0.1329\n",
      "Epoch 455/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1238 - jaccard: 0.5303 - val_loss: 0.5144 - val_jaccard: 0.1287\n",
      "Epoch 456/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1278 - jaccard: 0.5195 - val_loss: 0.5023 - val_jaccard: 0.1193\n",
      "Epoch 457/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1262 - jaccard: 0.5195 - val_loss: 0.4937 - val_jaccard: 0.1163\n",
      "Epoch 458/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1246 - jaccard: 0.5303 - val_loss: 0.5076 - val_jaccard: 0.1231\n",
      "Epoch 459/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1226 - jaccard: 0.5292 - val_loss: 0.4941 - val_jaccard: 0.1133\n",
      "Epoch 460/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1232 - jaccard: 0.5316 - val_loss: 0.4983 - val_jaccard: 0.1228\n",
      "Epoch 461/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1207 - jaccard: 0.5379 - val_loss: 0.5011 - val_jaccard: 0.1151\n",
      "Epoch 462/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1189 - jaccard: 0.5412 - val_loss: 0.5079 - val_jaccard: 0.1260\n",
      "Epoch 463/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1195 - jaccard: 0.5404 - val_loss: 0.5096 - val_jaccard: 0.1224\n",
      "Epoch 464/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1169 - jaccard: 0.5476 - val_loss: 0.5042 - val_jaccard: 0.1200\n",
      "Epoch 465/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1189 - jaccard: 0.5453 - val_loss: 0.5115 - val_jaccard: 0.1240\n",
      "Epoch 466/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1167 - jaccard: 0.5469 - val_loss: 0.5072 - val_jaccard: 0.1213\n",
      "Epoch 467/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1151 - jaccard: 0.5535 - val_loss: 0.5091 - val_jaccard: 0.1234\n",
      "Epoch 468/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1186 - jaccard: 0.5446 - val_loss: 0.5090 - val_jaccard: 0.1244\n",
      "Epoch 469/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1147 - jaccard: 0.5555 - val_loss: 0.5133 - val_jaccard: 0.1281\n",
      "Epoch 470/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1171 - jaccard: 0.5431 - val_loss: 0.5192 - val_jaccard: 0.1281\n",
      "Epoch 471/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1185 - jaccard: 0.5421 - val_loss: 0.5131 - val_jaccard: 0.1287\n",
      "Epoch 472/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1158 - jaccard: 0.5558 - val_loss: 0.5234 - val_jaccard: 0.1295\n",
      "Epoch 473/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1149 - jaccard: 0.5533 - val_loss: 0.5130 - val_jaccard: 0.1244\n",
      "Epoch 474/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1151 - jaccard: 0.5546 - val_loss: 0.5171 - val_jaccard: 0.1280\n",
      "Epoch 475/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1134 - jaccard: 0.5552 - val_loss: 0.5123 - val_jaccard: 0.1241\n",
      "Epoch 476/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1153 - jaccard: 0.5533 - val_loss: 0.5273 - val_jaccard: 0.1328\n",
      "Epoch 477/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1150 - jaccard: 0.5521 - val_loss: 0.5148 - val_jaccard: 0.1253\n",
      "Epoch 478/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1143 - jaccard: 0.5590 - val_loss: 0.5254 - val_jaccard: 0.1277\n",
      "Epoch 479/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1133 - jaccard: 0.5570 - val_loss: 0.5217 - val_jaccard: 0.1287\n",
      "Epoch 480/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1131 - jaccard: 0.5583 - val_loss: 0.5177 - val_jaccard: 0.1245\n",
      "Epoch 481/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1237 - jaccard: 0.5365 - val_loss: 0.5535 - val_jaccard: 0.1400\n",
      "Epoch 482/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1161 - jaccard: 0.5548 - val_loss: 0.5341 - val_jaccard: 0.1298\n",
      "Epoch 483/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1168 - jaccard: 0.5472 - val_loss: 0.5451 - val_jaccard: 0.1377\n",
      "Epoch 484/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1157 - jaccard: 0.5538 - val_loss: 0.5273 - val_jaccard: 0.1269\n",
      "Epoch 485/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1134 - jaccard: 0.5572 - val_loss: 0.5294 - val_jaccard: 0.1330\n",
      "Epoch 486/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1137 - jaccard: 0.5610 - val_loss: 0.5219 - val_jaccard: 0.1247\n",
      "Epoch 487/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1166 - jaccard: 0.5501 - val_loss: 0.5223 - val_jaccard: 0.1276\n",
      "Epoch 488/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1143 - jaccard: 0.5580 - val_loss: 0.5269 - val_jaccard: 0.1287\n",
      "Epoch 489/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1139 - jaccard: 0.5563 - val_loss: 0.5365 - val_jaccard: 0.1318\n",
      "Epoch 490/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1145 - jaccard: 0.5569 - val_loss: 0.5201 - val_jaccard: 0.1263\n",
      "Epoch 491/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1110 - jaccard: 0.5638 - val_loss: 0.5290 - val_jaccard: 0.1305\n",
      "Epoch 492/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1157 - jaccard: 0.5521 - val_loss: 0.5400 - val_jaccard: 0.1349\n",
      "Epoch 493/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1132 - jaccard: 0.5660 - val_loss: 0.5323 - val_jaccard: 0.1275\n",
      "Epoch 494/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1141 - jaccard: 0.5524 - val_loss: 0.5348 - val_jaccard: 0.1354\n",
      "Epoch 495/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1151 - jaccard: 0.5571 - val_loss: 0.5297 - val_jaccard: 0.1256\n",
      "Epoch 496/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1143 - jaccard: 0.5538 - val_loss: 0.5271 - val_jaccard: 0.1337\n",
      "Epoch 497/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1183 - jaccard: 0.5547 - val_loss: 0.5380 - val_jaccard: 0.1212\n",
      "Epoch 498/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1146 - jaccard: 0.5522 - val_loss: 0.5255 - val_jaccard: 0.1338\n",
      "Epoch 499/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1131 - jaccard: 0.5630 - val_loss: 0.5240 - val_jaccard: 0.1201\n",
      "Epoch 500/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1162 - jaccard: 0.5520 - val_loss: 0.5221 - val_jaccard: 0.1293\n",
      "Epoch 501/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1143 - jaccard: 0.5601 - val_loss: 0.5120 - val_jaccard: 0.1207\n",
      "Epoch 502/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1129 - jaccard: 0.5581 - val_loss: 0.5220 - val_jaccard: 0.1259\n",
      "Epoch 503/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1118 - jaccard: 0.5681 - val_loss: 0.5248 - val_jaccard: 0.1271\n",
      "Epoch 504/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1105 - jaccard: 0.5645 - val_loss: 0.5161 - val_jaccard: 0.1209\n",
      "Epoch 505/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1139 - jaccard: 0.5609 - val_loss: 0.5377 - val_jaccard: 0.1302\n",
      "Epoch 506/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1159 - jaccard: 0.5518 - val_loss: 0.5371 - val_jaccard: 0.1387\n",
      "Epoch 507/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1126 - jaccard: 0.5684 - val_loss: 0.5422 - val_jaccard: 0.1279\n",
      "Epoch 508/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1108 - jaccard: 0.5644 - val_loss: 0.5438 - val_jaccard: 0.1330\n",
      "Epoch 509/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1103 - jaccard: 0.5716 - val_loss: 0.5324 - val_jaccard: 0.1254\n",
      "Epoch 510/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1103 - jaccard: 0.5625 - val_loss: 0.5399 - val_jaccard: 0.1297\n",
      "Epoch 511/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1096 - jaccard: 0.5767 - val_loss: 0.5372 - val_jaccard: 0.1283\n",
      "Epoch 512/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1092 - jaccard: 0.5698 - val_loss: 0.5362 - val_jaccard: 0.1266\n",
      "Epoch 513/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1110 - jaccard: 0.5687 - val_loss: 0.5401 - val_jaccard: 0.1331\n",
      "Epoch 514/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1116 - jaccard: 0.5655 - val_loss: 0.5383 - val_jaccard: 0.1286\n",
      "Epoch 515/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1077 - jaccard: 0.5792 - val_loss: 0.5560 - val_jaccard: 0.1355\n",
      "Epoch 516/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1070 - jaccard: 0.5777 - val_loss: 0.5348 - val_jaccard: 0.1271\n",
      "Epoch 517/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1083 - jaccard: 0.5738 - val_loss: 0.5429 - val_jaccard: 0.1374\n",
      "Epoch 518/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1070 - jaccard: 0.5804 - val_loss: 0.5382 - val_jaccard: 0.1261\n",
      "Epoch 519/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1057 - jaccard: 0.5816 - val_loss: 0.5432 - val_jaccard: 0.1331\n",
      "Epoch 520/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1108 - jaccard: 0.5663 - val_loss: 0.5499 - val_jaccard: 0.1379\n",
      "Epoch 521/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1115 - jaccard: 0.5755 - val_loss: 0.5410 - val_jaccard: 0.1246\n",
      "Epoch 522/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1136 - jaccard: 0.5566 - val_loss: 0.5626 - val_jaccard: 0.1429\n",
      "Epoch 523/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1107 - jaccard: 0.5722 - val_loss: 0.5545 - val_jaccard: 0.1282\n",
      "Epoch 524/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1096 - jaccard: 0.5706 - val_loss: 0.5342 - val_jaccard: 0.1294\n",
      "Epoch 525/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1108 - jaccard: 0.5692 - val_loss: 0.5380 - val_jaccard: 0.1292\n",
      "Epoch 526/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1089 - jaccard: 0.5753 - val_loss: 0.5340 - val_jaccard: 0.1256\n",
      "Epoch 527/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1066 - jaccard: 0.5813 - val_loss: 0.5338 - val_jaccard: 0.1264\n",
      "Epoch 528/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1055 - jaccard: 0.5833 - val_loss: 0.5229 - val_jaccard: 0.1207\n",
      "Epoch 529/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1054 - jaccard: 0.5847 - val_loss: 0.5364 - val_jaccard: 0.1282\n",
      "Epoch 530/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1039 - jaccard: 0.5863 - val_loss: 0.5282 - val_jaccard: 0.1246\n",
      "Epoch 531/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1049 - jaccard: 0.5879 - val_loss: 0.5283 - val_jaccard: 0.1255\n",
      "Epoch 532/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1044 - jaccard: 0.5887 - val_loss: 0.5298 - val_jaccard: 0.1254\n",
      "Epoch 533/2000\n",
      "120/120 [==============================] - 4s - loss: 0.1032 - jaccard: 0.5912 - val_loss: 0.5370 - val_jaccard: 0.1268\n",
      "Epoch 534/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1048 - jaccard: 0.5837 - val_loss: 0.5322 - val_jaccard: 0.1305\n",
      "Epoch 535/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1043 - jaccard: 0.5900 - val_loss: 0.5360 - val_jaccard: 0.1218\n",
      "Epoch 536/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1088 - jaccard: 0.5768 - val_loss: 0.5463 - val_jaccard: 0.1362\n",
      "Epoch 537/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1066 - jaccard: 0.5835 - val_loss: 0.5348 - val_jaccard: 0.1260\n",
      "Epoch 538/2000\n",
      "120/120 [==============================] - 3s - loss: 0.1059 - jaccard: 0.5792 - val_loss: 0.5396 - val_jaccard: 0.1323\n",
      "Epoch 539/2000\n",
      " 30/120 [======>.......................] - ETA: 0s - loss: 0.1108 - jaccard: 0.5998"
     ]
    }
   ],
   "source": [
    "run += 1\n",
    "def train_and_predict(model,fit=True,use_existing=False):\n",
    "    print('This is run number: '+ str(run) + '...')\n",
    "    \n",
    "    if use_existing:\n",
    "        model.load_weights('./data/unet.hdf5')\n",
    "        \n",
    "    if fit:\n",
    "        \n",
    "        print('Fitting model...')\n",
    "        print('-'*30)\n",
    "        \n",
    "        quitter = EarlyStopping(monitor='loss', min_delta=0.001, patience=100, verbose=1, mode='auto')\n",
    "        lrreducer = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, verbose=1, mode='auto', epsilon=0.001, cooldown=2, min_lr=0)\n",
    "        model_checkpoint = ModelCheckpoint('./data/model_weights_class_3_run_'+str(run)+'.hdf5', monitor='loss', save_best_only=True)\n",
    "        csvlogger = CSVLogger('./data/training_log_run_'+str(run), separator=',', append=False)\n",
    "\n",
    "        #tensorboard = TensorBoard(log_dir='./logs/'+'run_'+str(run), histogram_freq=0, write_graph=True, write_images=False)\n",
    "        '''\n",
    "        screen -S tensorboard\n",
    "        tensorboard --logdir=logs\n",
    "        <ctrl + a,d to exit>\n",
    "        screen -r tensorboard\n",
    "        '''\n",
    "        \n",
    "        model.fit(x, y_oneclass,\n",
    "                  batch_size=30,\n",
    "                  nb_epoch=2000,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  callbacks=[model_checkpoint,csvlogger],\n",
    "                  validation_split=0.2,\n",
    "                  initial_epoch=0)\n",
    "    \n",
    "        print('Predicting masks on test data...')\n",
    "        print('-'*30)\n",
    "        \n",
    "    imgs_mask_test = model.predict(x, verbose=1)\n",
    "    np.save('jaccard_preds_all_data.npy', imgs_mask_test)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = train_and_predict(model,fit=True,use_existing=False)\n",
    "push('Training is done',\n",
    "     'Train loss: %f, train jaccard: %f, val loss %f, val jaccard%f' %(model.history.history['loss'][-1],model.history.history['jaccard'][-1],model.history.history['val_loss'][-1],model.history.history['val_jaccard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.load('jaccard_preds_all_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(x[120:150,...],y_oneclass[120:150,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm = False\n",
    "if norm:\n",
    "    preds = norm_preds(preds)\n",
    "i = np.random.choice(range(120,150),1)[0]\n",
    "print(i)\n",
    "plot_all(i,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_everything():\n",
    "    fig = plt.figure(figsize=(12,36))\n",
    "    #num = np.random.randint(10,25,10)\n",
    "    num = np.arange(0,10)\n",
    "    for i in range(1,10):\n",
    "        ax1 = fig.add_subplot(10,8,8*i-7)\n",
    "        ax1.imshow(preds[num[i],0,...],cmap='spectral')\n",
    "        \n",
    "        ax2 = fig.add_subplot(10,8,8*i-6)\n",
    "        ax2.imshow(y_oneclass[num[i],0,...],cmap='spectral')\n",
    "        \n",
    "        ax3 = fig.add_subplot(10,8,8*i-5)\n",
    "        ax3.imshow(preds[num[i],1,...],cmap='spectral')\n",
    "        \n",
    "        ax4 = fig.add_subplot(10,8,8*i-4)\n",
    "        ax4.imshow(y_oneclass[num[i],1,...],cmap='spectral')\n",
    "        \n",
    "        ax5 = fig.add_subplot(10,8,8*i-3)\n",
    "        ax5.imshow(preds[num[i],2,...],cmap='spectral')\n",
    "        \n",
    "        ax6 = fig.add_subplot(10,8,8*i-2)\n",
    "        ax6.imshow(y_oneclass[num[i],2,...],cmap='spectral')\n",
    "        \n",
    "        ax7 = fig.add_subplot(10,8,8*i-1)\n",
    "        ax7.imshow(preds[num[i],3,...],cmap='spectral')\n",
    "        \n",
    "        ax8 = fig.add_subplot(10,8,8*i)\n",
    "        ax8.imshow(y_oneclass[num[i],3,...],cmap='spectral')\n",
    "        \n",
    "    plt.show()\n",
    "plot_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_classifier(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defunct bits and pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_cancelled(model):\n",
    "    model.load_weights('./data/model_weights_class_3_run_54.hdf5')\n",
    "    imgs_mask_test = model.predict(x, verbose=1)\n",
    "    return imgs_mask_test\n",
    "\n",
    "preds = load_cancelled(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_classifier(model):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(model.history.history['loss'], \"-\",color='blue',label=\"Training final loss: \"+str(round(model.history.history['loss'][-1],4)))\n",
    "    ax.plot(model.history.history['jaccard'], \"-\",color='orange',label=\"testing final loss: \"+str(round(model.history.history['jaccard'][-1],4)))\n",
    "    #ax.set_xlim([0, epochs])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Jaccard / Loss')\n",
    "    plt.legend(loc='best')\n",
    "    ax.set_title('Metrics vs time (epochs)')\n",
    "    \n",
    "    '''ax = fig.add_subplot(122)\n",
    "    ax.plot(model.history['acc'], \"-\",color='blue',label=\"Training final acc: \"+str(round(model.history['acc'][-1],4)))\n",
    "    ax.plot(model.history['val_acc'], \"-\",color='orange',label=\"testing final acc: \"+str(round(model.history['val_acc'][-1],4)))\n",
    "    #ax.set_xlim([0, epochs])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    ax.set_title('Accuracy vs time (epochs)')    \n",
    "    '''\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the training data\n",
    "def import_data(load_whitened = False, normalize = True):\n",
    "    if load_whitened:\n",
    "        with open('./data/x_whitened_resized_array.pickle','rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            x = x.astype('float32')\n",
    "    else:\n",
    "        with open('./data/x_resized_array.pickle','rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            x = x.astype('float32')\n",
    "            # Normalize data to max values\n",
    "            for i in range(x.shape[0]):\n",
    "                for j in range(x.shape[1]):\n",
    "                    x[i,j,:,:] *= 1/x[i,j,:,:].max()\n",
    "                    \n",
    "    # Normalize values between 1 and 0\n",
    "    if normalize:\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                #x[i,j,:,:] /= x[i,j,:,:].max()\n",
    "                x[i,j,:,:] = (x[i,j,:,:].min() - x[i,j,:,:])/(x[i,j,:,:].min() - x[i,j,:,:].max())\n",
    "\n",
    "    with open('./data/y_resized_raster.pickle','rb') as f:\n",
    "        y = pickle.load(f)\n",
    "        y = y.astype(np.float32)\n",
    "        \n",
    "    y_oneclass = y[:,1:6,...]\n",
    "    \n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''\n",
    "    \n",
    "    return x, y, y_oneclass\n",
    "    \n",
    "    # Just use a single class: roads\n",
    "    #y = y[:,4,:,:]\n",
    "    #y = y[:,np.newaxis,:,:]\n",
    "\n",
    "    # y = y.reshape(y.shape[0],-1)\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def np_jaccard(y_true,y_pred,smooth=1.):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true * np.rint(y_pred))\n",
    "    return (intersection+smooth) / (np.sum(y_true) + np.sum(y_pred) - intersection+smooth)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
