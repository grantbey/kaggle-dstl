{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U-Net architecture\n",
    "\n",
    "See [here](https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py#L19) for code and [here](https://arxiv.org/pdf/1505.04597.pdf) for the original literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restart_kernel(restart=False):\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    if restart:\n",
    "        app.kernel.do_shutdown(True)\n",
    "\n",
    "restart_kernel(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Activation, Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "# \"tf\" assumes (rows, cols, channels) while \"th\" assumes (channels, rows, cols)\n",
    "# Possibly change this around natively in the data so the backend doesn't have to switch them\n",
    "# Only necessary if I use TF!\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pushbullet import Pushbullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run # 89\n"
     ]
    }
   ],
   "source": [
    "# Pushbullet notifier\n",
    "def push(title='Done!',text=''):\n",
    "    Pushbullet('o.YFPNNPfGRekivaCGHa4qMSgjZt8zJ6FL').devices[0].push_note(title,text)\n",
    "    \n",
    "# Import the training data\n",
    "def import_data(class_):\n",
    "    x = np.load('./data/x_augmented.npy','r')\n",
    "    y = np.load('./data/y_augmented.npy','r')\n",
    "    y_oneclass = y[:,class_:class_+1,...]\n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''    \n",
    "    return x, y, y_oneclass\n",
    "\n",
    "class_ = 2\n",
    "x, y, y_oneclass = import_data(class_)\n",
    "\n",
    "# Increment the counter\n",
    "def counter():\n",
    "    run = np.load('./data/run_counter.npy')\n",
    "    run += 1\n",
    "    np.save('./data/run_counter.npy',run)\n",
    "    return run\n",
    "run = counter()\n",
    "\n",
    "# Set the counter to a specific value\n",
    "def set_counter(run):\n",
    "    run = run\n",
    "    np.save('./data/run_counter.npy',run)\n",
    "    return run\n",
    "# Uncomment the next line to manually set the counter if something goes wrong\n",
    "#run = set_counter(89)\n",
    "print('This is run # %i' %run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 s, sys: 47.8 ms, total: 1.96 s\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def compiler(img_rows = x.shape[2],img_cols = x.shape[3],\n",
    "            nfilters = 32,activation = 'relu',init = 'he_normal',\n",
    "            lr=1.0,decay=0.0,momentum=0.0, nesterov=False,reg=0.01,p=[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2]):\n",
    "    \n",
    "    def jaccard(y_true, y_pred,smooth=1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "    \n",
    "    def Conv2DReluBatchNorm(n_filter, w_filter, h_filter, inputs, activation, init='he_uniform',dropout=0.2):\n",
    "        # Batch norm after activation / leakyrelu\n",
    "        #return BatchNormalization(mode=2, axis=1)(LeakyReLU()((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "        # Batch norm before activation\n",
    "        #return LeakyReLU()(BatchNormalization(mode=0, axis=1)((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "        # Batch norm after activation / relu\n",
    "        return BatchNormalization(mode=2, axis=1)(Activation(activation=activation)((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "    def up_conv(nfilters,filter_factor,inputs,init=init,activation=activation):\n",
    "        # No batch norm\n",
    "        #return LeakyReLU()(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs)))\n",
    "        \n",
    "        # Batch norm after activation\n",
    "        #return BatchNormalization(mode=2, axis=1)(LeakyReLU()(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs))))\n",
    "        \n",
    "        # Batch norm after activation / relu\n",
    "        return BatchNormalization(mode=2, axis=1)(Activation(activation=activation)(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs))))\n",
    "\n",
    "    inputs = Input((20, img_rows, img_cols))\n",
    "    padded = ZeroPadding2D(padding=(12,12))(inputs)\n",
    "    \n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, padded, activation=activation,init=init)\n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, conv1, activation=activation,init=init)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(p=p[0])(pool1)\n",
    "\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, pool1, activation=activation,init=init)\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv2, activation=activation,init=init)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(p=p[1])(pool2)\n",
    "\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, pool2, activation=activation,init=init)\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv3, activation=activation,init=init)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(p=p[2])(pool3)\n",
    "\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, pool3, activation=activation,init=init)\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv4, activation=activation,init=init)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = Dropout(p=p[3])(pool4)\n",
    "\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, pool4, activation=activation,init=init)\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, conv5, activation=activation,init=init)\n",
    "    conv5 = Dropout(p=p[4])(conv5)\n",
    "        \n",
    "    up6 = merge([up_conv(nfilters,8,conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, up6, activation=activation,init=init)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv6, activation=activation,init=init)\n",
    "    conv6 = Dropout(p=p[5])(conv6)\n",
    "\n",
    "    up7 = merge([up_conv(nfilters,4,conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, up7, activation=activation,init=init)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv7, activation=activation,init=init)\n",
    "    conv7 = Dropout(p=p[6])(conv7)\n",
    "\n",
    "    up8 = merge([up_conv(nfilters,2,conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, up8, activation=activation,init=init)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv8, activation=activation,init=init)\n",
    "    conv8 = Dropout(p=p[7])(conv8)\n",
    "\n",
    "    up9 = merge([up_conv(nfilters,1,conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, up9, activation=activation,init=init)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, conv9, activation=activation,init=init)\n",
    "    conv9 = Dropout(p=p[8])(conv9)\n",
    "    \n",
    "    conv10 = Conv2DReluBatchNorm(1, 1, 1, conv9, activation='relu',init=init)\n",
    "    cropped = Cropping2D(cropping=((12,12), (12,12)))(conv10)\n",
    "    output = Activation(activation='sigmoid')(cropped)\n",
    "    \n",
    "    model = Model(input=inputs, output=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=lr,decay=decay), loss='binary_crossentropy', metrics=[jaccard])\n",
    "    \n",
    "    return model\n",
    "\n",
    "p=[0.1,0.2,0.3,0.4,0.5,0.4,0.3,0.2,0.1] # current version\n",
    "#p=[0.2,0.3,0.4,0.5,0.5,0.5,0.4,0.3,0.2] # symmetric but more dropout\n",
    "#p=[0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6] # increasing\n",
    "\n",
    "model = compiler(img_rows=x.shape[2],img_cols=x.shape[3],\n",
    "            nfilters=16,activation='relu',init='he_normal',\n",
    "            lr=0.001,decay=0,momentum=0,reg=0,p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run # 89\n",
      "Train on 320 samples, validate on 80 samples\n",
      "Epoch 1/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5252 - jaccard: 0.0138 - val_loss: 0.4435 - val_jaccard: 0.0156\n",
      "Epoch 2/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4414 - jaccard: 0.0139 - val_loss: 0.4311 - val_jaccard: 0.0156\n",
      "Epoch 3/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4294 - jaccard: 0.0138 - val_loss: 0.4207 - val_jaccard: 0.0156\n",
      "Epoch 4/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4192 - jaccard: 0.0139 - val_loss: 0.4115 - val_jaccard: 0.0156\n",
      "Epoch 5/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4099 - jaccard: 0.0139 - val_loss: 0.4031 - val_jaccard: 0.0156\n",
      "Epoch 6/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4013 - jaccard: 0.0139 - val_loss: 0.3952 - val_jaccard: 0.0156\n",
      "Epoch 7/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3932 - jaccard: 0.0140 - val_loss: 0.3876 - val_jaccard: 0.0158\n",
      "Epoch 8/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3854 - jaccard: 0.0141 - val_loss: 0.3803 - val_jaccard: 0.0158\n",
      "Epoch 9/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3779 - jaccard: 0.0143 - val_loss: 0.3732 - val_jaccard: 0.0159\n",
      "Epoch 10/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3707 - jaccard: 0.0143 - val_loss: 0.3663 - val_jaccard: 0.0160\n",
      "Epoch 11/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3636 - jaccard: 0.0145 - val_loss: 0.3598 - val_jaccard: 0.0162\n",
      "Epoch 12/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3568 - jaccard: 0.0145 - val_loss: 0.3532 - val_jaccard: 0.0161\n",
      "Epoch 13/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3502 - jaccard: 0.0146 - val_loss: 0.3468 - val_jaccard: 0.0162\n",
      "Epoch 14/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3437 - jaccard: 0.0148 - val_loss: 0.3406 - val_jaccard: 0.0160\n",
      "Epoch 15/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3375 - jaccard: 0.0149 - val_loss: 0.3345 - val_jaccard: 0.0161\n",
      "Epoch 16/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3314 - jaccard: 0.0150 - val_loss: 0.3286 - val_jaccard: 0.0163\n",
      "Epoch 17/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3255 - jaccard: 0.0150 - val_loss: 0.3231 - val_jaccard: 0.0163\n",
      "Epoch 18/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3196 - jaccard: 0.0154 - val_loss: 0.3173 - val_jaccard: 0.0166\n",
      "Epoch 19/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3139 - jaccard: 0.0155 - val_loss: 0.3118 - val_jaccard: 0.0169\n",
      "Epoch 20/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3083 - jaccard: 0.0161 - val_loss: 0.3064 - val_jaccard: 0.0172\n",
      "Epoch 21/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3029 - jaccard: 0.0161 - val_loss: 0.3012 - val_jaccard: 0.0176\n",
      "Epoch 22/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2976 - jaccard: 0.0165 - val_loss: 0.2963 - val_jaccard: 0.0184\n",
      "Epoch 23/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2924 - jaccard: 0.0168 - val_loss: 0.2911 - val_jaccard: 0.0188\n",
      "Epoch 24/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2875 - jaccard: 0.0170 - val_loss: 0.2861 - val_jaccard: 0.0175\n",
      "Epoch 25/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2825 - jaccard: 0.0173 - val_loss: 0.2816 - val_jaccard: 0.0174\n",
      "Epoch 26/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2777 - jaccard: 0.0177 - val_loss: 0.2768 - val_jaccard: 0.0200\n",
      "Epoch 27/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2728 - jaccard: 0.0185 - val_loss: 0.2717 - val_jaccard: 0.0219\n",
      "Epoch 28/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2683 - jaccard: 0.0189 - val_loss: 0.2665 - val_jaccard: 0.0222\n",
      "Epoch 29/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2638 - jaccard: 0.0193 - val_loss: 0.2624 - val_jaccard: 0.0228\n",
      "Epoch 30/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2594 - jaccard: 0.0197 - val_loss: 0.2573 - val_jaccard: 0.0249\n",
      "Epoch 31/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2549 - jaccard: 0.0206 - val_loss: 0.2541 - val_jaccard: 0.0236\n",
      "Epoch 32/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2507 - jaccard: 0.0211 - val_loss: 0.2503 - val_jaccard: 0.0213\n",
      "Epoch 33/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2469 - jaccard: 0.0211 - val_loss: 0.2452 - val_jaccard: 0.0256\n",
      "Epoch 34/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2427 - jaccard: 0.0217 - val_loss: 0.2416 - val_jaccard: 0.0263\n",
      "Epoch 35/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2386 - jaccard: 0.0227 - val_loss: 0.2382 - val_jaccard: 0.0232\n",
      "Epoch 36/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2350 - jaccard: 0.0228 - val_loss: 0.2343 - val_jaccard: 0.0278\n",
      "Epoch 37/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2319 - jaccard: 0.0224 - val_loss: 0.2316 - val_jaccard: 0.0223\n",
      "Epoch 38/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2288 - jaccard: 0.0215 - val_loss: 0.2283 - val_jaccard: 0.0227\n",
      "Epoch 39/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2243 - jaccard: 0.0239 - val_loss: 0.2236 - val_jaccard: 0.0286\n",
      "Epoch 40/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2207 - jaccard: 0.0249 - val_loss: 0.2203 - val_jaccard: 0.0287\n",
      "Epoch 41/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2173 - jaccard: 0.0254 - val_loss: 0.2175 - val_jaccard: 0.0264\n",
      "Epoch 42/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2143 - jaccard: 0.0249 - val_loss: 0.2146 - val_jaccard: 0.0320\n",
      "Epoch 43/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2117 - jaccard: 0.0247 - val_loss: 0.2117 - val_jaccard: 0.0257\n",
      "Epoch 44/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2081 - jaccard: 0.0258 - val_loss: 0.2083 - val_jaccard: 0.0286\n",
      "Epoch 45/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2050 - jaccard: 0.0267 - val_loss: 0.2052 - val_jaccard: 0.0267\n",
      "Epoch 46/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2017 - jaccard: 0.0275 - val_loss: 0.2019 - val_jaccard: 0.0324\n",
      "Epoch 47/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1987 - jaccard: 0.0285 - val_loss: 0.1993 - val_jaccard: 0.0298\n",
      "Epoch 48/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1960 - jaccard: 0.0283 - val_loss: 0.1959 - val_jaccard: 0.0347\n",
      "Epoch 49/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1931 - jaccard: 0.0295 - val_loss: 0.1940 - val_jaccard: 0.0293\n",
      "Epoch 50/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1902 - jaccard: 0.0301 - val_loss: 0.1905 - val_jaccard: 0.0353\n",
      "Epoch 51/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1868 - jaccard: 0.0331 - val_loss: 0.1885 - val_jaccard: 0.0316\n",
      "Epoch 52/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1847 - jaccard: 0.0317 - val_loss: 0.1849 - val_jaccard: 0.0369\n",
      "Epoch 53/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1808 - jaccard: 0.0351 - val_loss: 0.1821 - val_jaccard: 0.0337\n",
      "Epoch 54/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1767 - jaccard: 0.0394 - val_loss: 0.1760 - val_jaccard: 0.0432\n",
      "Epoch 55/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1749 - jaccard: 0.0384 - val_loss: 0.1731 - val_jaccard: 0.0525\n",
      "Epoch 56/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1708 - jaccard: 0.0434 - val_loss: 0.1699 - val_jaccard: 0.0468\n",
      "Epoch 57/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1667 - jaccard: 0.0476 - val_loss: 0.1653 - val_jaccard: 0.0581\n",
      "Epoch 58/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1638 - jaccard: 0.0500 - val_loss: 0.1625 - val_jaccard: 0.0591\n",
      "Epoch 59/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1610 - jaccard: 0.0523 - val_loss: 0.1616 - val_jaccard: 0.0650\n",
      "Epoch 60/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1589 - jaccard: 0.0538 - val_loss: 0.1597 - val_jaccard: 0.0504\n",
      "Epoch 61/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1567 - jaccard: 0.0532 - val_loss: 0.1559 - val_jaccard: 0.0650\n",
      "Epoch 62/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1543 - jaccard: 0.0557 - val_loss: 0.1545 - val_jaccard: 0.0599\n",
      "Epoch 63/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1520 - jaccard: 0.0561 - val_loss: 0.1513 - val_jaccard: 0.0652\n",
      "Epoch 64/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1490 - jaccard: 0.0597 - val_loss: 0.1491 - val_jaccard: 0.0637\n",
      "Epoch 65/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1465 - jaccard: 0.0620 - val_loss: 0.1479 - val_jaccard: 0.0602\n",
      "Epoch 66/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1445 - jaccard: 0.0619 - val_loss: 0.1446 - val_jaccard: 0.0730\n",
      "Epoch 67/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1424 - jaccard: 0.0648 - val_loss: 0.1432 - val_jaccard: 0.0711\n",
      "Epoch 68/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1405 - jaccard: 0.0650 - val_loss: 0.1415 - val_jaccard: 0.0654\n",
      "Epoch 69/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1386 - jaccard: 0.0654 - val_loss: 0.1384 - val_jaccard: 0.0745\n",
      "Epoch 70/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1360 - jaccard: 0.0687 - val_loss: 0.1367 - val_jaccard: 0.0752\n",
      "Epoch 71/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1340 - jaccard: 0.0699 - val_loss: 0.1352 - val_jaccard: 0.0779\n",
      "Epoch 72/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1324 - jaccard: 0.0717 - val_loss: 0.1340 - val_jaccard: 0.0718\n",
      "Epoch 73/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1300 - jaccard: 0.0735 - val_loss: 0.1311 - val_jaccard: 0.0797\n",
      "Epoch 74/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1280 - jaccard: 0.0756 - val_loss: 0.1294 - val_jaccard: 0.0790\n",
      "Epoch 75/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1261 - jaccard: 0.0772 - val_loss: 0.1277 - val_jaccard: 0.0834\n",
      "Epoch 76/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1245 - jaccard: 0.0779 - val_loss: 0.1258 - val_jaccard: 0.0854\n",
      "Epoch 77/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1229 - jaccard: 0.0805 - val_loss: 0.1248 - val_jaccard: 0.0818\n",
      "Epoch 78/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1216 - jaccard: 0.0790 - val_loss: 0.1240 - val_jaccard: 0.0796\n",
      "Epoch 79/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1199 - jaccard: 0.0805 - val_loss: 0.1215 - val_jaccard: 0.0897\n",
      "Epoch 80/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1181 - jaccard: 0.0830 - val_loss: 0.1206 - val_jaccard: 0.0831\n",
      "Epoch 81/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1166 - jaccard: 0.0834 - val_loss: 0.1190 - val_jaccard: 0.0917\n",
      "Epoch 82/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1149 - jaccard: 0.0857 - val_loss: 0.1170 - val_jaccard: 0.0931\n",
      "Epoch 83/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1135 - jaccard: 0.0875 - val_loss: 0.1157 - val_jaccard: 0.0896\n",
      "Epoch 84/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1120 - jaccard: 0.0870 - val_loss: 0.1154 - val_jaccard: 0.0894\n",
      "Epoch 85/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1110 - jaccard: 0.0886 - val_loss: 0.1134 - val_jaccard: 0.0915\n",
      "Epoch 86/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1094 - jaccard: 0.0893 - val_loss: 0.1119 - val_jaccard: 0.0913\n",
      "Epoch 87/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1079 - jaccard: 0.0913 - val_loss: 0.1101 - val_jaccard: 0.0966\n",
      "Epoch 88/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1065 - jaccard: 0.0922 - val_loss: 0.1091 - val_jaccard: 0.0930\n",
      "Epoch 89/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1049 - jaccard: 0.0938 - val_loss: 0.1069 - val_jaccard: 0.1028\n",
      "Epoch 90/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1034 - jaccard: 0.0975 - val_loss: 0.1062 - val_jaccard: 0.0998\n",
      "Epoch 91/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1023 - jaccard: 0.0958 - val_loss: 0.1047 - val_jaccard: 0.1023\n",
      "Epoch 92/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1010 - jaccard: 0.0984 - val_loss: 0.1045 - val_jaccard: 0.1062\n",
      "Epoch 93/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1000 - jaccard: 0.1000 - val_loss: 0.1024 - val_jaccard: 0.1090\n",
      "Epoch 94/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0989 - jaccard: 0.1001 - val_loss: 0.1017 - val_jaccard: 0.1057\n",
      "Epoch 95/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0976 - jaccard: 0.1016 - val_loss: 0.1002 - val_jaccard: 0.1077\n",
      "Epoch 96/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0959 - jaccard: 0.1053 - val_loss: 0.0984 - val_jaccard: 0.1093\n",
      "Epoch 97/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0945 - jaccard: 0.1061 - val_loss: 0.0973 - val_jaccard: 0.1095\n",
      "Epoch 98/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0936 - jaccard: 0.1078 - val_loss: 0.0967 - val_jaccard: 0.1169\n",
      "Epoch 99/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0924 - jaccard: 0.1081 - val_loss: 0.0953 - val_jaccard: 0.1097\n",
      "Epoch 100/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0911 - jaccard: 0.1104 - val_loss: 0.0940 - val_jaccard: 0.1185\n",
      "Epoch 101/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0898 - jaccard: 0.1139 - val_loss: 0.0927 - val_jaccard: 0.1205\n",
      "Epoch 102/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0887 - jaccard: 0.1142 - val_loss: 0.0918 - val_jaccard: 0.1191\n",
      "Epoch 103/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0875 - jaccard: 0.1171 - val_loss: 0.0913 - val_jaccard: 0.1223\n",
      "Epoch 104/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0864 - jaccard: 0.1178 - val_loss: 0.0902 - val_jaccard: 0.1188\n",
      "Epoch 105/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0856 - jaccard: 0.1178 - val_loss: 0.0901 - val_jaccard: 0.1204\n",
      "Epoch 106/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0846 - jaccard: 0.1212 - val_loss: 0.0885 - val_jaccard: 0.1270\n",
      "Epoch 107/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0836 - jaccard: 0.1207 - val_loss: 0.0874 - val_jaccard: 0.1242\n",
      "Epoch 108/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0826 - jaccard: 0.1235 - val_loss: 0.0867 - val_jaccard: 0.1230\n",
      "Epoch 109/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0818 - jaccard: 0.1222 - val_loss: 0.0859 - val_jaccard: 0.1333\n",
      "Epoch 110/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0813 - jaccard: 0.1262 - val_loss: 0.0852 - val_jaccard: 0.1293\n",
      "Epoch 111/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0800 - jaccard: 0.1273 - val_loss: 0.0841 - val_jaccard: 0.1288\n",
      "Epoch 112/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0792 - jaccard: 0.1278 - val_loss: 0.0827 - val_jaccard: 0.1348\n",
      "Epoch 113/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0782 - jaccard: 0.1308 - val_loss: 0.0824 - val_jaccard: 0.1348\n",
      "Epoch 114/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0773 - jaccard: 0.1300 - val_loss: 0.0816 - val_jaccard: 0.1356\n",
      "Epoch 115/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0763 - jaccard: 0.1330 - val_loss: 0.0802 - val_jaccard: 0.1444\n",
      "Epoch 116/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0755 - jaccard: 0.1344 - val_loss: 0.0799 - val_jaccard: 0.1345\n",
      "Epoch 117/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0746 - jaccard: 0.1362 - val_loss: 0.0788 - val_jaccard: 0.1329\n",
      "Epoch 118/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0738 - jaccard: 0.1366 - val_loss: 0.0783 - val_jaccard: 0.1371\n",
      "Epoch 119/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0730 - jaccard: 0.1401 - val_loss: 0.0769 - val_jaccard: 0.1461\n",
      "Epoch 120/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0720 - jaccard: 0.1433 - val_loss: 0.0768 - val_jaccard: 0.1430\n",
      "Epoch 121/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0710 - jaccard: 0.1432 - val_loss: 0.0756 - val_jaccard: 0.1502\n",
      "Epoch 122/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0703 - jaccard: 0.1464 - val_loss: 0.0757 - val_jaccard: 0.1405\n",
      "Epoch 123/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0697 - jaccard: 0.1467 - val_loss: 0.0745 - val_jaccard: 0.1532\n",
      "Epoch 124/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0688 - jaccard: 0.1483 - val_loss: 0.0735 - val_jaccard: 0.1532\n",
      "Epoch 125/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0681 - jaccard: 0.1498 - val_loss: 0.0734 - val_jaccard: 0.1509\n",
      "Epoch 126/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0674 - jaccard: 0.1522 - val_loss: 0.0723 - val_jaccard: 0.1485\n",
      "Epoch 127/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0666 - jaccard: 0.1520 - val_loss: 0.0714 - val_jaccard: 0.1535\n",
      "Epoch 128/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0659 - jaccard: 0.1556 - val_loss: 0.0707 - val_jaccard: 0.1550\n",
      "Epoch 129/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0652 - jaccard: 0.1556 - val_loss: 0.0699 - val_jaccard: 0.1577\n",
      "Epoch 130/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0645 - jaccard: 0.1559 - val_loss: 0.0695 - val_jaccard: 0.1633\n",
      "Epoch 131/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0641 - jaccard: 0.1579 - val_loss: 0.0692 - val_jaccard: 0.1530\n",
      "Epoch 132/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0635 - jaccard: 0.1582 - val_loss: 0.0683 - val_jaccard: 0.1650\n",
      "Epoch 133/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0629 - jaccard: 0.1603 - val_loss: 0.0677 - val_jaccard: 0.1619\n",
      "Epoch 134/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0619 - jaccard: 0.1629 - val_loss: 0.0670 - val_jaccard: 0.1662\n",
      "Epoch 135/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0613 - jaccard: 0.1657 - val_loss: 0.0664 - val_jaccard: 0.1687\n",
      "Epoch 136/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0605 - jaccard: 0.1681 - val_loss: 0.0661 - val_jaccard: 0.1666\n",
      "Epoch 137/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0597 - jaccard: 0.1709 - val_loss: 0.0646 - val_jaccard: 0.1730\n",
      "Epoch 138/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0590 - jaccard: 0.1743 - val_loss: 0.0644 - val_jaccard: 0.1724\n",
      "Epoch 139/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0585 - jaccard: 0.1734 - val_loss: 0.0644 - val_jaccard: 0.1798\n",
      "Epoch 140/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0585 - jaccard: 0.1706 - val_loss: 0.0641 - val_jaccard: 0.1806\n",
      "Epoch 141/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0585 - jaccard: 0.1682 - val_loss: 0.0658 - val_jaccard: 0.1567\n",
      "Epoch 142/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0587 - jaccard: 0.1700 - val_loss: 0.0641 - val_jaccard: 0.1666\n",
      "Epoch 143/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0575 - jaccard: 0.1727 - val_loss: 0.0622 - val_jaccard: 0.1730\n",
      "Epoch 144/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0565 - jaccard: 0.1757 - val_loss: 0.0621 - val_jaccard: 0.1800\n",
      "Epoch 145/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0561 - jaccard: 0.1778 - val_loss: 0.0624 - val_jaccard: 0.1684\n",
      "Epoch 146/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0554 - jaccard: 0.1810 - val_loss: 0.0608 - val_jaccard: 0.1769\n",
      "Epoch 147/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0545 - jaccard: 0.1846 - val_loss: 0.0598 - val_jaccard: 0.1817\n",
      "Epoch 148/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0540 - jaccard: 0.1832 - val_loss: 0.0594 - val_jaccard: 0.1863\n",
      "Epoch 149/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0533 - jaccard: 0.1857 - val_loss: 0.0593 - val_jaccard: 0.1899\n",
      "Epoch 150/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0526 - jaccard: 0.1883 - val_loss: 0.0586 - val_jaccard: 0.1874\n",
      "Epoch 151/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0520 - jaccard: 0.1924 - val_loss: 0.0579 - val_jaccard: 0.1884\n",
      "Epoch 152/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0514 - jaccard: 0.1932 - val_loss: 0.0576 - val_jaccard: 0.1898\n",
      "Epoch 153/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0511 - jaccard: 0.1952 - val_loss: 0.0575 - val_jaccard: 0.1970\n",
      "Epoch 154/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0507 - jaccard: 0.1961 - val_loss: 0.0564 - val_jaccard: 0.1936\n",
      "Epoch 155/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0501 - jaccard: 0.1981 - val_loss: 0.0560 - val_jaccard: 0.1931\n",
      "Epoch 156/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0496 - jaccard: 0.1991 - val_loss: 0.0558 - val_jaccard: 0.1946\n",
      "Epoch 157/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0491 - jaccard: 0.2037 - val_loss: 0.0552 - val_jaccard: 0.1984\n",
      "Epoch 158/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0486 - jaccard: 0.2054 - val_loss: 0.0545 - val_jaccard: 0.2040\n",
      "Epoch 159/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0481 - jaccard: 0.2066 - val_loss: 0.0550 - val_jaccard: 0.2060\n",
      "Epoch 160/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0478 - jaccard: 0.2084 - val_loss: 0.0543 - val_jaccard: 0.2008\n",
      "Epoch 161/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0472 - jaccard: 0.2099 - val_loss: 0.0535 - val_jaccard: 0.2062\n",
      "Epoch 162/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0468 - jaccard: 0.2126 - val_loss: 0.0531 - val_jaccard: 0.2077\n",
      "Epoch 163/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0464 - jaccard: 0.2144 - val_loss: 0.0530 - val_jaccard: 0.2037\n",
      "Epoch 164/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0460 - jaccard: 0.2129 - val_loss: 0.0528 - val_jaccard: 0.2060\n",
      "Epoch 165/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0456 - jaccard: 0.2166 - val_loss: 0.0524 - val_jaccard: 0.2028\n",
      "Epoch 166/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0451 - jaccard: 0.2175 - val_loss: 0.0518 - val_jaccard: 0.2126\n",
      "Epoch 167/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0446 - jaccard: 0.2209 - val_loss: 0.0515 - val_jaccard: 0.2182\n",
      "Epoch 168/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0442 - jaccard: 0.2228 - val_loss: 0.0511 - val_jaccard: 0.2178\n",
      "Epoch 169/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0440 - jaccard: 0.2217 - val_loss: 0.0513 - val_jaccard: 0.2109\n",
      "Epoch 170/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0438 - jaccard: 0.2249 - val_loss: 0.0509 - val_jaccard: 0.2201\n",
      "Epoch 171/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0436 - jaccard: 0.2230 - val_loss: 0.0510 - val_jaccard: 0.2061\n",
      "Epoch 172/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0431 - jaccard: 0.2255 - val_loss: 0.0495 - val_jaccard: 0.2271\n",
      "Epoch 173/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0425 - jaccard: 0.2305 - val_loss: 0.0501 - val_jaccard: 0.2204\n",
      "Epoch 174/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0421 - jaccard: 0.2266 - val_loss: 0.0489 - val_jaccard: 0.2299\n",
      "Epoch 175/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0418 - jaccard: 0.2319 - val_loss: 0.0491 - val_jaccard: 0.2206\n",
      "Epoch 176/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0415 - jaccard: 0.2306 - val_loss: 0.0492 - val_jaccard: 0.2173\n",
      "Epoch 177/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0413 - jaccard: 0.2328 - val_loss: 0.0486 - val_jaccard: 0.2262\n",
      "Epoch 178/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0416 - jaccard: 0.2318 - val_loss: 0.0483 - val_jaccard: 0.2287\n",
      "Epoch 179/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0407 - jaccard: 0.2348 - val_loss: 0.0481 - val_jaccard: 0.2256\n",
      "Epoch 180/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0401 - jaccard: 0.2406 - val_loss: 0.0468 - val_jaccard: 0.2370\n",
      "Epoch 181/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0396 - jaccard: 0.2387 - val_loss: 0.0463 - val_jaccard: 0.2341\n",
      "Epoch 182/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0392 - jaccard: 0.2450 - val_loss: 0.0465 - val_jaccard: 0.2394\n",
      "Epoch 183/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0388 - jaccard: 0.2447 - val_loss: 0.0459 - val_jaccard: 0.2387\n",
      "Epoch 184/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0384 - jaccard: 0.2527 - val_loss: 0.0454 - val_jaccard: 0.2403\n",
      "Epoch 185/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0382 - jaccard: 0.2497 - val_loss: 0.0454 - val_jaccard: 0.2411\n",
      "Epoch 186/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0377 - jaccard: 0.2537 - val_loss: 0.0450 - val_jaccard: 0.2471\n",
      "Epoch 187/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0375 - jaccard: 0.2544 - val_loss: 0.0448 - val_jaccard: 0.2432\n",
      "Epoch 188/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0372 - jaccard: 0.2552 - val_loss: 0.0443 - val_jaccard: 0.2510\n",
      "Epoch 189/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0369 - jaccard: 0.2572 - val_loss: 0.0451 - val_jaccard: 0.2458\n",
      "Epoch 190/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0366 - jaccard: 0.2606 - val_loss: 0.0445 - val_jaccard: 0.2476\n",
      "Epoch 191/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0363 - jaccard: 0.2572 - val_loss: 0.0439 - val_jaccard: 0.2507\n",
      "Epoch 192/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0359 - jaccard: 0.2637 - val_loss: 0.0433 - val_jaccard: 0.2562\n",
      "Epoch 193/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0356 - jaccard: 0.2667 - val_loss: 0.0433 - val_jaccard: 0.2522\n",
      "Epoch 194/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0353 - jaccard: 0.2628 - val_loss: 0.0430 - val_jaccard: 0.2588\n",
      "Epoch 195/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0351 - jaccard: 0.2658 - val_loss: 0.0427 - val_jaccard: 0.2584\n",
      "Epoch 196/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0348 - jaccard: 0.2684 - val_loss: 0.0426 - val_jaccard: 0.2551\n",
      "Epoch 197/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0345 - jaccard: 0.2726 - val_loss: 0.0422 - val_jaccard: 0.2567\n",
      "Epoch 198/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0341 - jaccard: 0.2754 - val_loss: 0.0418 - val_jaccard: 0.2629\n",
      "Epoch 199/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0338 - jaccard: 0.2758 - val_loss: 0.0420 - val_jaccard: 0.2638\n",
      "Epoch 200/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0337 - jaccard: 0.2709 - val_loss: 0.0416 - val_jaccard: 0.2692\n",
      "Epoch 201/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0334 - jaccard: 0.2786 - val_loss: 0.0418 - val_jaccard: 0.2623\n",
      "Epoch 202/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0331 - jaccard: 0.2799 - val_loss: 0.0413 - val_jaccard: 0.2625\n",
      "Epoch 203/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0328 - jaccard: 0.2855 - val_loss: 0.0415 - val_jaccard: 0.2697\n",
      "Epoch 204/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0326 - jaccard: 0.2812 - val_loss: 0.0406 - val_jaccard: 0.2674\n",
      "Epoch 205/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0324 - jaccard: 0.2851 - val_loss: 0.0403 - val_jaccard: 0.2698\n",
      "Epoch 206/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0319 - jaccard: 0.2909 - val_loss: 0.0401 - val_jaccard: 0.2769\n",
      "Epoch 207/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0317 - jaccard: 0.2913 - val_loss: 0.0400 - val_jaccard: 0.2769\n",
      "Epoch 208/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0314 - jaccard: 0.2931 - val_loss: 0.0395 - val_jaccard: 0.2826\n",
      "Epoch 209/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0312 - jaccard: 0.2908 - val_loss: 0.0398 - val_jaccard: 0.2763\n",
      "Epoch 210/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0310 - jaccard: 0.2911 - val_loss: 0.0396 - val_jaccard: 0.2766\n",
      "Epoch 211/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0308 - jaccard: 0.2963 - val_loss: 0.0392 - val_jaccard: 0.2813\n",
      "Epoch 212/1000\n",
      "320/320 [==============================] - 20s - loss: 0.0306 - jaccard: 0.2965 - val_loss: 0.0389 - val_jaccard: 0.2791\n",
      "Epoch 213/1000\n",
      "140/320 [============>.................] - ETA: 10s - loss: 0.0308 - jaccard: 0.3103"
     ]
    }
   ],
   "source": [
    "def trainer(model,fit=True,use_existing=False):\n",
    "    print('This is run # %i' %run)\n",
    "    \n",
    "    if use_existing:\n",
    "        model.load_weights('./data/model_weights_class_{}_run_{}.hdf5'.format(_class,run))\n",
    "        \n",
    "    if fit:\n",
    "        quitter = EarlyStopping(monitor='loss', min_delta=0.001, patience=100, verbose=1, mode='auto')\n",
    "        lrreducer = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, verbose=1, mode='auto', epsilon=0.001, cooldown=2, min_lr=0)\n",
    "        model_checkpoint = ModelCheckpoint('./data/model_weights_class_{}_run_{}.hdf5'.format(class_,run), monitor='loss', save_best_only=True)\n",
    "        csvlogger = CSVLogger('./data/training_log_run_'+str(run), separator=',', append=True)\n",
    "\n",
    "        model.fit(x, y_oneclass,\n",
    "                  batch_size=20,\n",
    "                  nb_epoch=1000,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  callbacks=[model_checkpoint,csvlogger],\n",
    "                  validation_split=0.2,\n",
    "                  initial_epoch=0)\n",
    "            \n",
    "    preds = model.predict(x, verbose=1)\n",
    "    np.save('preds.npy', preds)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = trainer(model,fit=True,use_existing=False)\n",
    "model.save('u-net-complete-model-run_{}_class_{}.h5'.format(run,class_))\n",
    "push('Training is done',\n",
    "     'Train loss: %f, train jaccard: %f, val loss %f, val jaccard%f' %(model.history.history['loss'][-1],model.history.history['jaccard'][-1],model.history.history['val_loss'][-1],model.history.history['val_jaccard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
