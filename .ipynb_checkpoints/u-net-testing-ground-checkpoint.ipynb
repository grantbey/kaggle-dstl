{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and thoughts\n",
    "\n",
    "#### Current best ideas / runs that work / etc.\n",
    "- Batch size = ~1/6th of training data size seems to make all the difference\n",
    "- nfilters is only 16 for a single class\n",
    "- Perhaps the best method is to train 10 models, one for each class\n",
    "\n",
    "#### Ideas\n",
    "- Weight map: sum the total area of all classes in y, then calculate each class' proportion of the total and use `1-value` in place of 1 in the binary mask. This will cause low frequency classes to contribute more to the total loss, i.e. penalizing the model when it fails to predict low frequency classes.\n",
    "\n",
    "#### Data augmentation / image manipulation\n",
    "- [Histogram Equalization](http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html#sphx-glr-auto-examples-color-exposure-plot-equalize-py) (also see [here](https://www.kaggle.com/gabrielaltay/dstl-satellite-imagery-feature-detection/exploring-color-scaling-images/discussion), [here](http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/) and [here](https://medium.com/@vivek.yadav/improved-performance-of-deep-learning-neural-network-models-on-traffic-sign-classification-using-6355346da2dc#.x9nidcsh6))\n",
    "- Rotation (with reflection): see data-augmentation.ipynb\n",
    "- Image normalization: see image-preprocessing-new.ipynb\n",
    "\n",
    "#### Overfitting solutions\n",
    "- CNNs are supposed to be more robust to this because of the shared weight matrix of each filter\n",
    "- **Data augmentation!**\n",
    "- L2 regularization (added into the layers via `W_regularizer=l2(l=0.01)` parameter)\n",
    "- Dropout hurts the model in my experience..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U-Net architecture\n",
    "\n",
    "See [here](https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py#L19) for code and [here](https://arxiv.org/pdf/1505.04597.pdf) for the original literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Activation, Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D, BatchNormalization\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "# \"tf\" assumes (rows, cols, channels) while \"th\" assumes (channels, rows, cols)\n",
    "# Possibly change this around natively in the data so the backend doesn't have to switch them\n",
    "# Only necessary if I use TF!\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_preds(preds):\n",
    "    for i in range(preds.shape[0]):\n",
    "        preds[i,0,:,:] = (preds[i,0,:,:].min() - preds[i,0,:,:])/(preds[i,0,:,:].min()-preds[i,0,:,:].max())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_all(i,classType):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.imshow(preds[i,0,...],cmap='spectral')\n",
    "\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.imshow(np.rint(preds[i,0,...]),cmap='spectral')\n",
    "\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax3.imshow(x[i,17,...],cmap='Greys')\n",
    "\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    ax4.imshow(y_oneclass[i,classType,:,:],cmap='spectral')\n",
    "    \n",
    "    plt.show()\n",
    "#plot_all(2,2,classType,0.5)\n",
    "#push('PICTURES!','The plots are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the components of the pushbullet API\n",
    "from pushbullet import Pushbullet\n",
    "\n",
    "pb = Pushbullet('o.YFPNNPfGRekivaCGHa4qMSgjZt8zJ6FL')\n",
    "phone = pb.devices[0]\n",
    "\n",
    "# Run this cell after anything you want to be notified about!\n",
    "def push(title='Done!',text='Whatever it was, it\\'s done'):\n",
    "    phone.push_note(title,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the training data\n",
    "def import_data():\n",
    "    x = np.load('./data/x_augmented.npy','r')\n",
    "    y = np.load('./data/y_augmented.npy','r')\n",
    "    \n",
    "    '''with open('./data/x_resized_array.pickle','rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        x = x.astype(np.float32)\n",
    "        \n",
    "    with open('./data/y_resized_raster.pickle','rb') as f:\n",
    "        y = pickle.load(f)\n",
    "        y = y.astype(np.float32)'''\n",
    "    \n",
    "    y_oneclass = y[:,3:4,...]\n",
    "    \n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''\n",
    "    \n",
    "    return x, y, y_oneclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, y_oneclass = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.08 s, sys: 12.2 ms, total: 1.09 s\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "V2.0 U-Net with batchnorm\n",
    "'''\n",
    "def builder(img_rows = x.shape[2],img_cols = x.shape[3],\n",
    "            nfilters = 32,activation = 'relu',init = 'he_normal',\n",
    "            lr=1.0,decay=0.0,momentum=0.0, nesterov=False,reg=0.01):\n",
    "    \n",
    "    def jaccard(y_true, y_pred,smooth=1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "    \n",
    "    def Conv2DReluBatchNorm(n_filter, w_filter, h_filter, inputs, activation='relu',init='he_uniform',dropout=0.2):\n",
    "        return BatchNormalization(mode=0, axis=1)(Activation(activation=activation)(Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs)))\n",
    "        \n",
    "    def up_conv(nfilters,filter_factor,inputs,init=init,activation=activation):\n",
    "        return Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,activation=activation,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs))\n",
    "    \n",
    "    inputs = Input((20, img_rows, img_cols))\n",
    "    padded = ZeroPadding2D(padding=(12,12))(inputs)\n",
    "    \n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, padded, activation=activation,init=init)\n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, conv1, activation=activation,init=init)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, pool1, activation=activation,init=init)\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv2, activation=activation,init=init)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, pool2, activation=activation,init=init)\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv3, activation=activation,init=init)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, pool3, activation=activation,init=init)\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv4, activation=activation,init=init)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, pool4, activation=activation,init=init)\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, conv5, activation=activation,init=init)\n",
    "        \n",
    "    up6 = merge([up_conv(nfilters,8,conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, up6, activation=activation,init=init)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv6, activation=activation,init=init)\n",
    "\n",
    "    up7 = merge([up_conv(nfilters,4,conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, up7, activation=activation,init=init)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv7, activation=activation,init=init)\n",
    "\n",
    "    up8 = merge([up_conv(nfilters,2,conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, up8, activation=activation,init=init)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv8, activation=activation,init=init)\n",
    "\n",
    "    up9 = merge([up_conv(nfilters,1,conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, up9, activation=activation,init=init)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, conv9, activation=activation,init=init)\n",
    "\n",
    "    conv10 = Conv2DReluBatchNorm(1, 1, 1, conv9, activation='relu',init=init)\n",
    "    cropped = Cropping2D(cropping=((12,12), (12,12)))(conv10)\n",
    "    output = Activation(activation='sigmoid')(cropped)\n",
    "    \n",
    "    model = Model(input=inputs, output=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=lr,decay=decay), loss='binary_crossentropy', metrics=[jaccard])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = builder(img_rows=x.shape[2],img_cols=x.shape[3],\n",
    "            nfilters=8,activation='relu',init='he_normal',\n",
    "            lr=0.001,decay=0,momentum=0,reg=0.00001)\n",
    "\n",
    "#push('The model is compiled','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run number: 67...\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/1000\n",
      "120/120 [==============================] - 4s - loss: 0.7190 - jaccard: 0.1033 - val_loss: 0.7030 - val_jaccard: 0.1234\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5689 - jaccard: 0.0991 - val_loss: 0.6874 - val_jaccard: 0.1229\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5449 - jaccard: 0.0987 - val_loss: 0.6751 - val_jaccard: 0.1223\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5346 - jaccard: 0.0984 - val_loss: 0.6624 - val_jaccard: 0.1217\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5284 - jaccard: 0.0982 - val_loss: 0.6510 - val_jaccard: 0.1211\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5237 - jaccard: 0.0977 - val_loss: 0.6410 - val_jaccard: 0.1205\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5200 - jaccard: 0.0976 - val_loss: 0.6320 - val_jaccard: 0.1200\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5170 - jaccard: 0.0975 - val_loss: 0.6240 - val_jaccard: 0.1195\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5143 - jaccard: 0.0975 - val_loss: 0.6169 - val_jaccard: 0.1190\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5117 - jaccard: 0.0976 - val_loss: 0.6103 - val_jaccard: 0.1186\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5091 - jaccard: 0.0981 - val_loss: 0.6037 - val_jaccard: 0.1182\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5064 - jaccard: 0.0989 - val_loss: 0.5970 - val_jaccard: 0.1178\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 4s - loss: 0.5031 - jaccard: 0.1008 - val_loss: 0.5904 - val_jaccard: 0.1172\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4992 - jaccard: 0.1041 - val_loss: 0.5843 - val_jaccard: 0.1168\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4952 - jaccard: 0.1107 - val_loss: 0.5782 - val_jaccard: 0.1163\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4912 - jaccard: 0.1137 - val_loss: 0.5716 - val_jaccard: 0.1156\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4871 - jaccard: 0.1157 - val_loss: 0.5659 - val_jaccard: 0.1151\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4848 - jaccard: 0.1166 - val_loss: 0.5605 - val_jaccard: 0.1146\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4822 - jaccard: 0.1175 - val_loss: 0.5553 - val_jaccard: 0.1141\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4784 - jaccard: 0.1202 - val_loss: 0.5503 - val_jaccard: 0.1136\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4758 - jaccard: 0.1226 - val_loss: 0.5462 - val_jaccard: 0.1135\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4725 - jaccard: 0.1240 - val_loss: 0.5409 - val_jaccard: 0.1127\n",
      "Epoch 23/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4701 - jaccard: 0.1247 - val_loss: 0.5364 - val_jaccard: 0.1122\n",
      "Epoch 24/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4679 - jaccard: 0.1246 - val_loss: 0.5323 - val_jaccard: 0.1117\n",
      "Epoch 25/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4639 - jaccard: 0.1286 - val_loss: 0.5282 - val_jaccard: 0.1113\n",
      "Epoch 26/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4611 - jaccard: 0.1293 - val_loss: 0.5243 - val_jaccard: 0.1108\n",
      "Epoch 27/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4586 - jaccard: 0.1310 - val_loss: 0.5206 - val_jaccard: 0.1104\n",
      "Epoch 28/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4561 - jaccard: 0.1325 - val_loss: 0.5169 - val_jaccard: 0.1099\n",
      "Epoch 29/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4540 - jaccard: 0.1335 - val_loss: 0.5136 - val_jaccard: 0.1095\n",
      "Epoch 30/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4506 - jaccard: 0.1349 - val_loss: 0.5103 - val_jaccard: 0.1090\n",
      "Epoch 31/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4489 - jaccard: 0.1358 - val_loss: 0.5071 - val_jaccard: 0.1086\n",
      "Epoch 32/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4459 - jaccard: 0.1386 - val_loss: 0.5046 - val_jaccard: 0.1084\n",
      "Epoch 33/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4450 - jaccard: 0.1386 - val_loss: 0.5017 - val_jaccard: 0.1081\n",
      "Epoch 34/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4446 - jaccard: 0.1344 - val_loss: 0.4988 - val_jaccard: 0.1074\n",
      "Epoch 35/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4415 - jaccard: 0.1407 - val_loss: 0.4964 - val_jaccard: 0.1075\n",
      "Epoch 36/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4394 - jaccard: 0.1382 - val_loss: 0.4945 - val_jaccard: 0.1071\n",
      "Epoch 37/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4366 - jaccard: 0.1415 - val_loss: 0.4915 - val_jaccard: 0.1064\n",
      "Epoch 38/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4333 - jaccard: 0.1447 - val_loss: 0.4895 - val_jaccard: 0.1062\n",
      "Epoch 39/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4309 - jaccard: 0.1466 - val_loss: 0.4866 - val_jaccard: 0.1055\n",
      "Epoch 40/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4289 - jaccard: 0.1468 - val_loss: 0.4845 - val_jaccard: 0.1055\n",
      "Epoch 41/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4256 - jaccard: 0.1486 - val_loss: 0.4829 - val_jaccard: 0.1051\n",
      "Epoch 42/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4236 - jaccard: 0.1500 - val_loss: 0.4809 - val_jaccard: 0.1048\n",
      "Epoch 43/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4225 - jaccard: 0.1507 - val_loss: 0.4790 - val_jaccard: 0.1049\n",
      "Epoch 44/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4210 - jaccard: 0.1531 - val_loss: 0.4769 - val_jaccard: 0.1041\n",
      "Epoch 45/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4179 - jaccard: 0.1515 - val_loss: 0.4754 - val_jaccard: 0.1039\n",
      "Epoch 46/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4156 - jaccard: 0.1558 - val_loss: 0.4734 - val_jaccard: 0.1039\n",
      "Epoch 47/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4129 - jaccard: 0.1562 - val_loss: 0.4730 - val_jaccard: 0.1042\n",
      "Epoch 48/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4096 - jaccard: 0.1612 - val_loss: 0.4701 - val_jaccard: 0.1033\n",
      "Epoch 49/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4092 - jaccard: 0.1598 - val_loss: 0.4696 - val_jaccard: 0.1035\n",
      "Epoch 50/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4058 - jaccard: 0.1620 - val_loss: 0.4674 - val_jaccard: 0.1026\n",
      "Epoch 51/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4031 - jaccard: 0.1643 - val_loss: 0.4663 - val_jaccard: 0.1023\n",
      "Epoch 52/1000\n",
      "120/120 [==============================] - 3s - loss: 0.4037 - jaccard: 0.1628 - val_loss: 0.4662 - val_jaccard: 0.1038\n",
      "Epoch 53/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4016 - jaccard: 0.1623 - val_loss: 0.4623 - val_jaccard: 0.1012\n",
      "Epoch 54/1000\n",
      "120/120 [==============================] - 4s - loss: 0.4002 - jaccard: 0.1669 - val_loss: 0.4657 - val_jaccard: 0.1059\n",
      "Epoch 55/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3975 - jaccard: 0.1659 - val_loss: 0.4620 - val_jaccard: 0.1027\n",
      "Epoch 56/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3959 - jaccard: 0.1681 - val_loss: 0.4605 - val_jaccard: 0.1027\n",
      "Epoch 57/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3927 - jaccard: 0.1694 - val_loss: 0.4583 - val_jaccard: 0.1017\n",
      "Epoch 58/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3899 - jaccard: 0.1739 - val_loss: 0.4579 - val_jaccard: 0.1016\n",
      "Epoch 59/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3887 - jaccard: 0.1707 - val_loss: 0.4554 - val_jaccard: 0.1005\n",
      "Epoch 60/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3859 - jaccard: 0.1773 - val_loss: 0.4564 - val_jaccard: 0.1024\n",
      "Epoch 61/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3846 - jaccard: 0.1739 - val_loss: 0.4544 - val_jaccard: 0.1012\n",
      "Epoch 62/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3820 - jaccard: 0.1804 - val_loss: 0.4545 - val_jaccard: 0.1024\n",
      "Epoch 63/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3818 - jaccard: 0.1768 - val_loss: 0.4534 - val_jaccard: 0.1015\n",
      "Epoch 64/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3784 - jaccard: 0.1804 - val_loss: 0.4519 - val_jaccard: 0.1019\n",
      "Epoch 65/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3759 - jaccard: 0.1832 - val_loss: 0.4517 - val_jaccard: 0.1029\n",
      "Epoch 66/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3754 - jaccard: 0.1854 - val_loss: 0.4534 - val_jaccard: 0.1044\n",
      "Epoch 67/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3719 - jaccard: 0.1835 - val_loss: 0.4503 - val_jaccard: 0.1030\n",
      "Epoch 68/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3697 - jaccard: 0.1868 - val_loss: 0.4490 - val_jaccard: 0.1022\n",
      "Epoch 69/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3673 - jaccard: 0.1893 - val_loss: 0.4497 - val_jaccard: 0.1032\n",
      "Epoch 70/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3649 - jaccard: 0.1926 - val_loss: 0.4478 - val_jaccard: 0.1055\n",
      "Epoch 71/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3638 - jaccard: 0.1941 - val_loss: 0.4498 - val_jaccard: 0.1051\n",
      "Epoch 72/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3616 - jaccard: 0.1935 - val_loss: 0.4477 - val_jaccard: 0.1039\n",
      "Epoch 73/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3599 - jaccard: 0.1966 - val_loss: 0.4460 - val_jaccard: 0.1045\n",
      "Epoch 74/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3662 - jaccard: 0.1871 - val_loss: 0.4498 - val_jaccard: 0.1059\n",
      "Epoch 75/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3585 - jaccard: 0.1947 - val_loss: 0.4465 - val_jaccard: 0.1091\n",
      "Epoch 76/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3594 - jaccard: 0.1953 - val_loss: 0.4462 - val_jaccard: 0.1031\n",
      "Epoch 77/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3598 - jaccard: 0.1865 - val_loss: 0.4431 - val_jaccard: 0.1051\n",
      "Epoch 78/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3570 - jaccard: 0.1993 - val_loss: 0.4470 - val_jaccard: 0.1057\n",
      "Epoch 79/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3536 - jaccard: 0.1979 - val_loss: 0.4446 - val_jaccard: 0.1063\n",
      "Epoch 80/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3515 - jaccard: 0.1992 - val_loss: 0.4440 - val_jaccard: 0.1048\n",
      "Epoch 81/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3512 - jaccard: 0.1998 - val_loss: 0.4463 - val_jaccard: 0.1124\n",
      "Epoch 82/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3477 - jaccard: 0.2020 - val_loss: 0.4442 - val_jaccard: 0.1073\n",
      "Epoch 83/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3460 - jaccard: 0.2051 - val_loss: 0.4447 - val_jaccard: 0.1088\n",
      "Epoch 84/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3425 - jaccard: 0.2089 - val_loss: 0.4421 - val_jaccard: 0.1085\n",
      "Epoch 85/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3404 - jaccard: 0.2099 - val_loss: 0.4429 - val_jaccard: 0.1083\n",
      "Epoch 86/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3395 - jaccard: 0.2118 - val_loss: 0.4449 - val_jaccard: 0.1135\n",
      "Epoch 87/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3369 - jaccard: 0.2120 - val_loss: 0.4440 - val_jaccard: 0.1093\n",
      "Epoch 88/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3352 - jaccard: 0.2166 - val_loss: 0.4433 - val_jaccard: 0.1112\n",
      "Epoch 89/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3354 - jaccard: 0.2147 - val_loss: 0.4434 - val_jaccard: 0.1115\n",
      "Epoch 90/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3328 - jaccard: 0.2173 - val_loss: 0.4429 - val_jaccard: 0.1117\n",
      "Epoch 91/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3335 - jaccard: 0.2122 - val_loss: 0.4472 - val_jaccard: 0.1163\n",
      "Epoch 92/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3324 - jaccard: 0.2186 - val_loss: 0.4430 - val_jaccard: 0.1107\n",
      "Epoch 93/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3309 - jaccard: 0.2148 - val_loss: 0.4426 - val_jaccard: 0.1129\n",
      "Epoch 94/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3311 - jaccard: 0.2224 - val_loss: 0.4544 - val_jaccard: 0.1193\n",
      "Epoch 95/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3292 - jaccard: 0.2141 - val_loss: 0.4432 - val_jaccard: 0.1135\n",
      "Epoch 96/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3299 - jaccard: 0.2134 - val_loss: 0.4403 - val_jaccard: 0.1128\n",
      "Epoch 97/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3265 - jaccard: 0.2185 - val_loss: 0.4530 - val_jaccard: 0.1200\n",
      "Epoch 98/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3226 - jaccard: 0.2306 - val_loss: 0.4495 - val_jaccard: 0.1163\n",
      "Epoch 99/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3234 - jaccard: 0.2164 - val_loss: 0.4440 - val_jaccard: 0.1163\n",
      "Epoch 100/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3184 - jaccard: 0.2303 - val_loss: 0.4450 - val_jaccard: 0.1153\n",
      "Epoch 101/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3151 - jaccard: 0.2336 - val_loss: 0.4457 - val_jaccard: 0.1146\n",
      "Epoch 102/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3153 - jaccard: 0.2284 - val_loss: 0.4446 - val_jaccard: 0.1161\n",
      "Epoch 103/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3137 - jaccard: 0.2377 - val_loss: 0.4441 - val_jaccard: 0.1127\n",
      "Epoch 104/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3107 - jaccard: 0.2351 - val_loss: 0.4477 - val_jaccard: 0.1179\n",
      "Epoch 105/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3102 - jaccard: 0.2360 - val_loss: 0.4418 - val_jaccard: 0.1122\n",
      "Epoch 106/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3087 - jaccard: 0.2406 - val_loss: 0.4473 - val_jaccard: 0.1175\n",
      "Epoch 107/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3074 - jaccard: 0.2387 - val_loss: 0.4498 - val_jaccard: 0.1184\n",
      "Epoch 108/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3057 - jaccard: 0.2410 - val_loss: 0.4453 - val_jaccard: 0.1147\n",
      "Epoch 109/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3043 - jaccard: 0.2414 - val_loss: 0.4485 - val_jaccard: 0.1172\n",
      "Epoch 110/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3094 - jaccard: 0.2304 - val_loss: 0.4545 - val_jaccard: 0.1203\n",
      "Epoch 111/1000\n",
      "120/120 [==============================] - 3s - loss: 0.3043 - jaccard: 0.2484 - val_loss: 0.4633 - val_jaccard: 0.1236\n",
      "Epoch 112/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3015 - jaccard: 0.2452 - val_loss: 0.4511 - val_jaccard: 0.1185\n",
      "Epoch 113/1000\n",
      "120/120 [==============================] - 4s - loss: 0.3001 - jaccard: 0.2454 - val_loss: 0.4472 - val_jaccard: 0.1187\n",
      "Epoch 114/1000\n",
      "120/120 [==============================] - 4s - loss: 0.2979 - jaccard: 0.2501 - val_loss: 0.4428 - val_jaccard: 0.1148\n",
      "Epoch 115/1000\n",
      "120/120 [==============================] - 4s - loss: 0.2953 - jaccard: 0.2485 - val_loss: 0.4484 - val_jaccard: 0.1181\n",
      "Epoch 116/1000\n",
      "120/120 [==============================] - 3s - loss: 0.2997 - jaccard: 0.2487 - val_loss: 0.4534 - val_jaccard: 0.1198\n",
      "Epoch 117/1000\n",
      "120/120 [==============================] - 4s - loss: 0.2943 - jaccard: 0.2509 - val_loss: 0.4528 - val_jaccard: 0.1198\n",
      "Epoch 118/1000\n",
      "120/120 [==============================] - 4s - loss: 0.2932 - jaccard: 0.2505 - val_loss: 0.4467 - val_jaccard: 0.1172\n",
      "Epoch 119/1000\n",
      "120/120 [==============================] - 4s - loss: 0.2923 - jaccard: 0.2568 - val_loss: 0.4544 - val_jaccard: 0.1215\n",
      "Epoch 120/1000\n",
      "120/120 [==============================] - 4s - loss: 0.2912 - jaccard: 0.2531 - val_loss: 0.4509 - val_jaccard: 0.1187\n",
      "Epoch 121/1000\n",
      "120/120 [==============================] - 3s - loss: 0.2939 - jaccard: 0.2536 - val_loss: 0.4489 - val_jaccard: 0.1181\n",
      "Epoch 122/1000\n",
      " 90/120 [=====================>........] - ETA: 0s - loss: 0.2897 - jaccard: 0.2599"
     ]
    }
   ],
   "source": [
    "run += 1\n",
    "def train_and_predict(model,fit=True,use_existing=False):\n",
    "    print('This is run number: '+ str(run) + '...')\n",
    "    \n",
    "    if use_existing:\n",
    "        model.load_weights('./data/model_weights_class_3_run_'+str(run)+'.hdf5')\n",
    "        \n",
    "    if fit:\n",
    "        \n",
    "        print('Fitting model...')\n",
    "        print('-'*30)\n",
    "        \n",
    "        quitter = EarlyStopping(monitor='loss', min_delta=0.001, patience=100, verbose=1, mode='auto')\n",
    "        lrreducer = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, verbose=1, mode='auto', epsilon=0.001, cooldown=2, min_lr=0)\n",
    "        model_checkpoint = ModelCheckpoint('./data/model_weights_class_3_run_'+str(run)+'.hdf5', monitor='loss', save_best_only=True)\n",
    "        csvlogger = CSVLogger('./data/training_log_run_'+str(run), separator=',', append=False)\n",
    "\n",
    "        #tensorboard = TensorBoard(log_dir='./logs/'+'run_'+str(run), histogram_freq=0, write_graph=True, write_images=False)\n",
    "        '''\n",
    "        screen -S tensorboard\n",
    "        tensorboard --logdir=logs\n",
    "        <ctrl + a,d to exit>\n",
    "        screen -r tensorboard\n",
    "        '''\n",
    "        \n",
    "        model.fit(x, y_oneclass,\n",
    "                  batch_size=30,\n",
    "                  nb_epoch=1000,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  callbacks=[model_checkpoint,csvlogger],\n",
    "                  validation_split=0.2,\n",
    "                  initial_epoch=0)\n",
    "    \n",
    "        print('Predicting masks on test data...')\n",
    "        print('-'*30)\n",
    "        \n",
    "    imgs_mask_test = model.predict(x, verbose=1)\n",
    "    np.save('jaccard_preds_all_data.npy', imgs_mask_test)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = train_and_predict(model,fit=True,use_existing=False)\n",
    "push('Training is done',\n",
    "     'Train loss: %f, train jaccard: %f, val loss %f, val jaccard%f' %(model.history.history['loss'][-1],model.history.history['jaccard'][-1],model.history.history['val_loss'][-1],model.history.history['val_jaccard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.load('jaccard_preds_all_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(x[120:150,...],y_oneclass[120:150,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm = False\n",
    "if norm:\n",
    "    preds = norm_preds(preds)\n",
    "i = np.random.choice(range(120),1)[0]\n",
    "print(i)\n",
    "plot_all(i,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_everything():\n",
    "    fig = plt.figure(figsize=(12,36))\n",
    "    #num = np.random.randint(10,25,10)\n",
    "    num = np.arange(0,10)\n",
    "    for i in range(1,10):\n",
    "        ax1 = fig.add_subplot(10,8,8*i-7)\n",
    "        ax1.imshow(preds[num[i],0,...],cmap='spectral')\n",
    "        \n",
    "        ax2 = fig.add_subplot(10,8,8*i-6)\n",
    "        ax2.imshow(y_oneclass[num[i],0,...],cmap='spectral')\n",
    "        \n",
    "        ax3 = fig.add_subplot(10,8,8*i-5)\n",
    "        ax3.imshow(preds[num[i],1,...],cmap='spectral')\n",
    "        \n",
    "        ax4 = fig.add_subplot(10,8,8*i-4)\n",
    "        ax4.imshow(y_oneclass[num[i],1,...],cmap='spectral')\n",
    "        \n",
    "        ax5 = fig.add_subplot(10,8,8*i-3)\n",
    "        ax5.imshow(preds[num[i],2,...],cmap='spectral')\n",
    "        \n",
    "        ax6 = fig.add_subplot(10,8,8*i-2)\n",
    "        ax6.imshow(y_oneclass[num[i],2,...],cmap='spectral')\n",
    "        \n",
    "        ax7 = fig.add_subplot(10,8,8*i-1)\n",
    "        ax7.imshow(preds[num[i],3,...],cmap='spectral')\n",
    "        \n",
    "        ax8 = fig.add_subplot(10,8,8*i)\n",
    "        ax8.imshow(y_oneclass[num[i],3,...],cmap='spectral')\n",
    "        \n",
    "    plt.show()\n",
    "plot_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_classifier(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defunct bits and pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_cancelled(model):\n",
    "    model.load_weights('./data/model_weights_class_3_run_66.hdf5')\n",
    "    imgs_mask_test = model.predict(x, verbose=1)\n",
    "    return imgs_mask_test\n",
    "\n",
    "preds = load_cancelled(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_classifier(model):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(model.history.history['loss'], \"-\",color='blue',label=\"Training final loss: \"+str(round(model.history.history['loss'][-1],4)))\n",
    "    ax.plot(model.history.history['jaccard'], \"-\",color='orange',label=\"testing final loss: \"+str(round(model.history.history['jaccard'][-1],4)))\n",
    "    #ax.set_xlim([0, epochs])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Jaccard / Loss')\n",
    "    plt.legend(loc='best')\n",
    "    ax.set_title('Metrics vs time (epochs)')\n",
    "    \n",
    "    '''ax = fig.add_subplot(122)\n",
    "    ax.plot(model.history['acc'], \"-\",color='blue',label=\"Training final acc: \"+str(round(model.history['acc'][-1],4)))\n",
    "    ax.plot(model.history['val_acc'], \"-\",color='orange',label=\"testing final acc: \"+str(round(model.history['val_acc'][-1],4)))\n",
    "    #ax.set_xlim([0, epochs])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    ax.set_title('Accuracy vs time (epochs)')    \n",
    "    '''\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the training data\n",
    "def import_data(load_whitened = False, normalize = True):\n",
    "    if load_whitened:\n",
    "        with open('./data/x_whitened_resized_array.pickle','rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            x = x.astype('float32')\n",
    "    else:\n",
    "        with open('./data/x_resized_array.pickle','rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            x = x.astype('float32')\n",
    "            # Normalize data to max values\n",
    "            for i in range(x.shape[0]):\n",
    "                for j in range(x.shape[1]):\n",
    "                    x[i,j,:,:] *= 1/x[i,j,:,:].max()\n",
    "                    \n",
    "    # Normalize values between 1 and 0\n",
    "    if normalize:\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                #x[i,j,:,:] /= x[i,j,:,:].max()\n",
    "                x[i,j,:,:] = (x[i,j,:,:].min() - x[i,j,:,:])/(x[i,j,:,:].min() - x[i,j,:,:].max())\n",
    "\n",
    "    with open('./data/y_resized_raster.pickle','rb') as f:\n",
    "        y = pickle.load(f)\n",
    "        y = y.astype(np.float32)\n",
    "        \n",
    "    y_oneclass = y[:,1:6,...]\n",
    "    \n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''\n",
    "    \n",
    "    return x, y, y_oneclass\n",
    "    \n",
    "    # Just use a single class: roads\n",
    "    #y = y[:,4,:,:]\n",
    "    #y = y[:,np.newaxis,:,:]\n",
    "\n",
    "    # y = y.reshape(y.shape[0],-1)\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def np_jaccard(y_true,y_pred,smooth=1.):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true * np.rint(y_pred))\n",
    "    return (intersection+smooth) / (np.sum(y_true) + np.sum(y_pred) - intersection+smooth)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
