{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U-Net architecture\n",
    "\n",
    "See [here](https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py#L19) for code and [here](https://arxiv.org/pdf/1505.04597.pdf) for the original literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restart_kernel(restart=False):\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    if restart:\n",
    "        app.kernel.do_shutdown(True)\n",
    "\n",
    "restart_kernel(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Activation, Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "# \"tf\" assumes (rows, cols, channels) while \"th\" assumes (channels, rows, cols)\n",
    "# Possibly change this around natively in the data so the backend doesn't have to switch them\n",
    "# Only necessary if I use TF!\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pushbullet import Pushbullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run # 88\n"
     ]
    }
   ],
   "source": [
    "# Pushbullet notifier\n",
    "def push(title='Done!',text=''):\n",
    "    Pushbullet('o.YFPNNPfGRekivaCGHa4qMSgjZt8zJ6FL').devices[0].push_note(title,text)\n",
    "    \n",
    "# Import the training data\n",
    "def import_data():\n",
    "    x = np.load('./data/x_augmented.npy','r')\n",
    "    y = np.load('./data/y_augmented.npy','r')\n",
    "    y_oneclass = y[:,3:4,...]\n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''    \n",
    "    return x, y, y_oneclass\n",
    "x, y, y_oneclass = import_data()\n",
    "\n",
    "# Increment the counter\n",
    "def counter():\n",
    "    run = np.load('./data/run_counter.npy')\n",
    "    run += 1\n",
    "    np.save('./data/run_counter.npy',run)\n",
    "    return run\n",
    "run = counter()\n",
    "\n",
    "# Set the counter to a specific value\n",
    "def set_counter(run):\n",
    "    run = run\n",
    "    np.save('./data/run_counter.npy',run)\n",
    "    return run\n",
    "# Uncomment the next line to manually set the counter if something goes wrong\n",
    "#run = set_counter(87)\n",
    "print('This is run # %i' %run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 34.3 ms, total: 1.76 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def compiler(img_rows = x.shape[2],img_cols = x.shape[3],\n",
    "            nfilters = 32,activation = 'relu',init = 'he_normal',\n",
    "            lr=1.0,decay=0.0,momentum=0.0, nesterov=False,reg=0.01,p=[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2]):\n",
    "    \n",
    "    def jaccard(y_true, y_pred,smooth=1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "    \n",
    "    def Conv2DReluBatchNorm(n_filter, w_filter, h_filter, inputs, activation, init='he_uniform',dropout=0.2):\n",
    "        # Batch norm after activation / leakyrelu\n",
    "        #return BatchNormalization(mode=2, axis=1)(LeakyReLU()((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "        # Batch norm before activation\n",
    "        #return LeakyReLU()(BatchNormalization(mode=0, axis=1)((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "        # Batch norm after activation / relu\n",
    "        return BatchNormalization(mode=2, axis=1)(Activation(activation=activation)((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "    def up_conv(nfilters,filter_factor,inputs,init=init,activation=activation):\n",
    "        # No batch norm\n",
    "        #return LeakyReLU()(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs)))\n",
    "        \n",
    "        # Batch norm after activation\n",
    "        #return BatchNormalization(mode=2, axis=1)(LeakyReLU()(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs))))\n",
    "        \n",
    "        # Batch norm after activation / relu\n",
    "        return BatchNormalization(mode=2, axis=1)(Activation(activation=activation)(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_regularizer=l2(reg),W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs))))\n",
    "\n",
    "    inputs = Input((20, img_rows, img_cols))\n",
    "    padded = ZeroPadding2D(padding=(12,12))(inputs)\n",
    "    \n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, padded, activation=activation,init=init)\n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, conv1, activation=activation,init=init)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(p=p[0])(pool1)\n",
    "\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, pool1, activation=activation,init=init)\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv2, activation=activation,init=init)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(p=p[1])(pool2)\n",
    "\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, pool2, activation=activation,init=init)\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv3, activation=activation,init=init)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(p=p[2])(pool3)\n",
    "\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, pool3, activation=activation,init=init)\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv4, activation=activation,init=init)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = Dropout(p=p[3])(pool4)\n",
    "\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, pool4, activation=activation,init=init)\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, conv5, activation=activation,init=init)\n",
    "    conv5 = Dropout(p=p[4])(conv5)\n",
    "        \n",
    "    up6 = merge([up_conv(nfilters,8,conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, up6, activation=activation,init=init)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv6, activation=activation,init=init)\n",
    "    conv6 = Dropout(p=p[5])(conv6)\n",
    "\n",
    "    up7 = merge([up_conv(nfilters,4,conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, up7, activation=activation,init=init)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv7, activation=activation,init=init)\n",
    "    conv7 = Dropout(p=p[6])(conv7)\n",
    "\n",
    "    up8 = merge([up_conv(nfilters,2,conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, up8, activation=activation,init=init)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv8, activation=activation,init=init)\n",
    "    conv8 = Dropout(p=p[7])(conv8)\n",
    "\n",
    "    up9 = merge([up_conv(nfilters,1,conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, up9, activation=activation,init=init)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, conv9, activation=activation,init=init)\n",
    "    conv9 = Dropout(p=p[8])(conv9)\n",
    "    \n",
    "    conv10 = Conv2DReluBatchNorm(1, 1, 1, conv9, activation='relu',init=init)\n",
    "    cropped = Cropping2D(cropping=((12,12), (12,12)))(conv10)\n",
    "    output = Activation(activation='sigmoid')(cropped)\n",
    "    \n",
    "    model = Model(input=inputs, output=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=lr,decay=decay), loss='binary_crossentropy', metrics=[jaccard])\n",
    "    \n",
    "    return model\n",
    "\n",
    "p=[0.1,0.2,0.3,0.4,0.5,0.4,0.3,0.2,0.1] # current version\n",
    "#p=[0.2,0.3,0.4,0.5,0.5,0.5,0.4,0.3,0.2] # symmetric but more dropout\n",
    "#p=[0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6] # increasing\n",
    "\n",
    "model = compiler(img_rows=x.shape[2],img_cols=x.shape[3],\n",
    "            nfilters=16,activation='relu',init='he_normal',\n",
    "            lr=0.001,decay=0,momentum=0,reg=0,p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run # 88\n",
      "Train on 320 samples, validate on 80 samples\n",
      "Epoch 1/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5887 - jaccard: 0.0938 - val_loss: 0.5065 - val_jaccard: 0.0923\n",
      "Epoch 2/1000\n",
      "320/320 [==============================] - 20s - loss: 0.5025 - jaccard: 0.0908 - val_loss: 0.4965 - val_jaccard: 0.0920\n",
      "Epoch 3/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4927 - jaccard: 0.0924 - val_loss: 0.4870 - val_jaccard: 0.0950\n",
      "Epoch 4/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4839 - jaccard: 0.0944 - val_loss: 0.4780 - val_jaccard: 0.0964\n",
      "Epoch 5/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4754 - jaccard: 0.0962 - val_loss: 0.4688 - val_jaccard: 0.0996\n",
      "Epoch 6/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4655 - jaccard: 0.1011 - val_loss: 0.4585 - val_jaccard: 0.1073\n",
      "Epoch 7/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4564 - jaccard: 0.1053 - val_loss: 0.4513 - val_jaccard: 0.1065\n",
      "Epoch 8/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4492 - jaccard: 0.1080 - val_loss: 0.4436 - val_jaccard: 0.1106\n",
      "Epoch 9/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4427 - jaccard: 0.1096 - val_loss: 0.4371 - val_jaccard: 0.1142\n",
      "Epoch 10/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4360 - jaccard: 0.1115 - val_loss: 0.4319 - val_jaccard: 0.1128\n",
      "Epoch 11/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4313 - jaccard: 0.1115 - val_loss: 0.4259 - val_jaccard: 0.1134\n",
      "Epoch 12/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4251 - jaccard: 0.1145 - val_loss: 0.4225 - val_jaccard: 0.1129\n",
      "Epoch 13/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4191 - jaccard: 0.1162 - val_loss: 0.4149 - val_jaccard: 0.1166\n",
      "Epoch 14/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4134 - jaccard: 0.1177 - val_loss: 0.4098 - val_jaccard: 0.1201\n",
      "Epoch 15/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4086 - jaccard: 0.1174 - val_loss: 0.4052 - val_jaccard: 0.1205\n",
      "Epoch 16/1000\n",
      "320/320 [==============================] - 20s - loss: 0.4032 - jaccard: 0.1212 - val_loss: 0.4013 - val_jaccard: 0.1177\n",
      "Epoch 17/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3976 - jaccard: 0.1233 - val_loss: 0.3956 - val_jaccard: 0.1218\n",
      "Epoch 18/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3940 - jaccard: 0.1227 - val_loss: 0.3918 - val_jaccard: 0.1212\n",
      "Epoch 19/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3879 - jaccard: 0.1258 - val_loss: 0.3872 - val_jaccard: 0.1272\n",
      "Epoch 20/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3837 - jaccard: 0.1262 - val_loss: 0.3822 - val_jaccard: 0.1299\n",
      "Epoch 21/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3790 - jaccard: 0.1301 - val_loss: 0.3780 - val_jaccard: 0.1286\n",
      "Epoch 22/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3770 - jaccard: 0.1277 - val_loss: 0.3761 - val_jaccard: 0.1305\n",
      "Epoch 23/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3717 - jaccard: 0.1295 - val_loss: 0.3712 - val_jaccard: 0.1327\n",
      "Epoch 24/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3682 - jaccard: 0.1315 - val_loss: 0.3671 - val_jaccard: 0.1314\n",
      "Epoch 25/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3659 - jaccard: 0.1313 - val_loss: 0.3659 - val_jaccard: 0.1311\n",
      "Epoch 26/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3633 - jaccard: 0.1273 - val_loss: 0.3618 - val_jaccard: 0.1389\n",
      "Epoch 27/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3572 - jaccard: 0.1356 - val_loss: 0.3566 - val_jaccard: 0.1339\n",
      "Epoch 28/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3530 - jaccard: 0.1380 - val_loss: 0.3531 - val_jaccard: 0.1340\n",
      "Epoch 29/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3495 - jaccard: 0.1393 - val_loss: 0.3499 - val_jaccard: 0.1348\n",
      "Epoch 30/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3470 - jaccard: 0.1386 - val_loss: 0.3465 - val_jaccard: 0.1368\n",
      "Epoch 31/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3417 - jaccard: 0.1442 - val_loss: 0.3432 - val_jaccard: 0.1406\n",
      "Epoch 32/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3401 - jaccard: 0.1400 - val_loss: 0.3407 - val_jaccard: 0.1473\n",
      "Epoch 33/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3376 - jaccard: 0.1445 - val_loss: 0.3394 - val_jaccard: 0.1390\n",
      "Epoch 34/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3352 - jaccard: 0.1431 - val_loss: 0.3359 - val_jaccard: 0.1352\n",
      "Epoch 35/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3309 - jaccard: 0.1472 - val_loss: 0.3325 - val_jaccard: 0.1409\n",
      "Epoch 36/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3293 - jaccard: 0.1452 - val_loss: 0.3302 - val_jaccard: 0.1428\n",
      "Epoch 37/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3263 - jaccard: 0.1479 - val_loss: 0.3257 - val_jaccard: 0.1511\n",
      "Epoch 38/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3237 - jaccard: 0.1486 - val_loss: 0.3238 - val_jaccard: 0.1511\n",
      "Epoch 39/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3206 - jaccard: 0.1514 - val_loss: 0.3215 - val_jaccard: 0.1534\n",
      "Epoch 40/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3172 - jaccard: 0.1539 - val_loss: 0.3187 - val_jaccard: 0.1539\n",
      "Epoch 41/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3160 - jaccard: 0.1535 - val_loss: 0.3175 - val_jaccard: 0.1471\n",
      "Epoch 42/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3130 - jaccard: 0.1541 - val_loss: 0.3150 - val_jaccard: 0.1512\n",
      "Epoch 43/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3112 - jaccard: 0.1565 - val_loss: 0.3133 - val_jaccard: 0.1523\n",
      "Epoch 44/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3082 - jaccard: 0.1592 - val_loss: 0.3113 - val_jaccard: 0.1512\n",
      "Epoch 45/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3075 - jaccard: 0.1563 - val_loss: 0.3106 - val_jaccard: 0.1558\n",
      "Epoch 46/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3042 - jaccard: 0.1611 - val_loss: 0.3076 - val_jaccard: 0.1507\n",
      "Epoch 47/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3028 - jaccard: 0.1603 - val_loss: 0.3064 - val_jaccard: 0.1509\n",
      "Epoch 48/1000\n",
      "320/320 [==============================] - 20s - loss: 0.3002 - jaccard: 0.1591 - val_loss: 0.3035 - val_jaccard: 0.1679\n",
      "Epoch 49/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2972 - jaccard: 0.1664 - val_loss: 0.3001 - val_jaccard: 0.1626\n",
      "Epoch 50/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2965 - jaccard: 0.1620 - val_loss: 0.2978 - val_jaccard: 0.1693\n",
      "Epoch 51/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2909 - jaccard: 0.1726 - val_loss: 0.2956 - val_jaccard: 0.1659\n",
      "Epoch 52/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2906 - jaccard: 0.1707 - val_loss: 0.2953 - val_jaccard: 0.1624\n",
      "Epoch 53/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2891 - jaccard: 0.1707 - val_loss: 0.2931 - val_jaccard: 0.1632\n",
      "Epoch 54/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2875 - jaccard: 0.1697 - val_loss: 0.2917 - val_jaccard: 0.1774\n",
      "Epoch 55/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2833 - jaccard: 0.1778 - val_loss: 0.2892 - val_jaccard: 0.1745\n",
      "Epoch 56/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2820 - jaccard: 0.1779 - val_loss: 0.2872 - val_jaccard: 0.1727\n",
      "Epoch 57/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2799 - jaccard: 0.1794 - val_loss: 0.2858 - val_jaccard: 0.1788\n",
      "Epoch 58/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2802 - jaccard: 0.1780 - val_loss: 0.2852 - val_jaccard: 0.1730\n",
      "Epoch 59/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2769 - jaccard: 0.1824 - val_loss: 0.2836 - val_jaccard: 0.1696\n",
      "Epoch 60/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2747 - jaccard: 0.1827 - val_loss: 0.2817 - val_jaccard: 0.1805\n",
      "Epoch 61/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2742 - jaccard: 0.1823 - val_loss: 0.2802 - val_jaccard: 0.1777\n",
      "Epoch 62/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2723 - jaccard: 0.1864 - val_loss: 0.2792 - val_jaccard: 0.1740\n",
      "Epoch 63/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2711 - jaccard: 0.1841 - val_loss: 0.2769 - val_jaccard: 0.1800\n",
      "Epoch 64/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2699 - jaccard: 0.1880 - val_loss: 0.2772 - val_jaccard: 0.1786\n",
      "Epoch 65/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2686 - jaccard: 0.1861 - val_loss: 0.2757 - val_jaccard: 0.1891\n",
      "Epoch 66/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2669 - jaccard: 0.1890 - val_loss: 0.2740 - val_jaccard: 0.1878\n",
      "Epoch 67/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2660 - jaccard: 0.1881 - val_loss: 0.2741 - val_jaccard: 0.1781\n",
      "Epoch 68/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2638 - jaccard: 0.1933 - val_loss: 0.2715 - val_jaccard: 0.1874\n",
      "Epoch 69/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2636 - jaccard: 0.1915 - val_loss: 0.2706 - val_jaccard: 0.1909\n",
      "Epoch 70/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2625 - jaccard: 0.1909 - val_loss: 0.2703 - val_jaccard: 0.1923\n",
      "Epoch 71/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2609 - jaccard: 0.1937 - val_loss: 0.2671 - val_jaccard: 0.1877\n",
      "Epoch 72/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2584 - jaccard: 0.1975 - val_loss: 0.2659 - val_jaccard: 0.1937\n",
      "Epoch 73/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2570 - jaccard: 0.1984 - val_loss: 0.2650 - val_jaccard: 0.1945\n",
      "Epoch 74/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2544 - jaccard: 0.2023 - val_loss: 0.2642 - val_jaccard: 0.1939\n",
      "Epoch 75/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2533 - jaccard: 0.2025 - val_loss: 0.2660 - val_jaccard: 0.2100\n",
      "Epoch 76/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2530 - jaccard: 0.2060 - val_loss: 0.2624 - val_jaccard: 0.1943\n",
      "Epoch 77/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2519 - jaccard: 0.2032 - val_loss: 0.2622 - val_jaccard: 0.2018\n",
      "Epoch 78/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2531 - jaccard: 0.2020 - val_loss: 0.2621 - val_jaccard: 0.1828\n",
      "Epoch 79/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2513 - jaccard: 0.2015 - val_loss: 0.2597 - val_jaccard: 0.1916\n",
      "Epoch 80/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2482 - jaccard: 0.2096 - val_loss: 0.2582 - val_jaccard: 0.2002\n",
      "Epoch 81/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2458 - jaccard: 0.2100 - val_loss: 0.2563 - val_jaccard: 0.2078\n",
      "Epoch 82/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2453 - jaccard: 0.2139 - val_loss: 0.2579 - val_jaccard: 0.1898\n",
      "Epoch 83/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2460 - jaccard: 0.2085 - val_loss: 0.2564 - val_jaccard: 0.1949\n",
      "Epoch 84/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2443 - jaccard: 0.2112 - val_loss: 0.2530 - val_jaccard: 0.2051\n",
      "Epoch 85/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2419 - jaccard: 0.2155 - val_loss: 0.2541 - val_jaccard: 0.2074\n",
      "Epoch 86/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2417 - jaccard: 0.2166 - val_loss: 0.2524 - val_jaccard: 0.2113\n",
      "Epoch 87/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2394 - jaccard: 0.2178 - val_loss: 0.2510 - val_jaccard: 0.2070\n",
      "Epoch 88/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2389 - jaccard: 0.2195 - val_loss: 0.2507 - val_jaccard: 0.2083\n",
      "Epoch 89/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2383 - jaccard: 0.2196 - val_loss: 0.2493 - val_jaccard: 0.2066\n",
      "Epoch 90/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2371 - jaccard: 0.2202 - val_loss: 0.2499 - val_jaccard: 0.2051\n",
      "Epoch 91/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2369 - jaccard: 0.2187 - val_loss: 0.2491 - val_jaccard: 0.2131\n",
      "Epoch 92/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2352 - jaccard: 0.2238 - val_loss: 0.2482 - val_jaccard: 0.2105\n",
      "Epoch 93/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2361 - jaccard: 0.2225 - val_loss: 0.2481 - val_jaccard: 0.2155\n",
      "Epoch 94/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2342 - jaccard: 0.2253 - val_loss: 0.2468 - val_jaccard: 0.2211\n",
      "Epoch 95/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2330 - jaccard: 0.2257 - val_loss: 0.2452 - val_jaccard: 0.2107\n",
      "Epoch 96/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2309 - jaccard: 0.2299 - val_loss: 0.2441 - val_jaccard: 0.2146\n",
      "Epoch 97/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2315 - jaccard: 0.2260 - val_loss: 0.2427 - val_jaccard: 0.2182\n",
      "Epoch 98/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2287 - jaccard: 0.2329 - val_loss: 0.2430 - val_jaccard: 0.2237\n",
      "Epoch 99/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2296 - jaccard: 0.2283 - val_loss: 0.2443 - val_jaccard: 0.2066\n",
      "Epoch 100/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2289 - jaccard: 0.2312 - val_loss: 0.2430 - val_jaccard: 0.2166\n",
      "Epoch 101/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2270 - jaccard: 0.2341 - val_loss: 0.2416 - val_jaccard: 0.2312\n",
      "Epoch 102/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2254 - jaccard: 0.2370 - val_loss: 0.2400 - val_jaccard: 0.2245\n",
      "Epoch 103/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2240 - jaccard: 0.2386 - val_loss: 0.2400 - val_jaccard: 0.2262\n",
      "Epoch 104/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2246 - jaccard: 0.2397 - val_loss: 0.2395 - val_jaccard: 0.2266\n",
      "Epoch 105/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2244 - jaccard: 0.2351 - val_loss: 0.2405 - val_jaccard: 0.2132\n",
      "Epoch 106/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2235 - jaccard: 0.2387 - val_loss: 0.2386 - val_jaccard: 0.2224\n",
      "Epoch 107/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2232 - jaccard: 0.2367 - val_loss: 0.2382 - val_jaccard: 0.2239\n",
      "Epoch 108/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2216 - jaccard: 0.2426 - val_loss: 0.2403 - val_jaccard: 0.2421\n",
      "Epoch 109/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2237 - jaccard: 0.2379 - val_loss: 0.2390 - val_jaccard: 0.2236\n",
      "Epoch 110/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2196 - jaccard: 0.2448 - val_loss: 0.2365 - val_jaccard: 0.2249\n",
      "Epoch 111/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2165 - jaccard: 0.2493 - val_loss: 0.2360 - val_jaccard: 0.2405\n",
      "Epoch 112/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2172 - jaccard: 0.2502 - val_loss: 0.2343 - val_jaccard: 0.2305\n",
      "Epoch 113/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2157 - jaccard: 0.2493 - val_loss: 0.2340 - val_jaccard: 0.2293\n",
      "Epoch 114/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2161 - jaccard: 0.2485 - val_loss: 0.2356 - val_jaccard: 0.2460\n",
      "Epoch 115/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2155 - jaccard: 0.2490 - val_loss: 0.2351 - val_jaccard: 0.2465\n",
      "Epoch 116/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2154 - jaccard: 0.2536 - val_loss: 0.2351 - val_jaccard: 0.2233\n",
      "Epoch 117/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2142 - jaccard: 0.2543 - val_loss: 0.2312 - val_jaccard: 0.2424\n",
      "Epoch 118/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2127 - jaccard: 0.2542 - val_loss: 0.2316 - val_jaccard: 0.2296\n",
      "Epoch 119/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2122 - jaccard: 0.2546 - val_loss: 0.2316 - val_jaccard: 0.2293\n",
      "Epoch 120/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2097 - jaccard: 0.2604 - val_loss: 0.2306 - val_jaccard: 0.2425\n",
      "Epoch 121/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2095 - jaccard: 0.2598 - val_loss: 0.2308 - val_jaccard: 0.2369\n",
      "Epoch 122/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2087 - jaccard: 0.2633 - val_loss: 0.2287 - val_jaccard: 0.2481\n",
      "Epoch 123/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2089 - jaccard: 0.2611 - val_loss: 0.2296 - val_jaccard: 0.2538\n",
      "Epoch 124/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2081 - jaccard: 0.2646 - val_loss: 0.2318 - val_jaccard: 0.2273\n",
      "Epoch 125/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2070 - jaccard: 0.2635 - val_loss: 0.2297 - val_jaccard: 0.2433\n",
      "Epoch 126/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2078 - jaccard: 0.2639 - val_loss: 0.2275 - val_jaccard: 0.2430\n",
      "Epoch 127/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2048 - jaccard: 0.2686 - val_loss: 0.2272 - val_jaccard: 0.2520\n",
      "Epoch 128/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2045 - jaccard: 0.2693 - val_loss: 0.2269 - val_jaccard: 0.2501\n",
      "Epoch 129/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2051 - jaccard: 0.2660 - val_loss: 0.2268 - val_jaccard: 0.2405\n",
      "Epoch 130/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2030 - jaccard: 0.2731 - val_loss: 0.2263 - val_jaccard: 0.2489\n",
      "Epoch 131/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2028 - jaccard: 0.2717 - val_loss: 0.2257 - val_jaccard: 0.2549\n",
      "Epoch 132/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2029 - jaccard: 0.2715 - val_loss: 0.2253 - val_jaccard: 0.2554\n",
      "Epoch 133/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2032 - jaccard: 0.2730 - val_loss: 0.2250 - val_jaccard: 0.2399\n",
      "Epoch 134/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2022 - jaccard: 0.2731 - val_loss: 0.2250 - val_jaccard: 0.2591\n",
      "Epoch 135/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2002 - jaccard: 0.2764 - val_loss: 0.2259 - val_jaccard: 0.2572\n",
      "Epoch 136/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2003 - jaccard: 0.2770 - val_loss: 0.2248 - val_jaccard: 0.2470\n",
      "Epoch 137/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2017 - jaccard: 0.2691 - val_loss: 0.2271 - val_jaccard: 0.2417\n",
      "Epoch 138/1000\n",
      "320/320 [==============================] - 20s - loss: 0.2022 - jaccard: 0.2741 - val_loss: 0.2244 - val_jaccard: 0.2653\n",
      "Epoch 139/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1998 - jaccard: 0.2756 - val_loss: 0.2240 - val_jaccard: 0.2631\n",
      "Epoch 140/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1973 - jaccard: 0.2834 - val_loss: 0.2227 - val_jaccard: 0.2640\n",
      "Epoch 141/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1967 - jaccard: 0.2839 - val_loss: 0.2228 - val_jaccard: 0.2459\n",
      "Epoch 142/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1962 - jaccard: 0.2832 - val_loss: 0.2208 - val_jaccard: 0.2550\n",
      "Epoch 143/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1944 - jaccard: 0.2870 - val_loss: 0.2200 - val_jaccard: 0.2660\n",
      "Epoch 144/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1963 - jaccard: 0.2806 - val_loss: 0.2221 - val_jaccard: 0.2524\n",
      "Epoch 145/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1966 - jaccard: 0.2839 - val_loss: 0.2219 - val_jaccard: 0.2589\n",
      "Epoch 146/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1946 - jaccard: 0.2881 - val_loss: 0.2203 - val_jaccard: 0.2563\n",
      "Epoch 147/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1934 - jaccard: 0.2897 - val_loss: 0.2205 - val_jaccard: 0.2534\n",
      "Epoch 148/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1935 - jaccard: 0.2871 - val_loss: 0.2193 - val_jaccard: 0.2686\n",
      "Epoch 149/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1923 - jaccard: 0.2914 - val_loss: 0.2173 - val_jaccard: 0.2600\n",
      "Epoch 150/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1923 - jaccard: 0.2890 - val_loss: 0.2189 - val_jaccard: 0.2666\n",
      "Epoch 151/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1918 - jaccard: 0.2919 - val_loss: 0.2182 - val_jaccard: 0.2655\n",
      "Epoch 152/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1900 - jaccard: 0.2944 - val_loss: 0.2191 - val_jaccard: 0.2767\n",
      "Epoch 153/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1906 - jaccard: 0.2952 - val_loss: 0.2201 - val_jaccard: 0.2562\n",
      "Epoch 154/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1887 - jaccard: 0.2975 - val_loss: 0.2175 - val_jaccard: 0.2692\n",
      "Epoch 155/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1879 - jaccard: 0.3005 - val_loss: 0.2163 - val_jaccard: 0.2635\n",
      "Epoch 156/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1884 - jaccard: 0.2971 - val_loss: 0.2167 - val_jaccard: 0.2607\n",
      "Epoch 157/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1872 - jaccard: 0.2999 - val_loss: 0.2177 - val_jaccard: 0.2625\n",
      "Epoch 158/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1871 - jaccard: 0.2991 - val_loss: 0.2161 - val_jaccard: 0.2670\n",
      "Epoch 159/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1882 - jaccard: 0.3012 - val_loss: 0.2186 - val_jaccard: 0.2617\n",
      "Epoch 160/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1866 - jaccard: 0.3024 - val_loss: 0.2167 - val_jaccard: 0.2766\n",
      "Epoch 161/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1853 - jaccard: 0.3058 - val_loss: 0.2143 - val_jaccard: 0.2768\n",
      "Epoch 162/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1849 - jaccard: 0.3047 - val_loss: 0.2141 - val_jaccard: 0.2665\n",
      "Epoch 163/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1834 - jaccard: 0.3092 - val_loss: 0.2146 - val_jaccard: 0.2746\n",
      "Epoch 164/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1843 - jaccard: 0.3084 - val_loss: 0.2172 - val_jaccard: 0.2818\n",
      "Epoch 165/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1848 - jaccard: 0.3046 - val_loss: 0.2150 - val_jaccard: 0.2771\n",
      "Epoch 166/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1869 - jaccard: 0.3011 - val_loss: 0.2159 - val_jaccard: 0.2768\n",
      "Epoch 167/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1850 - jaccard: 0.3070 - val_loss: 0.2155 - val_jaccard: 0.2685\n",
      "Epoch 168/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1841 - jaccard: 0.3045 - val_loss: 0.2141 - val_jaccard: 0.2800\n",
      "Epoch 169/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1815 - jaccard: 0.3133 - val_loss: 0.2143 - val_jaccard: 0.2673\n",
      "Epoch 170/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1826 - jaccard: 0.3105 - val_loss: 0.2149 - val_jaccard: 0.2810\n",
      "Epoch 171/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1829 - jaccard: 0.3112 - val_loss: 0.2141 - val_jaccard: 0.2665\n",
      "Epoch 172/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1811 - jaccard: 0.3122 - val_loss: 0.2135 - val_jaccard: 0.2775\n",
      "Epoch 173/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1800 - jaccard: 0.3166 - val_loss: 0.2125 - val_jaccard: 0.2757\n",
      "Epoch 174/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1791 - jaccard: 0.3169 - val_loss: 0.2127 - val_jaccard: 0.2866\n",
      "Epoch 175/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1826 - jaccard: 0.3151 - val_loss: 0.2140 - val_jaccard: 0.2741\n",
      "Epoch 176/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1805 - jaccard: 0.3116 - val_loss: 0.2130 - val_jaccard: 0.2757\n",
      "Epoch 177/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1792 - jaccard: 0.3169 - val_loss: 0.2127 - val_jaccard: 0.2884\n",
      "Epoch 178/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1787 - jaccard: 0.3206 - val_loss: 0.2114 - val_jaccard: 0.2813\n",
      "Epoch 179/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1781 - jaccard: 0.3193 - val_loss: 0.2114 - val_jaccard: 0.2838\n",
      "Epoch 180/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1782 - jaccard: 0.3219 - val_loss: 0.2112 - val_jaccard: 0.2759\n",
      "Epoch 181/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1770 - jaccard: 0.3204 - val_loss: 0.2128 - val_jaccard: 0.2720\n",
      "Epoch 182/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1763 - jaccard: 0.3241 - val_loss: 0.2117 - val_jaccard: 0.2937\n",
      "Epoch 183/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1778 - jaccard: 0.3228 - val_loss: 0.2113 - val_jaccard: 0.2715\n",
      "Epoch 184/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1752 - jaccard: 0.3257 - val_loss: 0.2115 - val_jaccard: 0.2786\n",
      "Epoch 185/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1758 - jaccard: 0.3244 - val_loss: 0.2104 - val_jaccard: 0.2849\n",
      "Epoch 186/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1766 - jaccard: 0.3207 - val_loss: 0.2108 - val_jaccard: 0.2926\n",
      "Epoch 187/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1744 - jaccard: 0.3290 - val_loss: 0.2108 - val_jaccard: 0.2914\n",
      "Epoch 188/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1737 - jaccard: 0.3279 - val_loss: 0.2098 - val_jaccard: 0.2815\n",
      "Epoch 189/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1751 - jaccard: 0.3266 - val_loss: 0.2129 - val_jaccard: 0.2986\n",
      "Epoch 190/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1757 - jaccard: 0.3266 - val_loss: 0.2124 - val_jaccard: 0.2888\n",
      "Epoch 191/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1723 - jaccard: 0.3325 - val_loss: 0.2094 - val_jaccard: 0.2916\n",
      "Epoch 192/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1739 - jaccard: 0.3280 - val_loss: 0.2120 - val_jaccard: 0.2768\n",
      "Epoch 193/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1733 - jaccard: 0.3338 - val_loss: 0.2117 - val_jaccard: 0.2918\n",
      "Epoch 194/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1726 - jaccard: 0.3317 - val_loss: 0.2107 - val_jaccard: 0.2958\n",
      "Epoch 195/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1719 - jaccard: 0.3342 - val_loss: 0.2089 - val_jaccard: 0.2834\n",
      "Epoch 196/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1703 - jaccard: 0.3363 - val_loss: 0.2083 - val_jaccard: 0.2923\n",
      "Epoch 197/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1698 - jaccard: 0.3376 - val_loss: 0.2095 - val_jaccard: 0.2971\n",
      "Epoch 198/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1697 - jaccard: 0.3385 - val_loss: 0.2078 - val_jaccard: 0.2945\n",
      "Epoch 199/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1695 - jaccard: 0.3343 - val_loss: 0.2091 - val_jaccard: 0.2863\n",
      "Epoch 200/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1697 - jaccard: 0.3412 - val_loss: 0.2081 - val_jaccard: 0.2831\n",
      "Epoch 201/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1686 - jaccard: 0.3393 - val_loss: 0.2104 - val_jaccard: 0.2932\n",
      "Epoch 202/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1685 - jaccard: 0.3413 - val_loss: 0.2083 - val_jaccard: 0.2990\n",
      "Epoch 203/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1696 - jaccard: 0.3382 - val_loss: 0.2108 - val_jaccard: 0.2884\n",
      "Epoch 204/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1683 - jaccard: 0.3407 - val_loss: 0.2078 - val_jaccard: 0.3017\n",
      "Epoch 205/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1689 - jaccard: 0.3394 - val_loss: 0.2088 - val_jaccard: 0.2987\n",
      "Epoch 206/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1676 - jaccard: 0.3414 - val_loss: 0.2071 - val_jaccard: 0.2944\n",
      "Epoch 207/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1663 - jaccard: 0.3460 - val_loss: 0.2077 - val_jaccard: 0.2897\n",
      "Epoch 208/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1664 - jaccard: 0.3434 - val_loss: 0.2097 - val_jaccard: 0.2968\n",
      "Epoch 209/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1657 - jaccard: 0.3475 - val_loss: 0.2086 - val_jaccard: 0.2907\n",
      "Epoch 210/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1681 - jaccard: 0.3428 - val_loss: 0.2078 - val_jaccard: 0.2836\n",
      "Epoch 211/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1693 - jaccard: 0.3378 - val_loss: 0.2098 - val_jaccard: 0.3061\n",
      "Epoch 212/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1678 - jaccard: 0.3418 - val_loss: 0.2082 - val_jaccard: 0.2922\n",
      "Epoch 213/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1661 - jaccard: 0.3463 - val_loss: 0.2071 - val_jaccard: 0.2943\n",
      "Epoch 214/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1648 - jaccard: 0.3508 - val_loss: 0.2077 - val_jaccard: 0.2974\n",
      "Epoch 215/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1643 - jaccard: 0.3512 - val_loss: 0.2074 - val_jaccard: 0.3005\n",
      "Epoch 216/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1647 - jaccard: 0.3472 - val_loss: 0.2086 - val_jaccard: 0.3013\n",
      "Epoch 217/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1631 - jaccard: 0.3559 - val_loss: 0.2067 - val_jaccard: 0.3089\n",
      "Epoch 218/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1635 - jaccard: 0.3523 - val_loss: 0.2078 - val_jaccard: 0.2918\n",
      "Epoch 219/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1633 - jaccard: 0.3535 - val_loss: 0.2059 - val_jaccard: 0.2979\n",
      "Epoch 220/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1629 - jaccard: 0.3523 - val_loss: 0.2067 - val_jaccard: 0.3033\n",
      "Epoch 221/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1626 - jaccard: 0.3559 - val_loss: 0.2062 - val_jaccard: 0.2983\n",
      "Epoch 222/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1633 - jaccard: 0.3524 - val_loss: 0.2067 - val_jaccard: 0.3017\n",
      "Epoch 223/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1618 - jaccard: 0.3579 - val_loss: 0.2052 - val_jaccard: 0.2996\n",
      "Epoch 224/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1635 - jaccard: 0.3546 - val_loss: 0.2065 - val_jaccard: 0.2953\n",
      "Epoch 225/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1621 - jaccard: 0.3572 - val_loss: 0.2067 - val_jaccard: 0.3119\n",
      "Epoch 226/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1613 - jaccard: 0.3553 - val_loss: 0.2059 - val_jaccard: 0.3058\n",
      "Epoch 227/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1603 - jaccard: 0.3627 - val_loss: 0.2049 - val_jaccard: 0.3040\n",
      "Epoch 228/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1592 - jaccard: 0.3623 - val_loss: 0.2064 - val_jaccard: 0.3119\n",
      "Epoch 229/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1599 - jaccard: 0.3623 - val_loss: 0.2074 - val_jaccard: 0.3108\n",
      "Epoch 230/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1594 - jaccard: 0.3634 - val_loss: 0.2068 - val_jaccard: 0.3057\n",
      "Epoch 231/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1610 - jaccard: 0.3590 - val_loss: 0.2069 - val_jaccard: 0.3035\n",
      "Epoch 232/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1599 - jaccard: 0.3616 - val_loss: 0.2067 - val_jaccard: 0.3090\n",
      "Epoch 233/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1595 - jaccard: 0.3654 - val_loss: 0.2055 - val_jaccard: 0.3077\n",
      "Epoch 234/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1582 - jaccard: 0.3651 - val_loss: 0.2046 - val_jaccard: 0.3132\n",
      "Epoch 235/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1576 - jaccard: 0.3672 - val_loss: 0.2050 - val_jaccard: 0.3052\n",
      "Epoch 236/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1574 - jaccard: 0.3692 - val_loss: 0.2063 - val_jaccard: 0.3172\n",
      "Epoch 237/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1600 - jaccard: 0.3624 - val_loss: 0.2049 - val_jaccard: 0.2942\n",
      "Epoch 238/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1620 - jaccard: 0.3574 - val_loss: 0.2076 - val_jaccard: 0.3113\n",
      "Epoch 239/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1610 - jaccard: 0.3586 - val_loss: 0.2058 - val_jaccard: 0.2987\n",
      "Epoch 240/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1585 - jaccard: 0.3673 - val_loss: 0.2055 - val_jaccard: 0.3103\n",
      "Epoch 241/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1577 - jaccard: 0.3697 - val_loss: 0.2050 - val_jaccard: 0.3115\n",
      "Epoch 242/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1575 - jaccard: 0.3696 - val_loss: 0.2051 - val_jaccard: 0.3133\n",
      "Epoch 243/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1568 - jaccard: 0.3676 - val_loss: 0.2074 - val_jaccard: 0.3235\n",
      "Epoch 244/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1565 - jaccard: 0.3720 - val_loss: 0.2063 - val_jaccard: 0.3037\n",
      "Epoch 245/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1553 - jaccard: 0.3731 - val_loss: 0.2064 - val_jaccard: 0.3174\n",
      "Epoch 246/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1559 - jaccard: 0.3747 - val_loss: 0.2049 - val_jaccard: 0.3118\n",
      "Epoch 247/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1571 - jaccard: 0.3690 - val_loss: 0.2073 - val_jaccard: 0.3118\n",
      "Epoch 248/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1557 - jaccard: 0.3703 - val_loss: 0.2065 - val_jaccard: 0.3182\n",
      "Epoch 249/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1564 - jaccard: 0.3704 - val_loss: 0.2056 - val_jaccard: 0.3096\n",
      "Epoch 250/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1545 - jaccard: 0.3778 - val_loss: 0.2055 - val_jaccard: 0.3098\n",
      "Epoch 251/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1551 - jaccard: 0.3753 - val_loss: 0.2070 - val_jaccard: 0.2997\n",
      "Epoch 252/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1553 - jaccard: 0.3744 - val_loss: 0.2053 - val_jaccard: 0.3093\n",
      "Epoch 253/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1542 - jaccard: 0.3757 - val_loss: 0.2052 - val_jaccard: 0.3239\n",
      "Epoch 254/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1537 - jaccard: 0.3771 - val_loss: 0.2050 - val_jaccard: 0.3173\n",
      "Epoch 255/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1528 - jaccard: 0.3798 - val_loss: 0.2050 - val_jaccard: 0.3162\n",
      "Epoch 256/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1526 - jaccard: 0.3777 - val_loss: 0.2040 - val_jaccard: 0.3177\n",
      "Epoch 257/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1527 - jaccard: 0.3821 - val_loss: 0.2044 - val_jaccard: 0.3103\n",
      "Epoch 258/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1522 - jaccard: 0.3829 - val_loss: 0.2037 - val_jaccard: 0.3114\n",
      "Epoch 259/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1527 - jaccard: 0.3794 - val_loss: 0.2051 - val_jaccard: 0.3099\n",
      "Epoch 260/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1509 - jaccard: 0.3833 - val_loss: 0.2051 - val_jaccard: 0.3240\n",
      "Epoch 261/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1524 - jaccard: 0.3789 - val_loss: 0.2054 - val_jaccard: 0.3232\n",
      "Epoch 262/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1519 - jaccard: 0.3848 - val_loss: 0.2041 - val_jaccard: 0.3159\n",
      "Epoch 263/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1515 - jaccard: 0.3830 - val_loss: 0.2048 - val_jaccard: 0.3173\n",
      "Epoch 264/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1526 - jaccard: 0.3821 - val_loss: 0.2039 - val_jaccard: 0.3192\n",
      "Epoch 265/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1514 - jaccard: 0.3848 - val_loss: 0.2024 - val_jaccard: 0.3203\n",
      "Epoch 266/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1511 - jaccard: 0.3851 - val_loss: 0.2035 - val_jaccard: 0.3158\n",
      "Epoch 267/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1505 - jaccard: 0.3867 - val_loss: 0.2041 - val_jaccard: 0.3168\n",
      "Epoch 268/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1506 - jaccard: 0.3850 - val_loss: 0.2064 - val_jaccard: 0.3099\n",
      "Epoch 269/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1516 - jaccard: 0.3850 - val_loss: 0.2045 - val_jaccard: 0.3197\n",
      "Epoch 270/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1526 - jaccard: 0.3766 - val_loss: 0.2057 - val_jaccard: 0.3176\n",
      "Epoch 271/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1522 - jaccard: 0.3861 - val_loss: 0.2052 - val_jaccard: 0.3134\n",
      "Epoch 272/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1508 - jaccard: 0.3835 - val_loss: 0.2047 - val_jaccard: 0.3215\n",
      "Epoch 273/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1490 - jaccard: 0.3905 - val_loss: 0.2043 - val_jaccard: 0.3266\n",
      "Epoch 274/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1496 - jaccard: 0.3903 - val_loss: 0.2045 - val_jaccard: 0.3272\n",
      "Epoch 275/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1492 - jaccard: 0.3874 - val_loss: 0.2058 - val_jaccard: 0.3242\n",
      "Epoch 276/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1498 - jaccard: 0.3888 - val_loss: 0.2048 - val_jaccard: 0.3123\n",
      "Epoch 277/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1481 - jaccard: 0.3925 - val_loss: 0.2044 - val_jaccard: 0.3231\n",
      "Epoch 278/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1488 - jaccard: 0.3916 - val_loss: 0.2046 - val_jaccard: 0.3257\n",
      "Epoch 279/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1483 - jaccard: 0.3917 - val_loss: 0.2057 - val_jaccard: 0.3172\n",
      "Epoch 280/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1478 - jaccard: 0.3925 - val_loss: 0.2060 - val_jaccard: 0.3220\n",
      "Epoch 281/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1472 - jaccard: 0.3962 - val_loss: 0.2048 - val_jaccard: 0.3236\n",
      "Epoch 282/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1475 - jaccard: 0.3918 - val_loss: 0.2038 - val_jaccard: 0.3148\n",
      "Epoch 283/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1469 - jaccard: 0.3974 - val_loss: 0.2066 - val_jaccard: 0.3133\n",
      "Epoch 284/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1491 - jaccard: 0.3873 - val_loss: 0.2054 - val_jaccard: 0.3226\n",
      "Epoch 285/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1479 - jaccard: 0.3978 - val_loss: 0.2043 - val_jaccard: 0.3285\n",
      "Epoch 286/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1476 - jaccard: 0.3941 - val_loss: 0.2073 - val_jaccard: 0.3087\n",
      "Epoch 287/1000\n",
      "320/320 [==============================] - 20s - loss: 0.1468 - jaccard: 0.3929 - val_loss: 0.2049 - val_jaccard: 0.3273\n",
      "Epoch 288/1000\n",
      "200/320 [=================>............] - ETA: 6s - loss: 0.1478 - jaccard: 0.3943"
     ]
    }
   ],
   "source": [
    "def trainer(model,fit=True,use_existing=False):\n",
    "    print('This is run # %i' %run)\n",
    "    \n",
    "    if use_existing:\n",
    "        model.load_weights('./data/model_weights_class_3_run_'+str(run)+'.hdf5')\n",
    "        \n",
    "    if fit:\n",
    "        quitter = EarlyStopping(monitor='loss', min_delta=0.001, patience=100, verbose=1, mode='auto')\n",
    "        lrreducer = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, verbose=1, mode='auto', epsilon=0.001, cooldown=2, min_lr=0)\n",
    "        model_checkpoint = ModelCheckpoint('./data/model_weights_class_3_run_'+str(run)+'.hdf5', monitor='loss', save_best_only=True)\n",
    "        csvlogger = CSVLogger('./data/training_log_run_'+str(run), separator=',', append=True)\n",
    "\n",
    "        model.fit(x, y_oneclass,\n",
    "                  batch_size=20,\n",
    "                  nb_epoch=1000,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  callbacks=[model_checkpoint,csvlogger],\n",
    "                  validation_split=0.2,\n",
    "                  initial_epoch=0)\n",
    "            \n",
    "    preds = model.predict(x, verbose=1)\n",
    "    np.save('preds.npy', preds)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = trainer(model,fit=True,use_existing=False)\n",
    "model.save('u-net-complete-model.h5')\n",
    "push('Training is done',\n",
    "     'Train loss: %f, train jaccard: %f, val loss %f, val jaccard%f' %(model.history.history['loss'][-1],model.history.history['jaccard'][-1],model.history.history['val_loss'][-1],model.history.history['val_jaccard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
