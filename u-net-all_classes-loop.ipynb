{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U-Net architecture\n",
    "\n",
    "See [here](https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py#L19) for code and [here](https://arxiv.org/pdf/1505.04597.pdf) for the original literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def restart_kernel(restart=False):\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    if restart:\n",
    "        app.kernel.do_shutdown(True)\n",
    "\n",
    "restart_kernel(False)\n",
    "\n",
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=gpu,lib.cnmem=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Reshape, Input, merge, Activation, Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "# \"tf\" assumes (rows, cols, channels) while \"th\" assumes (channels, rows, cols)\n",
    "# Possibly change this around natively in the data so the backend doesn't have to switch them\n",
    "# Only necessary if I use TF!\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pushbullet import Pushbullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run # 115\n"
     ]
    }
   ],
   "source": [
    "# Pushbullet notifier\n",
    "def push(title='Done!',text=''):\n",
    "    Pushbullet('o.YFPNNPfGRekivaCGHa4qMSgjZt8zJ6FL').devices[0].push_note(title,text)\n",
    "    \n",
    "# Import the training data\n",
    "def import_data(img_h,img_w):\n",
    "    x = np.load('./data/training-data/tiled_patched_images_576_576.npy','r')\n",
    "    y = np.load('./data/training-data/tiled_patched_masks_576_576.npy','r')\n",
    "    #y = y.reshape((y.shape[0],y.shape[1],y.shape[2]*y.shape[3]))\n",
    "    '''\n",
    "    Classes:\n",
    "    0 Buildings - large building, residential, non-residential, fuel storage facility, fortified building\n",
    "    1 Misc. Manmade structures \n",
    "    2 Road \n",
    "    3 Track - poor/dirt/cart track, footpath/trail\n",
    "    4 Trees - woodland, hedgerows, groups of trees, standalone trees\n",
    "    5 Crops - contour ploughing/cropland, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "    6 Waterway \n",
    "    7 Standing water\n",
    "    8 Vehicle Large - large vehicle (e.g. lorry, truck,bus), logistics vehicle\n",
    "    9 Vehicle Small - small vehicle (car, van), motorbike\n",
    "    '''    \n",
    "    return x, y\n",
    "\n",
    "img_h,img_w = 160,160\n",
    "x, y = import_data(img_h,img_w)\n",
    "\n",
    "# Increment the counter\n",
    "def counter():\n",
    "    run = np.load('./data/misc/run_counter.npy')\n",
    "    run += 1\n",
    "    np.save('./data/misc/run_counter.npy',run)\n",
    "    return run\n",
    "run = counter()\n",
    "\n",
    "# Set the counter to a specific value\n",
    "def set_counter(run):\n",
    "    run = run\n",
    "    np.save('./data/misc/run_counter.npy',run)\n",
    "    return run\n",
    "# Uncomment the next line to manually set the counter if something goes wrong\n",
    "#run = set_counter(108)\n",
    "print('This is run # %i' %run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(324, 20, 160, 160)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 31.3 ms, total: 1.36 s\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def compiler(img_rows = x.shape[2],img_cols = x.shape[3],\n",
    "            nfilters = 64,activation = 'relu',init = 'he_normal',\n",
    "            lr=0.001,decay=0.0,p=[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2]):\n",
    "    \n",
    "    def jaccard(y_true, y_pred,smooth=1.):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "    \n",
    "    def Conv2DReluBatchNorm(n_filter, w_filter, h_filter, inputs, activation, init='he_uniform',dropout=0.2):\n",
    "        return BatchNormalization(mode=2, axis=1)(Activation(activation=activation)((Convolution2D(n_filter, w_filter, h_filter, border_mode='same',init=init,W_constraint = maxnorm(3))(inputs))))\n",
    "        \n",
    "    def up_conv(nfilters,filter_factor,inputs,init=init,activation=activation):\n",
    "        return BatchNormalization(mode=2, axis=1)(Activation(activation=activation)(Convolution2D(nfilters*filter_factor, 2, 2, border_mode='same',init=init,W_constraint = maxnorm(3))(UpSampling2D(size=(2, 2))(inputs))))\n",
    "\n",
    "    inputs = Input((20, img_rows, img_cols))\n",
    "    \n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, inputs, activation=activation,init=init)\n",
    "    conv1 = Conv2DReluBatchNorm(nfilters, 3, 3, conv1, activation=activation,init=init)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(p=p[0])(pool1)\n",
    "\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, pool1, activation=activation,init=init)\n",
    "    conv2 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv2, activation=activation,init=init)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(p=p[1])(pool2)\n",
    "\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, pool2, activation=activation,init=init)\n",
    "    conv3 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv3, activation=activation,init=init)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(p=p[2])(pool3)\n",
    "\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, pool3, activation=activation,init=init)\n",
    "    conv4 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv4, activation=activation,init=init)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = Dropout(p=p[3])(pool4)\n",
    "\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, pool4, activation=activation,init=init)\n",
    "    conv5 = Conv2DReluBatchNorm(nfilters*16, 3, 3, conv5, activation=activation,init=init)\n",
    "    conv5 = Dropout(p=p[4])(conv5)\n",
    "        \n",
    "    up6 = merge([up_conv(nfilters,8,conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, up6, activation=activation,init=init)\n",
    "    conv6 = Conv2DReluBatchNorm(nfilters*8, 3, 3, conv6, activation=activation,init=init)\n",
    "    conv6 = Dropout(p=p[5])(conv6)\n",
    "\n",
    "    up7 = merge([up_conv(nfilters,4,conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, up7, activation=activation,init=init)\n",
    "    conv7 = Conv2DReluBatchNorm(nfilters*4, 3, 3, conv7, activation=activation,init=init)\n",
    "    conv7 = Dropout(p=p[6])(conv7)\n",
    "\n",
    "    up8 = merge([up_conv(nfilters,2,conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, up8, activation=activation,init=init)\n",
    "    conv8 = Conv2DReluBatchNorm(nfilters*2, 3, 3, conv8, activation=activation,init=init)\n",
    "    conv8 = Dropout(p=p[7])(conv8)\n",
    "\n",
    "    up9 = merge([up_conv(nfilters,1,conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, up9, activation=activation,init=init)\n",
    "    conv9 = Conv2DReluBatchNorm(nfilters, 3, 3, conv9, activation=activation,init=init)\n",
    "    conv9 = Dropout(p=p[8])(conv9)\n",
    "    \n",
    "    conv10 = Conv2DReluBatchNorm(1, 1, 1, conv9, activation='relu',init=init)\n",
    "    output = Activation(activation='sigmoid')(conv10)\n",
    "    \n",
    "    model = Model(input=inputs, output=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=lr,decay=decay), loss='binary_crossentropy',metrics=[jaccard])\n",
    "\n",
    "    return model\n",
    "\n",
    "#p = [0]*9\n",
    "p=[0.1,0.2,0.3,0.4,0.5,0.4,0.3,0.2,0.1] # current version\n",
    "#p=[0.2,0.3,0.4,0.5,0.5,0.5,0.4,0.3,0.2] # symmetric but more dropout\n",
    "#p=[0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6] # increasing\n",
    "\n",
    "model = compiler(img_rows=x.shape[2],img_cols=x.shape[3],\n",
    "            nfilters=16,activation='relu',init='he_normal',\n",
    "            lr=0.001,p=p)\n",
    "model.save_weights('./data/misc/initial_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is run # 115 for class 3\n",
      "Train on 259 samples, validate on 65 samples\n",
      "Epoch 1/1000\n",
      "259/259 [==============================] - 13s - loss: 0.7751 - jaccard: 0.0361 - val_loss: 0.7428 - val_jaccard: 0.0399\n",
      "Epoch 2/1000\n",
      "259/259 [==============================] - 13s - loss: 0.7200 - jaccard: 0.0391 - val_loss: 0.6986 - val_jaccard: 0.0344\n",
      "Epoch 3/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6825 - jaccard: 0.0350 - val_loss: 0.6798 - val_jaccard: 0.0336\n",
      "Epoch 4/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6725 - jaccard: 0.0345 - val_loss: 0.6650 - val_jaccard: 0.0330\n",
      "Epoch 5/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6628 - jaccard: 0.0341 - val_loss: 0.6589 - val_jaccard: 0.0333\n",
      "Epoch 6/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6532 - jaccard: 0.0349 - val_loss: 0.6516 - val_jaccard: 0.0334\n",
      "Epoch 7/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6466 - jaccard: 0.0350 - val_loss: 0.6447 - val_jaccard: 0.0342\n",
      "Epoch 8/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6394 - jaccard: 0.0358 - val_loss: 0.6376 - val_jaccard: 0.0341\n",
      "Epoch 9/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6348 - jaccard: 0.0361 - val_loss: 0.6333 - val_jaccard: 0.0350\n",
      "Epoch 10/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6287 - jaccard: 0.0363 - val_loss: 0.6266 - val_jaccard: 0.0351\n",
      "Epoch 11/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6212 - jaccard: 0.0369 - val_loss: 0.6213 - val_jaccard: 0.0341\n",
      "Epoch 12/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6155 - jaccard: 0.0367 - val_loss: 0.6156 - val_jaccard: 0.0363\n",
      "Epoch 13/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6092 - jaccard: 0.0382 - val_loss: 0.6094 - val_jaccard: 0.0354\n",
      "Epoch 14/1000\n",
      "259/259 [==============================] - 13s - loss: 0.6041 - jaccard: 0.0380 - val_loss: 0.6026 - val_jaccard: 0.0360\n",
      "Epoch 15/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5979 - jaccard: 0.0385 - val_loss: 0.5975 - val_jaccard: 0.0371\n",
      "Epoch 16/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5914 - jaccard: 0.0394 - val_loss: 0.5918 - val_jaccard: 0.0369\n",
      "Epoch 17/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5861 - jaccard: 0.0393 - val_loss: 0.5875 - val_jaccard: 0.0361\n",
      "Epoch 18/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5807 - jaccard: 0.0392 - val_loss: 0.5824 - val_jaccard: 0.0373\n",
      "Epoch 19/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5760 - jaccard: 0.0395 - val_loss: 0.5775 - val_jaccard: 0.0377\n",
      "Epoch 20/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5680 - jaccard: 0.0410 - val_loss: 0.5729 - val_jaccard: 0.0395\n",
      "Epoch 21/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5633 - jaccard: 0.0408 - val_loss: 0.5687 - val_jaccard: 0.0392\n",
      "Epoch 22/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5574 - jaccard: 0.0416 - val_loss: 0.5616 - val_jaccard: 0.0382\n",
      "Epoch 23/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5523 - jaccard: 0.0418 - val_loss: 0.5564 - val_jaccard: 0.0396\n",
      "Epoch 24/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5479 - jaccard: 0.0426 - val_loss: 0.5511 - val_jaccard: 0.0404\n",
      "Epoch 25/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5427 - jaccard: 0.0416 - val_loss: 0.5490 - val_jaccard: 0.0401\n",
      "Epoch 26/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5361 - jaccard: 0.0435 - val_loss: 0.5408 - val_jaccard: 0.0399\n",
      "Epoch 27/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5298 - jaccard: 0.0439 - val_loss: 0.5367 - val_jaccard: 0.0416\n",
      "Epoch 28/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5253 - jaccard: 0.0446 - val_loss: 0.5322 - val_jaccard: 0.0405\n",
      "Epoch 29/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5212 - jaccard: 0.0436 - val_loss: 0.5301 - val_jaccard: 0.0410\n",
      "Epoch 30/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5146 - jaccard: 0.0460 - val_loss: 0.5224 - val_jaccard: 0.0410\n",
      "Epoch 31/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5112 - jaccard: 0.0444 - val_loss: 0.5186 - val_jaccard: 0.0397\n",
      "Epoch 32/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5065 - jaccard: 0.0453 - val_loss: 0.5178 - val_jaccard: 0.0414\n",
      "Epoch 33/1000\n",
      "259/259 [==============================] - 13s - loss: 0.5010 - jaccard: 0.0459 - val_loss: 0.5147 - val_jaccard: 0.0405\n",
      "Epoch 34/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4976 - jaccard: 0.0460 - val_loss: 0.5057 - val_jaccard: 0.0404\n",
      "Epoch 35/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4917 - jaccard: 0.0466 - val_loss: 0.5027 - val_jaccard: 0.0404\n",
      "Epoch 36/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4863 - jaccard: 0.0473 - val_loss: 0.4992 - val_jaccard: 0.0423\n",
      "Epoch 37/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4827 - jaccard: 0.0477 - val_loss: 0.4956 - val_jaccard: 0.0410\n",
      "Epoch 38/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4778 - jaccard: 0.0474 - val_loss: 0.4914 - val_jaccard: 0.0422\n",
      "Epoch 39/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4725 - jaccard: 0.0487 - val_loss: 0.4865 - val_jaccard: 0.0425\n",
      "Epoch 40/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4684 - jaccard: 0.0485 - val_loss: 0.4857 - val_jaccard: 0.0417\n",
      "Epoch 41/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4625 - jaccard: 0.0497 - val_loss: 0.4808 - val_jaccard: 0.0429\n",
      "Epoch 42/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4583 - jaccard: 0.0501 - val_loss: 0.4772 - val_jaccard: 0.0424\n",
      "Epoch 43/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4537 - jaccard: 0.0514 - val_loss: 0.4703 - val_jaccard: 0.0428\n",
      "Epoch 44/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4507 - jaccard: 0.0511 - val_loss: 0.4707 - val_jaccard: 0.0426\n",
      "Epoch 45/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4489 - jaccard: 0.0499 - val_loss: 0.4691 - val_jaccard: 0.0454\n",
      "Epoch 46/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4437 - jaccard: 0.0510 - val_loss: 0.4623 - val_jaccard: 0.0425\n",
      "Epoch 47/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4387 - jaccard: 0.0511 - val_loss: 0.4591 - val_jaccard: 0.0449\n",
      "Epoch 48/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4335 - jaccard: 0.0532 - val_loss: 0.4565 - val_jaccard: 0.0454\n",
      "Epoch 49/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4292 - jaccard: 0.0533 - val_loss: 0.4516 - val_jaccard: 0.0462\n",
      "Epoch 50/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4262 - jaccard: 0.0534 - val_loss: 0.4523 - val_jaccard: 0.0470\n",
      "Epoch 51/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4213 - jaccard: 0.0542 - val_loss: 0.4465 - val_jaccard: 0.0460\n",
      "Epoch 52/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4164 - jaccard: 0.0559 - val_loss: 0.4439 - val_jaccard: 0.0448\n",
      "Epoch 53/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4126 - jaccard: 0.0559 - val_loss: 0.4429 - val_jaccard: 0.0452\n",
      "Epoch 54/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4108 - jaccard: 0.0551 - val_loss: 0.4342 - val_jaccard: 0.0445\n",
      "Epoch 55/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4062 - jaccard: 0.0561 - val_loss: 0.4351 - val_jaccard: 0.0443\n",
      "Epoch 56/1000\n",
      "259/259 [==============================] - 13s - loss: 0.4027 - jaccard: 0.0569 - val_loss: 0.4302 - val_jaccard: 0.0445\n",
      "Epoch 57/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3988 - jaccard: 0.0557 - val_loss: 0.4319 - val_jaccard: 0.0469\n",
      "Epoch 58/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3950 - jaccard: 0.0586 - val_loss: 0.4240 - val_jaccard: 0.0455\n",
      "Epoch 59/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3916 - jaccard: 0.0580 - val_loss: 0.4219 - val_jaccard: 0.0490\n",
      "Epoch 60/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3880 - jaccard: 0.0591 - val_loss: 0.4179 - val_jaccard: 0.0469\n",
      "Epoch 61/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3840 - jaccard: 0.0592 - val_loss: 0.4190 - val_jaccard: 0.0484\n",
      "Epoch 62/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3827 - jaccard: 0.0588 - val_loss: 0.4134 - val_jaccard: 0.0489\n",
      "Epoch 63/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3775 - jaccard: 0.0610 - val_loss: 0.4109 - val_jaccard: 0.0459\n",
      "Epoch 64/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3758 - jaccard: 0.0597 - val_loss: 0.4048 - val_jaccard: 0.0459\n",
      "Epoch 65/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3712 - jaccard: 0.0612 - val_loss: 0.4034 - val_jaccard: 0.0457\n",
      "Epoch 66/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3677 - jaccard: 0.0613 - val_loss: 0.4021 - val_jaccard: 0.0488\n",
      "Epoch 67/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3636 - jaccard: 0.0636 - val_loss: 0.3992 - val_jaccard: 0.0487\n",
      "Epoch 68/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3590 - jaccard: 0.0628 - val_loss: 0.4038 - val_jaccard: 0.0519\n",
      "Epoch 69/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3562 - jaccard: 0.0643 - val_loss: 0.3959 - val_jaccard: 0.0519\n",
      "Epoch 70/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3547 - jaccard: 0.0654 - val_loss: 0.3936 - val_jaccard: 0.0491\n",
      "Epoch 71/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3509 - jaccard: 0.0646 - val_loss: 0.3969 - val_jaccard: 0.0468\n",
      "Epoch 72/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3486 - jaccard: 0.0655 - val_loss: 0.3875 - val_jaccard: 0.0501\n",
      "Epoch 73/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3441 - jaccard: 0.0662 - val_loss: 0.3856 - val_jaccard: 0.0518\n",
      "Epoch 74/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3410 - jaccard: 0.0674 - val_loss: 0.3847 - val_jaccard: 0.0487\n",
      "Epoch 75/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3384 - jaccard: 0.0667 - val_loss: 0.3792 - val_jaccard: 0.0535\n",
      "Epoch 76/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3349 - jaccard: 0.0694 - val_loss: 0.3755 - val_jaccard: 0.0520\n",
      "Epoch 77/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3311 - jaccard: 0.0694 - val_loss: 0.3763 - val_jaccard: 0.0524\n",
      "Epoch 78/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3296 - jaccard: 0.0685 - val_loss: 0.3703 - val_jaccard: 0.0509\n",
      "Epoch 79/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3258 - jaccard: 0.0704 - val_loss: 0.3753 - val_jaccard: 0.0544\n",
      "Epoch 80/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3228 - jaccard: 0.0709 - val_loss: 0.3721 - val_jaccard: 0.0543\n",
      "Epoch 81/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3195 - jaccard: 0.0728 - val_loss: 0.3678 - val_jaccard: 0.0527\n",
      "Epoch 82/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3171 - jaccard: 0.0725 - val_loss: 0.3660 - val_jaccard: 0.0540\n",
      "Epoch 83/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3156 - jaccard: 0.0718 - val_loss: 0.3633 - val_jaccard: 0.0554\n",
      "Epoch 84/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3124 - jaccard: 0.0743 - val_loss: 0.3553 - val_jaccard: 0.0526\n",
      "Epoch 85/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3099 - jaccard: 0.0732 - val_loss: 0.3560 - val_jaccard: 0.0514\n",
      "Epoch 86/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3068 - jaccard: 0.0753 - val_loss: 0.3521 - val_jaccard: 0.0498\n",
      "Epoch 87/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3057 - jaccard: 0.0742 - val_loss: 0.3507 - val_jaccard: 0.0538\n",
      "Epoch 88/1000\n",
      "259/259 [==============================] - 13s - loss: 0.3022 - jaccard: 0.0754 - val_loss: 0.3511 - val_jaccard: 0.0535\n",
      "Epoch 89/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2991 - jaccard: 0.0766 - val_loss: 0.3521 - val_jaccard: 0.0560\n",
      "Epoch 90/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2955 - jaccard: 0.0778 - val_loss: 0.3478 - val_jaccard: 0.0526\n",
      "Epoch 91/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2941 - jaccard: 0.0781 - val_loss: 0.3444 - val_jaccard: 0.0543\n",
      "Epoch 92/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2913 - jaccard: 0.0781 - val_loss: 0.3448 - val_jaccard: 0.0569\n",
      "Epoch 93/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2893 - jaccard: 0.0789 - val_loss: 0.3418 - val_jaccard: 0.0560\n",
      "Epoch 94/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2857 - jaccard: 0.0807 - val_loss: 0.3374 - val_jaccard: 0.0570\n",
      "Epoch 95/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2837 - jaccard: 0.0805 - val_loss: 0.3409 - val_jaccard: 0.0585\n",
      "Epoch 96/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2828 - jaccard: 0.0807 - val_loss: 0.3381 - val_jaccard: 0.0574\n",
      "Epoch 97/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2788 - jaccard: 0.0820 - val_loss: 0.3367 - val_jaccard: 0.0582\n",
      "Epoch 98/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2773 - jaccard: 0.0822 - val_loss: 0.3316 - val_jaccard: 0.0588\n",
      "Epoch 99/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2747 - jaccard: 0.0845 - val_loss: 0.3275 - val_jaccard: 0.0577\n",
      "Epoch 100/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2730 - jaccard: 0.0838 - val_loss: 0.3276 - val_jaccard: 0.0543\n",
      "Epoch 101/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2696 - jaccard: 0.0850 - val_loss: 0.3281 - val_jaccard: 0.0587\n",
      "Epoch 102/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2669 - jaccard: 0.0869 - val_loss: 0.3237 - val_jaccard: 0.0576\n",
      "Epoch 103/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2650 - jaccard: 0.0868 - val_loss: 0.3210 - val_jaccard: 0.0573\n",
      "Epoch 104/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2626 - jaccard: 0.0878 - val_loss: 0.3230 - val_jaccard: 0.0562\n",
      "Epoch 105/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2608 - jaccard: 0.0873 - val_loss: 0.3179 - val_jaccard: 0.0594\n",
      "Epoch 106/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2587 - jaccard: 0.0891 - val_loss: 0.3139 - val_jaccard: 0.0596\n",
      "Epoch 107/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2563 - jaccard: 0.0905 - val_loss: 0.3104 - val_jaccard: 0.0573\n",
      "Epoch 108/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2554 - jaccard: 0.0891 - val_loss: 0.3089 - val_jaccard: 0.0577\n",
      "Epoch 109/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2533 - jaccard: 0.0916 - val_loss: 0.3166 - val_jaccard: 0.0572\n",
      "Epoch 110/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2511 - jaccard: 0.0902 - val_loss: 0.3173 - val_jaccard: 0.0643\n",
      "Epoch 111/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2481 - jaccard: 0.0924 - val_loss: 0.3194 - val_jaccard: 0.0647\n",
      "Epoch 112/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2469 - jaccard: 0.0931 - val_loss: 0.3089 - val_jaccard: 0.0619\n",
      "Epoch 113/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2450 - jaccard: 0.0934 - val_loss: 0.3114 - val_jaccard: 0.0620\n",
      "Epoch 114/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2433 - jaccard: 0.0952 - val_loss: 0.3032 - val_jaccard: 0.0588\n",
      "Epoch 115/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2423 - jaccard: 0.0937 - val_loss: 0.3019 - val_jaccard: 0.0617\n",
      "Epoch 116/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2391 - jaccard: 0.0951 - val_loss: 0.3066 - val_jaccard: 0.0611\n",
      "Epoch 117/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2374 - jaccard: 0.0953 - val_loss: 0.3053 - val_jaccard: 0.0651\n",
      "Epoch 118/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2351 - jaccard: 0.0983 - val_loss: 0.3008 - val_jaccard: 0.0646\n",
      "Epoch 119/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2345 - jaccard: 0.0982 - val_loss: 0.2998 - val_jaccard: 0.0605\n",
      "Epoch 120/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2329 - jaccard: 0.0971 - val_loss: 0.2957 - val_jaccard: 0.0618\n",
      "Epoch 121/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2299 - jaccard: 0.0997 - val_loss: 0.2991 - val_jaccard: 0.0658\n",
      "Epoch 122/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2278 - jaccard: 0.1003 - val_loss: 0.2985 - val_jaccard: 0.0648\n",
      "Epoch 123/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2274 - jaccard: 0.0990 - val_loss: 0.2975 - val_jaccard: 0.0656\n",
      "Epoch 124/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2244 - jaccard: 0.1023 - val_loss: 0.2898 - val_jaccard: 0.0657\n",
      "Epoch 125/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2230 - jaccard: 0.1020 - val_loss: 0.2924 - val_jaccard: 0.0669\n",
      "Epoch 126/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2202 - jaccard: 0.1047 - val_loss: 0.2975 - val_jaccard: 0.0663\n",
      "Epoch 127/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2190 - jaccard: 0.1055 - val_loss: 0.2948 - val_jaccard: 0.0653\n",
      "Epoch 128/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2179 - jaccard: 0.1044 - val_loss: 0.2899 - val_jaccard: 0.0681\n",
      "Epoch 129/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2163 - jaccard: 0.1067 - val_loss: 0.2871 - val_jaccard: 0.0669\n",
      "Epoch 130/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2142 - jaccard: 0.1080 - val_loss: 0.2805 - val_jaccard: 0.0635\n",
      "Epoch 131/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2130 - jaccard: 0.1070 - val_loss: 0.2839 - val_jaccard: 0.0618\n",
      "Epoch 132/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2121 - jaccard: 0.1077 - val_loss: 0.2874 - val_jaccard: 0.0634\n",
      "Epoch 133/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2102 - jaccard: 0.1073 - val_loss: 0.2842 - val_jaccard: 0.0652\n",
      "Epoch 134/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2079 - jaccard: 0.1104 - val_loss: 0.2798 - val_jaccard: 0.0655\n",
      "Epoch 135/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2077 - jaccard: 0.1083 - val_loss: 0.2872 - val_jaccard: 0.0686\n",
      "Epoch 136/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2061 - jaccard: 0.1105 - val_loss: 0.2773 - val_jaccard: 0.0629\n",
      "Epoch 137/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2041 - jaccard: 0.1119 - val_loss: 0.2790 - val_jaccard: 0.0675\n",
      "Epoch 138/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2021 - jaccard: 0.1117 - val_loss: 0.2862 - val_jaccard: 0.0730\n",
      "Epoch 139/1000\n",
      "259/259 [==============================] - 13s - loss: 0.2009 - jaccard: 0.1138 - val_loss: 0.2837 - val_jaccard: 0.0729\n",
      "Epoch 140/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1994 - jaccard: 0.1158 - val_loss: 0.2735 - val_jaccard: 0.0676\n",
      "Epoch 141/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1985 - jaccard: 0.1143 - val_loss: 0.2718 - val_jaccard: 0.0686\n",
      "Epoch 142/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1965 - jaccard: 0.1160 - val_loss: 0.2750 - val_jaccard: 0.0696\n",
      "Epoch 143/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1954 - jaccard: 0.1159 - val_loss: 0.2813 - val_jaccard: 0.0753\n",
      "Epoch 144/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1944 - jaccard: 0.1160 - val_loss: 0.2817 - val_jaccard: 0.0748\n",
      "Epoch 145/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1929 - jaccard: 0.1190 - val_loss: 0.2681 - val_jaccard: 0.0683\n",
      "Epoch 146/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1915 - jaccard: 0.1195 - val_loss: 0.2636 - val_jaccard: 0.0651\n",
      "Epoch 147/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1903 - jaccard: 0.1182 - val_loss: 0.2675 - val_jaccard: 0.0690\n",
      "Epoch 148/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1884 - jaccard: 0.1202 - val_loss: 0.2670 - val_jaccard: 0.0693\n",
      "Epoch 149/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1874 - jaccard: 0.1188 - val_loss: 0.2747 - val_jaccard: 0.0738\n",
      "Epoch 150/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1846 - jaccard: 0.1256 - val_loss: 0.2623 - val_jaccard: 0.0734\n",
      "Epoch 151/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1836 - jaccard: 0.1254 - val_loss: 0.2678 - val_jaccard: 0.0741\n",
      "Epoch 152/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1824 - jaccard: 0.1246 - val_loss: 0.2741 - val_jaccard: 0.0767\n",
      "Epoch 153/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1800 - jaccard: 0.1278 - val_loss: 0.2613 - val_jaccard: 0.0744\n",
      "Epoch 154/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1796 - jaccard: 0.1281 - val_loss: 0.2573 - val_jaccard: 0.0727\n",
      "Epoch 155/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1776 - jaccard: 0.1306 - val_loss: 0.2545 - val_jaccard: 0.0694\n",
      "Epoch 156/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1773 - jaccard: 0.1280 - val_loss: 0.2555 - val_jaccard: 0.0708\n",
      "Epoch 157/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1757 - jaccard: 0.1298 - val_loss: 0.2674 - val_jaccard: 0.0794\n",
      "Epoch 158/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1742 - jaccard: 0.1310 - val_loss: 0.2603 - val_jaccard: 0.0775\n",
      "Epoch 159/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1720 - jaccard: 0.1340 - val_loss: 0.2617 - val_jaccard: 0.0757\n",
      "Epoch 160/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1723 - jaccard: 0.1324 - val_loss: 0.2553 - val_jaccard: 0.0728\n",
      "Epoch 161/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1705 - jaccard: 0.1353 - val_loss: 0.2525 - val_jaccard: 0.0742\n",
      "Epoch 162/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1698 - jaccard: 0.1343 - val_loss: 0.2551 - val_jaccard: 0.0767\n",
      "Epoch 163/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1677 - jaccard: 0.1369 - val_loss: 0.2570 - val_jaccard: 0.0744\n",
      "Epoch 164/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1674 - jaccard: 0.1373 - val_loss: 0.2580 - val_jaccard: 0.0788\n",
      "Epoch 165/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1669 - jaccard: 0.1377 - val_loss: 0.2496 - val_jaccard: 0.0741\n",
      "Epoch 166/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1649 - jaccard: 0.1383 - val_loss: 0.2503 - val_jaccard: 0.0778\n",
      "Epoch 167/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1646 - jaccard: 0.1388 - val_loss: 0.2416 - val_jaccard: 0.0727\n",
      "Epoch 168/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1632 - jaccard: 0.1390 - val_loss: 0.2487 - val_jaccard: 0.0737\n",
      "Epoch 169/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1612 - jaccard: 0.1416 - val_loss: 0.2539 - val_jaccard: 0.0781\n",
      "Epoch 170/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1614 - jaccard: 0.1419 - val_loss: 0.2587 - val_jaccard: 0.0796\n",
      "Epoch 171/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1603 - jaccard: 0.1436 - val_loss: 0.2509 - val_jaccard: 0.0828\n",
      "Epoch 172/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1587 - jaccard: 0.1424 - val_loss: 0.2457 - val_jaccard: 0.0785\n",
      "Epoch 173/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1572 - jaccard: 0.1483 - val_loss: 0.2369 - val_jaccard: 0.0696\n",
      "Epoch 174/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1557 - jaccard: 0.1475 - val_loss: 0.2451 - val_jaccard: 0.0769\n",
      "Epoch 175/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1549 - jaccard: 0.1472 - val_loss: 0.2484 - val_jaccard: 0.0823\n",
      "Epoch 176/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1534 - jaccard: 0.1501 - val_loss: 0.2456 - val_jaccard: 0.0797\n",
      "Epoch 177/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1531 - jaccard: 0.1496 - val_loss: 0.2412 - val_jaccard: 0.0782\n",
      "Epoch 178/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1527 - jaccard: 0.1503 - val_loss: 0.2396 - val_jaccard: 0.0788\n",
      "Epoch 179/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1518 - jaccard: 0.1493 - val_loss: 0.2486 - val_jaccard: 0.0834\n",
      "Epoch 180/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1499 - jaccard: 0.1517 - val_loss: 0.2545 - val_jaccard: 0.0855\n",
      "Epoch 181/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1494 - jaccard: 0.1529 - val_loss: 0.2470 - val_jaccard: 0.0836\n",
      "Epoch 182/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1477 - jaccard: 0.1570 - val_loss: 0.2365 - val_jaccard: 0.0766\n",
      "Epoch 183/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1477 - jaccard: 0.1549 - val_loss: 0.2487 - val_jaccard: 0.0806\n",
      "Epoch 184/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1463 - jaccard: 0.1546 - val_loss: 0.2435 - val_jaccard: 0.0830\n",
      "Epoch 185/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1455 - jaccard: 0.1578 - val_loss: 0.2302 - val_jaccard: 0.0787\n",
      "Epoch 186/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1449 - jaccard: 0.1551 - val_loss: 0.2473 - val_jaccard: 0.0832\n",
      "Epoch 187/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1453 - jaccard: 0.1564 - val_loss: 0.2386 - val_jaccard: 0.0826\n",
      "Epoch 188/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1440 - jaccard: 0.1570 - val_loss: 0.2259 - val_jaccard: 0.0754\n",
      "Epoch 189/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1419 - jaccard: 0.1627 - val_loss: 0.2312 - val_jaccard: 0.0785\n",
      "Epoch 190/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1416 - jaccard: 0.1601 - val_loss: 0.2445 - val_jaccard: 0.0880\n",
      "Epoch 191/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1411 - jaccard: 0.1593 - val_loss: 0.2365 - val_jaccard: 0.0869\n",
      "Epoch 192/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1405 - jaccard: 0.1640 - val_loss: 0.2209 - val_jaccard: 0.0747\n",
      "Epoch 193/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1389 - jaccard: 0.1628 - val_loss: 0.2408 - val_jaccard: 0.0811\n",
      "Epoch 194/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1381 - jaccard: 0.1643 - val_loss: 0.2389 - val_jaccard: 0.0866\n",
      "Epoch 195/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1363 - jaccard: 0.1689 - val_loss: 0.2282 - val_jaccard: 0.0811\n",
      "Epoch 196/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1365 - jaccard: 0.1662 - val_loss: 0.2232 - val_jaccard: 0.0798\n",
      "Epoch 197/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1357 - jaccard: 0.1663 - val_loss: 0.2353 - val_jaccard: 0.0903\n",
      "Epoch 198/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1342 - jaccard: 0.1693 - val_loss: 0.2322 - val_jaccard: 0.0859\n",
      "Epoch 199/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1337 - jaccard: 0.1684 - val_loss: 0.2255 - val_jaccard: 0.0862\n",
      "Epoch 200/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1319 - jaccard: 0.1744 - val_loss: 0.2212 - val_jaccard: 0.0846\n",
      "Epoch 201/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1318 - jaccard: 0.1747 - val_loss: 0.2150 - val_jaccard: 0.0763\n",
      "Epoch 202/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1316 - jaccard: 0.1709 - val_loss: 0.2211 - val_jaccard: 0.0832\n",
      "Epoch 203/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1297 - jaccard: 0.1745 - val_loss: 0.2230 - val_jaccard: 0.0872\n",
      "Epoch 204/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1293 - jaccard: 0.1767 - val_loss: 0.2177 - val_jaccard: 0.0821\n",
      "Epoch 205/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1279 - jaccard: 0.1770 - val_loss: 0.2349 - val_jaccard: 0.0890\n",
      "Epoch 206/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1268 - jaccard: 0.1797 - val_loss: 0.2237 - val_jaccard: 0.0892\n",
      "Epoch 207/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1267 - jaccard: 0.1795 - val_loss: 0.2266 - val_jaccard: 0.0877\n",
      "Epoch 208/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1254 - jaccard: 0.1807 - val_loss: 0.2188 - val_jaccard: 0.0894\n",
      "Epoch 209/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1249 - jaccard: 0.1835 - val_loss: 0.2236 - val_jaccard: 0.0852\n",
      "Epoch 210/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1243 - jaccard: 0.1821 - val_loss: 0.2274 - val_jaccard: 0.0923\n",
      "Epoch 211/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1230 - jaccard: 0.1858 - val_loss: 0.2228 - val_jaccard: 0.0905\n",
      "Epoch 212/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1223 - jaccard: 0.1860 - val_loss: 0.2219 - val_jaccard: 0.0862\n",
      "Epoch 213/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1219 - jaccard: 0.1860 - val_loss: 0.2210 - val_jaccard: 0.0907\n",
      "Epoch 214/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1206 - jaccard: 0.1901 - val_loss: 0.2178 - val_jaccard: 0.0858\n",
      "Epoch 215/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1206 - jaccard: 0.1881 - val_loss: 0.2224 - val_jaccard: 0.0896\n",
      "Epoch 216/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1203 - jaccard: 0.1840 - val_loss: 0.2365 - val_jaccard: 0.0960\n",
      "Epoch 217/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1190 - jaccard: 0.1914 - val_loss: 0.2233 - val_jaccard: 0.0942\n",
      "Epoch 218/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1195 - jaccard: 0.1893 - val_loss: 0.2190 - val_jaccard: 0.0892\n",
      "Epoch 219/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1179 - jaccard: 0.1911 - val_loss: 0.2246 - val_jaccard: 0.0898\n",
      "Epoch 220/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1164 - jaccard: 0.1956 - val_loss: 0.2271 - val_jaccard: 0.0934\n",
      "Epoch 221/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1164 - jaccard: 0.1933 - val_loss: 0.2317 - val_jaccard: 0.0978\n",
      "Epoch 222/1000\n",
      "240/259 [==========================>...] - ETA: 0s - loss: 0.1150 - jaccard: 0.1990\n",
      "Epoch 00221: reducing learning rate to 0.0005000000237487257.\n",
      "259/259 [==============================] - 13s - loss: 0.1149 - jaccard: 0.1988 - val_loss: 0.2153 - val_jaccard: 0.0896\n",
      "Epoch 223/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1138 - jaccard: 0.1994 - val_loss: 0.2235 - val_jaccard: 0.0921\n",
      "Epoch 224/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1137 - jaccard: 0.1983 - val_loss: 0.2169 - val_jaccard: 0.0904\n",
      "Epoch 225/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1124 - jaccard: 0.2042 - val_loss: 0.2155 - val_jaccard: 0.0927\n",
      "Epoch 226/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1114 - jaccard: 0.2045 - val_loss: 0.2240 - val_jaccard: 0.0958\n",
      "Epoch 227/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1121 - jaccard: 0.2023 - val_loss: 0.2185 - val_jaccard: 0.0924\n",
      "Epoch 228/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1116 - jaccard: 0.2020 - val_loss: 0.2153 - val_jaccard: 0.0930\n",
      "Epoch 229/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1122 - jaccard: 0.2039 - val_loss: 0.2061 - val_jaccard: 0.0858\n",
      "Epoch 230/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1116 - jaccard: 0.2027 - val_loss: 0.2277 - val_jaccard: 0.0986\n",
      "Epoch 231/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1096 - jaccard: 0.2059 - val_loss: 0.2211 - val_jaccard: 0.0944\n",
      "Epoch 232/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1097 - jaccard: 0.2079 - val_loss: 0.2145 - val_jaccard: 0.0932\n",
      "Epoch 233/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1094 - jaccard: 0.2076 - val_loss: 0.2179 - val_jaccard: 0.0944\n",
      "Epoch 234/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1083 - jaccard: 0.2117 - val_loss: 0.2161 - val_jaccard: 0.0931\n",
      "Epoch 235/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1088 - jaccard: 0.2078 - val_loss: 0.2163 - val_jaccard: 0.0925\n",
      "Epoch 236/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1082 - jaccard: 0.2128 - val_loss: 0.2099 - val_jaccard: 0.0896\n",
      "Epoch 237/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1076 - jaccard: 0.2104 - val_loss: 0.2189 - val_jaccard: 0.0950\n",
      "Epoch 238/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1073 - jaccard: 0.2122 - val_loss: 0.2225 - val_jaccard: 0.0973\n",
      "Epoch 239/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1071 - jaccard: 0.2120 - val_loss: 0.2133 - val_jaccard: 0.0943\n",
      "Epoch 240/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1067 - jaccard: 0.2150 - val_loss: 0.2120 - val_jaccard: 0.0918\n",
      "Epoch 241/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1073 - jaccard: 0.2099 - val_loss: 0.2165 - val_jaccard: 0.0951\n",
      "Epoch 242/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1065 - jaccard: 0.2128 - val_loss: 0.2234 - val_jaccard: 0.0986\n",
      "Epoch 243/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1053 - jaccard: 0.2152 - val_loss: 0.2177 - val_jaccard: 0.0957\n",
      "Epoch 244/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1057 - jaccard: 0.2161 - val_loss: 0.2129 - val_jaccard: 0.0911\n",
      "Epoch 245/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1052 - jaccard: 0.2162 - val_loss: 0.2186 - val_jaccard: 0.0964\n",
      "Epoch 246/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1050 - jaccard: 0.2142 - val_loss: 0.2242 - val_jaccard: 0.1016\n",
      "Epoch 247/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1046 - jaccard: 0.2187 - val_loss: 0.2112 - val_jaccard: 0.0920\n",
      "Epoch 248/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1049 - jaccard: 0.2146 - val_loss: 0.2214 - val_jaccard: 0.0996\n",
      "Epoch 249/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1038 - jaccard: 0.2190 - val_loss: 0.2144 - val_jaccard: 0.0967\n",
      "Epoch 250/1000\n",
      "240/259 [==========================>...] - ETA: 0s - loss: 0.1032 - jaccard: 0.2179\n",
      "Epoch 00249: reducing learning rate to 0.0002500000118743628.\n",
      "259/259 [==============================] - 13s - loss: 0.1036 - jaccard: 0.2191 - val_loss: 0.2097 - val_jaccard: 0.0956\n",
      "Epoch 251/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1030 - jaccard: 0.2189 - val_loss: 0.2150 - val_jaccard: 0.0985\n",
      "Epoch 252/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1019 - jaccard: 0.2238 - val_loss: 0.2133 - val_jaccard: 0.0961\n",
      "Epoch 253/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1020 - jaccard: 0.2221 - val_loss: 0.2160 - val_jaccard: 0.0979\n",
      "Epoch 254/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1018 - jaccard: 0.2249 - val_loss: 0.2157 - val_jaccard: 0.0973\n",
      "Epoch 255/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1013 - jaccard: 0.2237 - val_loss: 0.2146 - val_jaccard: 0.0977\n",
      "Epoch 256/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1021 - jaccard: 0.2241 - val_loss: 0.2143 - val_jaccard: 0.0961\n",
      "Epoch 257/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1010 - jaccard: 0.2231 - val_loss: 0.2155 - val_jaccard: 0.0977\n",
      "Epoch 258/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1025 - jaccard: 0.2219 - val_loss: 0.2168 - val_jaccard: 0.0987\n",
      "Epoch 259/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1013 - jaccard: 0.2218 - val_loss: 0.2117 - val_jaccard: 0.0968\n",
      "Epoch 260/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1007 - jaccard: 0.2266 - val_loss: 0.2150 - val_jaccard: 0.0991\n",
      "Epoch 261/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1002 - jaccard: 0.2261 - val_loss: 0.2156 - val_jaccard: 0.0977\n",
      "Epoch 262/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1010 - jaccard: 0.2239 - val_loss: 0.2140 - val_jaccard: 0.0974\n",
      "Epoch 263/1000\n",
      "259/259 [==============================] - 13s - loss: 0.1002 - jaccard: 0.2254 - val_loss: 0.2157 - val_jaccard: 0.0991\n",
      "Epoch 264/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0998 - jaccard: 0.2292 - val_loss: 0.2150 - val_jaccard: 0.0977\n",
      "Epoch 265/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0997 - jaccard: 0.2267 - val_loss: 0.2140 - val_jaccard: 0.0969\n",
      "Epoch 266/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0995 - jaccard: 0.2286 - val_loss: 0.2144 - val_jaccard: 0.0975\n",
      "Epoch 267/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0993 - jaccard: 0.2281 - val_loss: 0.2147 - val_jaccard: 0.0991\n",
      "Epoch 268/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0991 - jaccard: 0.2295 - val_loss: 0.2135 - val_jaccard: 0.0958\n",
      "Epoch 269/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0990 - jaccard: 0.2294 - val_loss: 0.2162 - val_jaccard: 0.0974\n",
      "Epoch 270/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0995 - jaccard: 0.2285 - val_loss: 0.2141 - val_jaccard: 0.0982\n",
      "Epoch 271/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0985 - jaccard: 0.2313 - val_loss: 0.2155 - val_jaccard: 0.0994\n",
      "Epoch 272/1000\n",
      "240/259 [==========================>...] - ETA: 0s - loss: 0.0989 - jaccard: 0.2273\n",
      "Epoch 00271: reducing learning rate to 0.0001250000059371814.\n",
      "259/259 [==============================] - 13s - loss: 0.0992 - jaccard: 0.2269 - val_loss: 0.2159 - val_jaccard: 0.0994\n",
      "Epoch 273/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0987 - jaccard: 0.2300 - val_loss: 0.2122 - val_jaccard: 0.0973\n",
      "Epoch 274/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0982 - jaccard: 0.2305 - val_loss: 0.2131 - val_jaccard: 0.0973\n",
      "Epoch 275/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0977 - jaccard: 0.2330 - val_loss: 0.2138 - val_jaccard: 0.0985\n",
      "Epoch 276/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0980 - jaccard: 0.2318 - val_loss: 0.2156 - val_jaccard: 0.0989\n",
      "Epoch 277/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0982 - jaccard: 0.2296 - val_loss: 0.2134 - val_jaccard: 0.0979\n",
      "Epoch 278/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0983 - jaccard: 0.2294 - val_loss: 0.2149 - val_jaccard: 0.0993\n",
      "Epoch 279/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0980 - jaccard: 0.2316 - val_loss: 0.2149 - val_jaccard: 0.0991\n",
      "Epoch 280/1000\n",
      "259/259 [==============================] - 13s - loss: 0.0973 - jaccard: 0.2327 - val_loss: 0.2151 - val_jaccard: 0.0989\n",
      "Epoch 00279: early stopping\n",
      "324/324 [==============================] - 4s     \n"
     ]
    }
   ],
   "source": [
    "def trainer(model,class_,nb_epochs=1000,fit=True,use_existing=False):\n",
    "    print('This is run # {} for class {}'.format(run,class_))\n",
    "    \n",
    "    if use_existing:\n",
    "        model.load_weights('./data/weights/model_weights_class_{}_run_{}.hdf5'.format(class_,run))\n",
    "        \n",
    "    if fit:\n",
    "        quitter = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=50, verbose=1, mode='auto')\n",
    "        lrreducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, verbose=1, mode='auto', epsilon=0.001, cooldown=2, min_lr=0)\n",
    "        model_checkpoint = ModelCheckpoint('./data/weights/model_weights_class_{}_run_{}.hdf5'.format(class_,run), monitor='val_loss', save_best_only=True)\n",
    "        csvlogger = CSVLogger('./data/logs/training_log_class_{}_run_{}'.format(class_,run), separator=',', append=True)\n",
    "        # tensorboard = TensorBoard(log_dir='./data/logs/'+'tensorboard_all-classes-run_{:04d}'.format(run), histogram_freq=0, write_graph=True, write_images=False)\n",
    "        # tensorboard --logdir=data/logs\n",
    "        \n",
    "        model.fit(x, y[:,class_:class_+1,:,:],\n",
    "                  batch_size=20,\n",
    "                  nb_epoch=nb_epochs,\n",
    "                  verbose=1,\n",
    "                  shuffle=True,\n",
    "                  callbacks=[model_checkpoint,csvlogger,quitter,lrreducer],\n",
    "                  validation_split=0.2,\n",
    "                  initial_epoch=0)\n",
    "            \n",
    "    preds = model.predict(x, verbose=1)\n",
    "    np.save('./data/predictions/predictions_run_{}_class_{}.npy'.format(run,class_), preds)\n",
    "    \n",
    "    return model\n",
    "\n",
    "for class_ in [3]:\n",
    "    model.load_weights('./data/misc/initial_weights.hdf5')\n",
    "    model = trainer(model,class_,fit=True,use_existing=False)\n",
    "    model.save('./data/models/u-net-complete-model-run_{}_class_{}.hdf5'.format(run,class_))\n",
    "    push('Training on class {} is done'.format(class_),\n",
    "     'Train loss: %f, train jaccard: %f, val loss %f, val jaccard%f' %(model.history.history['loss'][-1],model.history.history['jaccard'][-1],model.history.history['val_loss'][-1],model.history.history['val_jaccard'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
